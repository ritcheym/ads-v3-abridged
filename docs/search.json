[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Skills",
    "section": "",
    "text": "This book provides an overview of the basic skills needed to turn raw data into informative summaries and visualisations presented in professional reports and presentations. The book will introduce learners to R (R Core Team, 2022), a programming language that can help automate working with data. The book will cover importing and processing data from spreadsheets, producing data summaries of descriptive statistics in tables, creating beautiful and informative visualisations, and constructing reports and presentations that automatically update when the underlying data changes.\nBy the end of this book, you will be able to use R to:"
  },
  {
    "objectID": "index.html#structure-of-the-course",
    "href": "index.html#structure-of-the-course",
    "title": "Applied Data Skills",
    "section": "Structure of the course",
    "text": "Structure of the course\nThis book accompanies a 10-week course, covering one chapter per week. Each chapter will introduce you to some new skills and concepts using concrete examples. At various points, there will be multiple-choice or fill-in-the-blank questions for you to check your understanding. Each chapter has accompanying walk-through videos, where an instructor demonstrates the skills covered in the chapter. Each chapter also has accompanying exercises that you should do to reinforce your learning."
  },
  {
    "objectID": "index.html#how-to-learn-data-skills",
    "href": "index.html#how-to-learn-data-skills",
    "title": "Applied Data Skills",
    "section": "How to learn data skills",
    "text": "How to learn data skills\n\nLearning data skills is kind of like having a gym membership (thanks to Phil McAleer for the analogy). You’ll be given state-of-the-art equipment to use and instructions for how to use them, but your data skills won’t get any stronger unless you practice.\n\nData skills do not require you to memorise lots of code. You will be introduced to many different functions, but the main skill to learn is how to efficiently find the information you need. This will require getting used to the structure of help files and cheat sheets, learning how to Goggle your problem and choose a helpful solution, and learning how to read error messages.\n\nLearning to code involves making a lot of mistakes. These mistakes are completely essential to the process, so try not to feel too frustrated. Many of the chapter exercises will give you broken code to fix so you get experience seeing what common errors look like. As you become a more experienced coder, you might not make fewer errors, but you’ll recover from them much faster.\n\n\n\n\nR Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/"
  },
  {
    "objectID": "01-intro.html#sec-ilo-intro",
    "href": "01-intro.html#sec-ilo-intro",
    "title": "1  Intro to R and RStudio",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nInstall R and RStudio\nBe able to install add-on packages\nBe able to get help for packages and functions\nBe able to create objects by writing and running code in the console"
  },
  {
    "objectID": "01-intro.html#sec-walkthrough-intro",
    "href": "01-intro.html#sec-walkthrough-intro",
    "title": "1  Intro to R and RStudio",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. We recommend first trying to work through each section of the book on your own and then watching the video if you get stuck, or if you would like more information. This will feel slower than just starting with the video, but you will learn more in the long-run. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence.\nDownload the RStudio IDE Cheatsheet"
  },
  {
    "objectID": "01-intro.html#sec-intro-installing-r",
    "href": "01-intro.html#sec-intro-installing-r",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.1 Installing R and RStudio",
    "text": "1.1 Installing R and RStudio\nR is a programming language that you will write code in and RStudio is a program that makes working in R easier.\nAppendix A has technical details on installing R and RStudio on your computer. If you need any help installing R, please ask on Teams or attend office hours. Once you have installed R and RStudio, come back to this chapter. If you already had R and/or RStudio installed, we recommend updating to the latest version before you work through this course. Appendix B has more details on how to do that. Here, we’ll concentrate on introducing you to RStudio’s interface and getting it configured.\n\n\n\n\n\n\nBook version\n\n\n\n\n\n\nThis book was created using R version 4.2.1 (2022-06-23) (Funny-Looking Kid) and RStudio version 2022.12.0.353 (Elsbeth Geranium). Most of the content of this book will work fine in versions of R above 4.0 and earlier versions of RStudio, although there may be some small differences in the interface.\n\n\n\n\n1.1.1 RStudio\nWhen you installed R, that gave your computer the ability to process the R programming language, and also installed an app called “R”. We will never use that app. Instead, we will use RStudio.\nRStudio is an Integrated Development Environment (IDE). Think of it as knowing English and using a plain text editor like NotePad to write a book versus using a word processor like Microsoft Word. You could do it, but it would be much harder without things like spell-checking and formatting and you wouldn’t be able to use some of the advanced features that Word has developed. In a similar way, you can use R without R Studio but we wouldn’t recommend it. RStudio serves as a text editor, file manager, spreadsheet viewer, and more. The key thing to remember is that although you will do all of your work using RStudio for this course, you are actually using two pieces of software, which means that from time to time, both of them may have separate updates.\nRStudio is arranged with four window panes.\n\n\n\n\nFigure 1.1: The RStudio IDE\n\n\n\n\nBy default, the upper left pane is the source pane, where you view, write, and edit code from files and view data tables in a spreadsheet format. When you first open RStudio, this pane won’t display until we open a document or load in some data – don’t worry, we’ll get to that soon.\nThe lower left pane is the console pane, where you can type in commands and view output messages. You can write code in the console to test it out. The code will run and can create objects in the environment, but the code itself won’t be saved. You need to write your code into a script in the source pane to save it, which we’ll cover in Chapter 2.\nThe right panes have several different tabs that show you information about your code. The most used tabs in the upper right pane are the Environment tab and the Help tab. The Environment tab lists some information about the objects that you have defined in your code. We’ll learn more about the Help tab in Section 1.3.5.\nIn the lower right pane, the most used tabs are the Files tab for directory structure, the Plots tab for plots made in a script, the Packages tab for managing add-on packages (see Section 1.3), and the Viewer tab to display reports created by your scripts. You can change the location of panes and what tabs are shown under Tools > Global Options… > Pane Layout.\n\n1.1.2 Reproducibility\nIn this class, you will be learning how to make reproducible reports. This involves writing scripts that transform data, create summaries and visualisations, and embed them in a report in a way that always gives you the same results.\nWhen you do things reproducibly, others (and future you) can understand and check your work. You can also reuse your work more easily. For example, if you need to create a report every month with the social media analytics for your company, a reproducible report allows you to download a new month’s data and create the report within seconds. It might take a little longer to set up the report in the first instance with reproducible methods, but the time it saves you in the long run is invaluable.\n\n\n\n\n\n\nSettings for Reproducibility\n\n\n\nSection A.4 shows you how to change two important settings in the Global Options to increase reproducibility. Your settings should have:\n\nRestore .RData into workspace at startup: \nChecked\nNot Checked\n\nSave workspace to .RData on exit: \nAlways\nNever\nAsk\n\n\n\n\n\n1.1.3 Themes and accessiblilty\nYou can customise how R Studio looks to make it work for you. Click Tools > Global Options > Appearance. You can change the default font, font size, and general appearance of R Studio, including using dark mode. Play around with the settings and see what you prefer - you’re going to spend a lot of time with R, it might as well look nice!"
  },
  {
    "objectID": "01-intro.html#sec-intro-sessions",
    "href": "01-intro.html#sec-intro-sessions",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.2 Sessions",
    "text": "1.2 Sessions\nIf you have the above settings configured correctly, when you open up RStudio and start writing code, loading packages, and creating objects, you will be doing so in a new session and your Environment tab should be completely empty. If you find that your code isn’t working and you can’t figure out why, it might be worth restarting your R session. This will clear the environment and detach all loaded packages - think of it like restarting your phone. There are several ways that you can restart R:\n\nMenu: Session > Restart R\n\n\nCmd-Shift-F10 or Ctl-Shift-F10\n\ntype .rs.restartR() in the console\n\nTry doing each of these now. Additionally, now would be a good time to create a notebook where you can keep a record of useful hints and tips and things to try when your code isn’t working. Add “restart R session” to this notebook as your first item."
  },
  {
    "objectID": "01-intro.html#sec-packages",
    "href": "01-intro.html#sec-packages",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.3 Packages and functions",
    "text": "1.3 Packages and functions\nWhen you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as base R and you can think of them like the default apps that come pre-loaded on your phone.\nOne of the great things about R, however, is that it is user extensible: anyone can create a new add-on that extends its functionality. There are currently thousands of packages that R users have created to solve many different kinds of problems, or just simply to have fun. For example, there are packages for data visualisation, machine learning, interactive dashboards, web scraping, and playing games such as Sudoku.\nAdd-on packages are not distributed with base R, but have to be downloaded and installed from an archive, in the same way that you would, for instance, download and install PokemonGo on your smartphone. The main repository where packages reside is called CRAN, the Comprehensive R Archive Network.\nThere is an important distinction between installing a package and loading a package.\n\n1.3.1 Installing a package\n\nThis is done using install.packages(). This is like installing an app on your phone: you only have to do it once and the app will remain installed until you remove it. For instance, if you want to use PokemonGo on your phone, you install it once from the App Store or Play Store; you don’t have to re-install it each time you want to use it. Once you launch the app, it will run in the background until you close it or restart your phone. Likewise, when you install a package, the package will be available (but not loaded) every time you open up R.\nInstall the tidyverse package on your system. This is the main package we will use throughout this book for data wrangling, summaries, and visualisation. It is actually a bundle of packages, which we’ll explain further in Section 1.3.4.\n\n\n\nRun in the console\n\ninstall.packages(\"tidyverse\")\n\n\nIf you get a message that says something like package ‘tidyverse’ successfully unpacked and MD5 sums checked, the installation was successful. If you get an error and the package wasn’t installed, check the troubleshooting section of Section B.4.\n\n\n\n\n\n\nInstall packages from the console only\n\n\n\nNever install a package from inside a script. Only do this from the console pane or the packages tab of the lower right pane.\n\n\nHere are some other packages you’ll want to install for the first two chapters.\n\n\n\nRun in the console\n\ninstall.packages(\"rmarkdown\") # for creating R markdown files\ninstall.packages(\"devtools\")  # for installing packages from github\n\n\nOnce you’ve installed the devtools package, you can also install packages from repositories other than CRAN, such as github. The following code installs the development version of a package for making waffle plots.\n\n\n\nRun in the console\n\n# install waffle package \ndevtools::install_github(\"hrbrmstr/waffle\")\n\n\n\n1.3.2 Loading a package\nThis is done using the library() function. This is like launching an app on your phone: the functionality is only there where the app is launched and remains there until you close the app or restart. For example, when you run library(devtools) within a session, the functions in the package referred to by rio will be made available for your R session. The next time you start R, you will need to run library(devtools) again if you want to access that package.\nAfter installing the beepr package, you can load it for your current R session as follows:\n\n\n\nRun in the console\n\nlibrary(beepr)\n\n\nYou might get some red text when you load a package, this is normal. It is usually warning you that this package has functions that have the same name as other packages you’ve already loaded.\n\n\n\n\n\n\nNote\n\n\n\nYou can use the convention package::function() to indicate in which add-on package a function resides. For instance, if you see readr::read_csv(), that refers to the function read_csv() in the {readr} add-on package. If the package is loaded using library(), you don’t have to specify the package name before a function unless there is a conflict (e.g., you have two packages loaded that have a function with the same name).\n\n\n\n1.3.3 Using a function\nNow you can run the function beep().\n\n\n\nRun in the console\n\nbeep()\n\n\nA function is a name that refers to some code you can reuse. We’ll start by using functions that are provided for you in packages, but you can also write your own functions. After the function name, there is a pair of parentheses, which contain zero or more arguments. These are options that you can set. In the example above, the sound argument has a default value of 1, which makes a “ping” sound. Try changing the argument to an integer between 1 and 11.\n\n\n\nRun in the console\n\nbeep(sound = 8)\n\n\nIf you type a function into the console pane, it will run as soon as you hit enter. If you put the function in a script or R Markdown document in the source pane, it won’t run until you run the script, knit the R Markdown file, or run a code chunk. You’ll learn more about this in Chapter 2.\n\n1.3.4 Tidyverse\ntidyverse is a meta-package that loads several packages we’ll be using in almost every chapter in this book:\n\n\nggplot2, for data visualisation (Chapter 3)\n\nreadr, for data import (Chapter 4)\n\ntibble, for tables (Chapter 4)\n\ntidyr, for data tidying (Chapter 8)\n\ndplyr, for data manipulation (Chapter 9)\n\nstringr, for strings\n\n\nforcats, for factors\n\n\npurrr, for repeating things\n\nWhen you install tidyverse, it also installs some other useful packages that you can load individually. You can get the full list using tidyverse_packages(), but the packages we’ll be using in this book are:\n\n\ngooglesheets4, for working with Google spreadsheets\n\nreadxl, for Excel files\n\nlubridate, for working with dates\n\nhms, for working with times\n\nrvest, for web scraping\n\n1.3.5 Function Help\nWhen you load the tidyverse it automatically loads all of the above packages, however, it can be helpful to know which package a function comes from if you need to Google it. If a function is in base R or a loaded package, you can type ?function_name in the console to access the help file. At the top of the help it will give you the function and package name.\nIf the package isn’t loaded, use ?package_name::function_name or specify the package in the help() function. When you aren’t sure what package the function is in, use the shortcut ??function_name.\n\n\n\nRun in the console\n\n# if the package is loaded\n?beepr\nhelp(\"beepr\")\n\n# works whether or not the package is loaded\n?beepr::beep\nhelp(\"beep\", package=\"beepr\") \n\n# shows a list of potentially matching functions\n??beep\n\n\n\n\n\nFunction help is always organised in the same way. For example, look at the help for ?glue::trim. At the top, it tells you the name of the function and its package in curly brackets, then a short description of the function, followed by a longer description. The Usage section shows the function with all of its arguments. If any of those arguments have default values, they will be shown like function(arg = default). The Arguments section lists each argument with an explanation. There may be a Details section after this with even more detail about the functions. The Examples section is last, and shows examples that you can run in your console window to see how the function works.\n\n\n\n\n\n\nFunction Help\n\n\n\n\nWhat is the first argument to the mean function? \ntrim\nna.rm\nmean\nx\n\nWhat package is read_excel in? \nreadr\nreadxl\nbase\nstats\n\n\n\n\n\n1.3.6 Arguments\nYou can look up the arguments/options that a function has by using the help documentation. Some arguments are required, and some are optional. Optional arguments will often use a default (normally specified in the help documentation) if you do not enter any value.\nAs an example, look at the help documentation for the function sample() which randomly samples items from a list.\n\n\n\nRun in the console\n\n?sample\n\n\nThe help documentation for sample() should appear in the bottom right help panel. In the usage section, we see that sample() takes the following form:\n\nsample(x, size, replace = FALSE, prob = NULL)\n\nIn the arguments section, there are explanations for each of the arguments. x is the list of items we want to choose from, size is the number of items we want to choose, replace is whether or not each item may be selected more than once, and prob gives the probability that each item is chosen. In the details section it notes that if no values are entered for replace or prob it will use defaults of FALSE (each item can only be chosen once) and NULL (all items will have equal probability of being chosen). Because there is no default value for x or size, they must be specified otherwise the code won’t run.\nLet’s try an example and just change the required arguments to x and size to ask R to choose 5 random letters (letters is a built-in vector of the 26 lower-case Latin letters).\n\n\n\n\nsample(x = letters, size = 5)\n\n[1] \"z\" \"v\" \"y\" \"w\" \"j\"\n\n\n\n\n\n\n\n\nWhy are my letters different to your letters?\n\n\n\n\n\nsample() generates a random sample. Each time you run the code, you’ll generate a different set of random letters (try it). The function set.seed() controls the random number generator - if you’re using any functions that use randomness (such as sample()), running set.seed() will ensure that you get the same result (in many cases this may not be what you want to do). To get the same numbers we do, run set.seed(1242016) in the console, and then run sample(x = letters, size = 5) again.\n\n\n\nNow we can change the default value for the replace argument to produce a set of letters that is allowed to have duplicates.\n\nset.seed(8675309)\nsample(x = letters, size = 5, replace = TRUE)\n\n[1] \"t\" \"k\" \"j\" \"k\" \"m\"\n\n\nThis time R has still produced 5 random letters, but now this set of letters has two instances of “k”. Always remember to use the help documentation to help you understand what arguments a function requires.\n\n1.3.7 Argument names\nIn the above examples, we have written out the argument names in our code (i.e., x, size, replace), however, this is not strictly necessary. The following two lines of code would both produce the same result (although each time you run sample() it will produce a slightly different result, because it’s random, but they would still work the same):\n\nsample(x = letters, size = 5, replace = TRUE)\nsample(letters, 5, TRUE)\n\nImportantly, if you do not write out the argument names, R will use the default order of arguments. That is, for sample it will assume that the first value you enter is x, the second value is size and the third value is replace.\nIf you write out the argument names, then you can write the arguments in whatever order you like:\n\nsample(size = 5, replace = TRUE, x = letters)\n\nWhen you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see code examples online that do not use argument names, so it is important to be able to understand which argument each bit of code is referring to (or look up the help documentation to check).\nIn this course, we will always write out the argument names the first time we use each function. However, in subsequent uses they may be omitted.\n\n1.3.8 Tab auto-complete\nOne very useful feature of R Studio is tab auto-complete for functions. If you write the name of the function and then press the tab key, R Studio will show you the arguments that function takes along with a brief description. If you press enter on the argument name it will fill in the name for you, just like auto-complete on your phone. This is incredibly useful when you are first learning R and you should remember to use this feature frequently.\n\n\n\n\nFigure 1.2: Tab auto-complete"
  },
  {
    "objectID": "01-intro.html#sec-objects",
    "href": "01-intro.html#sec-objects",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.4 Objects",
    "text": "1.4 Objects\nA large part of your coding will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses. You assign content to an object using <-.\nRun the following code in the console, but change the values of name and age to your own details and change christmas to a holiday or date you care about.\n\n\n\nRun in the console\n\nname <- \"Emily\"\nage <- 36\ntoday <- Sys.Date()\nchristmas <- as.Date(\"2023-12-25\")\n\n\nYou’ll see that four objects now appear in the environment pane:\n\n\nname is character (text) data. In order for R to recognise it as text, it must be enclosed in double quotation marks \" \".\n\nage is numeric data. In order for R to recognise this as a number, it must not be enclosed in quotation marks.\n\ntoday stores the result of the function Sys.Date(). This function returns your computer system’s date. Unlike name and age, which are hard-coded (i.e., they will always return the values you enter), the contents of the object today will change dynamically with the date. That is, if you run that function tomorrow, it will update the date to tomorrow’s date.\n\nchristmas is also a date but it’s hard-coded as a specific date. It’s wrapped within the as.Date() function that tells R to interpret the character string you provide as a date rather than text.\n\n\n\n\n\n\n\nNote\n\n\n\nTo print the contents of an object, type the object’s name in the console and press enter. Try printing all four objects now.\n\n\nFinally, a key concept to understand is that objects can interact and you can save the results of those interactions in new object. Edit and run the following code to create these new objects, and then print the contents of each new object.\n\n\n\nRun in the console\n\ndecade <- age + 10\nfull_name <- paste(name, \"Nordmann\")\nhow_long <- christmas - today"
  },
  {
    "objectID": "01-intro.html#sec-help",
    "href": "01-intro.html#sec-help",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.5 Getting help",
    "text": "1.5 Getting help\nYou will feel like you need a lot of help when you’re starting to learn. This won’t really go away; it’s impossible to memorise everything. The goal is to learn enough about the structure of R that you can look things up quickly. This is why we’ll introduce specialised jargon in the glossary for each chapter; it’s easier to google “convert character to numeric in R” than “make numbers in quotes be actual numbers not words”. In addition to the function help described above, here’s some additional resources you should use often.\n\n1.5.1 Package reference manuals\nStart up help in a browser by entering help.start() in the console. Click on Packages under Reference to see a list of packages. Scroll down to the readxl package and click on it to see a list of the functions that are available in that package.\n\n1.5.2 Googling\nIf the function help doesn’t help, or you’re not even sure what function you need, try Googling your question. It will take some practice to be able to use the right jargon in your search terms to get what you want. It helps to put “R” or “tidyverse” in the search text, or the name of the relevant package, like “ggplot2”.\n\n1.5.3 Vignettes\nMany packages, especially tidyverse ones, have helpful websites with vignettes explaining how to use their functions. Some of the vignettes are also available inside R. You can access them from a package’s help page or with the vignette() function.\n\n\n\nRun in the console\n\n# opens a list of available vignettes\nvignette(package = \"ggplot2\")\n\n# opens a specific vignette in the Help pane\nvignette(\"ggplot2-specs\", package = \"ggplot2\")"
  },
  {
    "objectID": "01-intro.html#sec-glossary-intro",
    "href": "01-intro.html#sec-glossary-intro",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.6 Glossary",
    "text": "1.6 Glossary\nThe glossary at the end of each chapter defines common jargon you might encounter while learning R. This specialised vocabulary can help you to communicate more efficiently and to search for solutions to problems. The terms below link to our PsyTeachR glossary, which contains further information and examples.\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nargument\n\n\nA variable that provides input to a function.\n\n\n\n\nbase r\n\n\nThe set of R functions that come with a basic installation of R, before you add external packages.\n\n\n\n\ncharacter\n\n\nA data type representing strings of text.\n\n\n\n\nchunk\n\n\nA section of code in an R Markdown file\n\n\n\n\nconflict\n\n\nHaving two packages loaded that have a function with the same name.\n\n\n\n\ncran\n\n\nThe Comprehensive R Archive Network: a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.\n\n\n\n\ndata wrangling\n\n\nThe process of preparing data for visualisation and statistical analysis.\n\n\n\n\ndefault value\n\n\nA value that a function uses for an argument if it is skipped.\n\n\n\n\nfactor\n\n\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\n\n\nfunction\n\n\nA named section of code that can be reused.\n\n\n\n\nide\n\n\nIntegrated Development Environment: a program that serves as a text editor, file manager, and provides functions to help you read and write code. RStudio is an IDE for R.\n\n\n\n\nknit\n\n\nTo create an HTML, PDF, or Word document from an R Markdown (Rmd) document\n\n\n\n\nnumeric\n\n\nA data type representing a real decimal number or integer.\n\n\n\n\nobject\n\n\nA word that identifies and stores the value of some data for later use.\n\n\n\n\npackage\n\n\nA group of R functions.\n\n\n\n\npanes\n\n\nRStudio is arranged with four window “panes”.\n\n\n\n\nr markdown\n\n\nThe R-specific version of markdown: a way to specify formatting, such as headers, paragraphs, lists, bolding, and links, as well as code blocks and inline code.\n\n\n\n\nscript\n\n\nA plain-text file that contains commands in a coding language, such as R.\n\n\n\n\nstring\n\n\nA piece of text inside of quotes.\n\n\n\n\nvector\n\n\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings."
  },
  {
    "objectID": "01-intro.html#sec-resources-intro",
    "href": "01-intro.html#sec-resources-intro",
    "title": "1  Intro to R and RStudio",
    "section": "\n1.7 Further Resources",
    "text": "1.7 Further Resources\n\nRStudio IDE Cheatsheet\nRStudio Cloud"
  },
  {
    "objectID": "02-reports.html#sec-ilo-reports",
    "href": "02-reports.html#sec-ilo-reports",
    "title": "2  Reports with R Markdown",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to structure a project\nBe able to knit a simple reproducible report with R Markdown\nBe able to create code chunks, tables, images, and inline R in an R Markdown document\n\nDownload the R Markdown Cheat Sheet"
  },
  {
    "objectID": "02-reports.html#sec-walkthrough-reports",
    "href": "02-reports.html#sec-walkthrough-reports",
    "title": "2  Reports with R Markdown",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "02-reports.html#sec-setup-reports",
    "href": "02-reports.html#sec-setup-reports",
    "title": "2  Reports with R Markdown",
    "section": "\n2.1 Setup",
    "text": "2.1 Setup\nFor reference, here are the packages we will use in this chapter. You may need to install them, as explained in Section 1.3.1, if running the code below in the console pane gives you an error.\n\n\n\nChapter packages\n\nlibrary(tidyverse)  # various data manipulation functions\nlibrary(knitr)      # for rendering a report from a script\nlibrary(rmarkdown)  # for using R markdown\nlibrary(kableExtra) # for styling tables"
  },
  {
    "objectID": "02-reports.html#sec-projects",
    "href": "02-reports.html#sec-projects",
    "title": "2  Reports with R Markdown",
    "section": "\n2.2 Organising a project",
    "text": "2.2 Organising a project\nBefore we write any code, first, we need to get organised. Projects in RStudio are a way to group all the files you need for one project. Most projects include scripts, data files, and output files like the PDF report created by the script or images.\n\n2.2.1 Default working directory\nFirst, make a new directory (i.e., folder) on your computer where you will keep all of your R projects. Name it something like “R-projects” (avoid spaces and other special characters). Make sure you know how to get to this directory using your computer’s Finder or Explorer.\n\n\n\n\n\n\nAvoid networked drives\n\n\n\nIf possible, don’t use a network or cloud drive (e.g., OneDrive or Dropbox), as this can sometimes cause problems.\n\n\nNext, open Tools > Global Options…, navigate to the General pane, and set the “Default working directory (when not in a project)” to this directory. Now, if you’re not working in a project, any files or images you make will be saved in this working directory.\n\n\n\n\n\n\nAvoid long path names\n\n\n\nOn some versions of Windows 10 and 11, it can cause problems if path names are longer than 260 characters. Set your default working directory to a path with a length well below that to avoid problems when R creates temporary files while rendering a report.\n\n\nYou can set the working directory to another location manually with menu commands: Session > Set Working Directory > Choose Directory… However, there’s a better way of organising your files by using Projects in RStudio.\n\n2.2.2 Start a Project\nStart by making a directory inside your default project directory where you will keep all of your materials for this class; we’d suggest naming it something like ADS-23.\nTo create a new project for the work we’ll do in this chapter:\n\nFile > New Project…\nName the project 02-reports\n\nSave it inside the ADS-23 directory\n\nRStudio will restart itself and open with this new project directory as the working directory.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Starting a new project.\n\n\nClick on the Files tab in the lower right pane to see the contents of the project directory. You will see a file called 02-reports.Rproj, which is a file that contains all of the project information. When you’re in the Finder/Explorer, you can double-click on it to open up the project.\n\n\n\n\n\n\nDot files\n\n\n\nDepending on your settings, you may also see a directory called .Rproj.user, which contains your specific user settings. You can ignore this and other “invisible” files that start with a full stop.\n\n\n\n\n\n\n\n\nDon’t nest projects\n\n\n\nDon’t ever save a new project inside another project directory. This can cause some hard-to-resolve problems.\n\n\n\n2.2.3 Naming Things\nBefore we start creating new files, it’s important to review how to name your files. This might seem a bit pedantic, but following clear naming rules so that both people and computers can easily find things will make your life much easier in the long run. Here are some important principles:\n\nfile and directory names should only contain letters, numbers, dashes, and underscores, with a full stop (.) between the file name and extension (that means no spaces!)\nbe consistent with capitalisation (set a rule to make it easy to remember, like always use lowercase)\nuse underscores (_) to separate parts of the file name, like the title and date, and dashes (-) to separate words in each part (e.g., social-media-report_2021-10.Rmd)\nname files with a pattern that alphabetises in a sensible order and makes it easy for you to find the file you’re looking for\nprefix a file name with an underscore to move it to the top of the list, or prefix all files with numbers to control their order\n\nFor example, these file names are a mess:\n\nreport.doc\nreport final.doc\nData (Customers) 11-15.xls\nCustomers Data Nov 12.xls\nfinal report2.doc\nproject notes.txt\nVendor Data November 15.xls\n\nHere is one way to structure them so that similar files have the same structure and it’s easy for a human to scan the list or to use code to find relevant files. See if you can figure out what the last one should be.\n\n_project-notes.txt\nreport_v1.doc\nreport_v2.doc\nreport_v3.doc\ndata_customer_2021-11-12.xls\ndata_customer_2021-11-15.xls\n\nvendor-data_2021-11-15.xls\ndata-vendor-2021_11_15.xls\ndata_vendor_2021-11-15.xls\ndata_2021-11-15_vendor.xls\n\n\n\n\n\n\n\nNaming practice\n\n\n\nThink of other ways to name the files above. Look at some of your own project files and see what you can improve."
  },
  {
    "objectID": "02-reports.html#sec-rmarkdown",
    "href": "02-reports.html#sec-rmarkdown",
    "title": "2  Reports with R Markdown",
    "section": "\n2.3 R Markdown",
    "text": "2.3 R Markdown\nThroughout this course we will use R Markdown to create reproducible reports with a table of contents, text, tables, images, and code. The text can be written using markdown, which is a way to specify formatting, such as headers, paragraphs, lists, bolding, and links.\n\n2.3.1 New document\nTo open a new R Markdown document, click File > New File > R Markdown. You will be prompted to give it a title; title it Important Info. You can also change the author name. Keep the output format as HTML.\nOnce you’ve opened a new document be sure to save it by clicking File > Save As…. You should name this file important_info (if you are on a Mac and can see the file extension, name it important_info.Rmd). This file will automatically be saved in your project folder (i.e., your working directory) so you should now see this file appear in your file viewer pane.\nWhen you first open a new R Markdown document you will see a bunch of welcome text that looks like this:\n\n\n\n\nFigure 2.2: New R Markdown text\n\n\n\n\nDo the following steps:\n\nChange the title to “Important Information” and the author to your name\nDelete everything after the setup chunk\nSkip a line after the setup chunk and type “## My info” (with the hashes but without the quotation marks); make sure there are no spaces before the hashes and at least one space after the hashes before the subtitle\nSkip a line and click the insert new code menu (a green box with a C and a plus sign) then choose R\n\n\nYour Markdown document should now look something like this:\n\n\n\n\nFigure 2.3: New R chunk\n\n\n\n\n\n2.3.2 Code chunks\nWhat you have created is a subtitle and a code chunk. In R Markdown, anything written in a grey code chunk is assumed to be code, and anything written in the white space (between the code chunks) is regarded as normal text (the actual colours will depend on which theme you have applied, but we will refer to the default white and grey). This makes it easy to combine both text and code in one document.\n\n\n\n\n\n\nCode chunk errors\n\n\n\nWhen you create a new code chunk you should notice that the grey box starts and ends with three back ticks ```. One common mistake is to accidentally delete these back ticks. Remember, code chunks and text entry are different colours - if the colour of certain parts of your Markdown doesn’t look right, check that you haven’t deleted the back ticks.\n\n\nIn your code chunk, write the code you created in Section 1.4.\n\n\n\nimportant_info.Rmd\n\nname <- \"Emily\"\nage <- 36\ntoday <- Sys.Date()\nchristmas <- as.Date(\"2023-12-25\")\n\n\n\n\n\n\n\n\nConsole vs Scripts\n\n\n\nIn Chapter 1, we asked you to type code into the console. Now, we want you to put code into code chunks in R Markdown files to make the code reproducible. This way, you can re-run your code any time the data changes to update the report, and you or others can inspect the code to identify and fix any errors.\nHowever, there will still be times that you need to put code in the console instead of in a script, such as when you install a new package. In this book, code chunks will be labelled with whether you should run them in the console or add the code to a script.\n\n\n\n2.3.3 Running code\nWhen you’re working in an R Markdown document, there are several ways to run your lines of code.\nFirst, you can highlight the code you want to run and then click Run > Run Selected Line(s), however this is tedious and can cause problems if you don’t highlight exactly the code you want to run.\nAlternatively, you can press the green “play” button at the top-right of the code chunk and this will run all lines of code in that chunk.\n\n\n\n\nFigure 2.4: Click the green arrow to run all the code in the chunk.\n\n\n\n\nEven better is to learn some of the keyboard short cuts for R Studio. To run a single line of code, make sure that the cursor is in the line of code you want to run (it can be anywhere) and press Ctrl+Enter or Cmd+Enter. If you want to run all of the code in the code chunk, press Ctrl+Shift+Enter or Cmd+Shift+Enter. Learn these short cuts; they will make your life easier!\n\n\n\n\nFigure 2.5: Use the keyboard shortcut to run only highlighted code, or run one line at a time by placing the cursor on a line without highlighting anything.\n\n\n\n\nRun your code using each of the methods above. You should see the variables name, age, today, and christmas appear in the environment pane. (Restart R to reset.)\n\n2.3.4 Inline code\nWe keep talking about using R Markdown for reproducible reports, but it’s easier to show you than tell you why this is so powerful and to give you an insight into how this course will (hopefully!) change the way you work with data forever!\nOne important feature of R Markdown is that you can combine text and code to insert values into your writing using inline coding. If you’ve ever had to copy and paste a value or text from one file to another, you’ll know how easy it can be to make mistakes. Inline code avoids this. Again it’s easier to show you what inline code does rather than to explain it so let’s have a go.\nFirst, copy and paste this text to the white space underneath your code chunk. If you used a different variable name than christmas, you should update this with the name of the object you created, but otherwise don’t change anything else.\nMy name is `r name` and I am `r age` years old. \nIt is `r christmas - today` days until Christmas, \nwhich is my favourite holiday.\n\n\n\n\n\n\nDisplaying Plots\n\n\n\nYou cannot display a plot using inline R. Plots should be displayed from code chunks.\n\n\n\n2.3.5 Knitting your file\nNow we are going to knit, or compile, the file into a document type of our choosing. In this case we’ll create a default html file, but you will learn how to create other files like Word and PDF throughout this course. To knit your file, click Knit > Knit to HMTL.\nR Markdown will create and display a new HTML document, but it will also automatically save this file in your working directory.\nAs if by magic, that slightly odd bit of text you copied and pasted now appears as a normal sentence with the values pulled in from the objects you created.\n\nMy name is Emily and I am 36 years old. It is 368 days until Christmas, which is my favourite holiday.\n\n\n\n\n\n\n\nKnitting with Code\n\n\n\n\n\nYou can also knit by typing the following code into the console. Never put this in an Rmd script itself, or it will try to knit itself in an infinite loop.\n\n\n\nRun in the console\n\nrmarkdown::render(\"important_info.Rmd\")\n\n# alternatively, you can use this, but may get a warning\nknitr::knit2html(\"important_info.Rmd\")"
  },
  {
    "objectID": "02-reports.html#loading-data",
    "href": "02-reports.html#loading-data",
    "title": "2  Reports with R Markdown",
    "section": "\n2.4 Loading data",
    "text": "2.4 Loading data\nNow let’s try another example of using Markdown, but this time rather than using objects we have created from scratch, we will read in a data file.\nSave and close your important_info.Rmd document. Then open and save a new Markdown document, this time named sales_data.Rmd. You can again get rid of everything after the setup chunk. Add library(tidyverse) to the setup chunk so that tidyverse functions are available to your script.\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n```\n\n\n\n2.4.1 Online sources\nFirst, let’s try loading data that is stored online. Create a code chunk in your document and copy, paste, and run the below code. This code loads some simulated sales data.\n\nThe data is stored in a .csv file so we’re going to use the read_csv() function to load it in.\nNote that the url is contained within double quotation marks - it won’t work without this.\n\n\n\n\nsales_data.Rmd\n\nsales_online <- read_csv(\"https://psyteachr.github.io/ads-v2/data/sales_data_sample.csv\")\n\n\n\n\n\n\n\n\nCould not find function\n\n\n\nIf you get an error message that looks like:\n\nError in read_csv(“https://psyteachr.github.io/ads-v2/data/sales_data_sample.csv”) :\ncould not find function “read_csv”\n\nThis means that you have not loaded tidyverse. Check that library(tidyverse) is in the setup chunk and that you have run the setup chunk.\n\n\nThis dataset is simulated sales data for different types of vehicles (originally from Kaggle) where each line of data is a single order. There are multiple ways to view and check a dataset in R. Do each of the following and make a note of what information each approach seems to give you. If you’d like more information about each of these functions, you can look up the help documentation with ?function:\n\nClick on the sales_online object in the environment pane\nRun head(sales_online) in the console\nRun summary(sales_online) in the console\nRun str(sales_online) in the console\nRun View(sales_online) in the console\n\n2.4.2 Local data files\nMore commonly, you will be working from data files that are stored locally on your computer. But where should you put all of your files? You usually want to have all your scripts and data files for a single project inside one folder on your computer, that project’s working directory, and we have already set up the main directory 02-reportsfor this chapter.\nYou can organise files in subdirectories inside this main project directory, such as putting all raw data files in a subdirectory called data and saving any image files to a subdirectory called images. Using subdirectories helps avoid one single folder becoming too cluttered, which is important if you’re working on big projects.\nIn your 02-reports directory, create a new folder named data, download a copy of the sales data file, and save it in this new subdirectory.\nTo load in data from a local file, again we can use the read_csv() function, but this time rather than specifying a url, give it the subdirectory and file name.\n\n\n\nsales_data.Rmd\n\nsales_local <- read_csv(\"data/sales_data_sample.csv\")\n\n\n\n\n\n\n\n\nTab-autocomplete file names\n\n\n\nUse tab auto-complete when typing file names in a code chunk. After you type the first quote, hit tab to see a drop-down menu of the files in your working directory. You can start typing the name of the subdirectory or file to narrow it down. This is really useful for avoiding annoying errors because of typos or files not being where you expect.\n\n\nThings to note:\n\nYou must include the file extension (in this case .csv)\nThe subdirectory folder name (data) and the file name are separated by a forward slash /\n\nPrecision is important, if you have a typo in the file name it won’t be able to find your file; remember that R is case sensitive - Sales_Data.csv is a completely different file to sales_data.csv as far as R is concerned.\n\n\n\n\n\n\n\nView sales_local\n\n\n\nRun head(), summary(), str(), and View() on sales_local to confirm that the data is the same as sales_online."
  },
  {
    "objectID": "02-reports.html#writing-a-report",
    "href": "02-reports.html#writing-a-report",
    "title": "2  Reports with R Markdown",
    "section": "\n2.5 Writing a report",
    "text": "2.5 Writing a report\nWe’re going to write a basic report for this sales dataset using R Markdown to show you some of the features. We’ll be expanding on almost every bit of what we’re about to show you throughout this course; the most important outcome is that you start to get comfortable with how R Markdown works and what you can use it to do.\n\n2.5.1 Data analysis\nFor this report we’re just going to present some simple sales stats for three types of vehicles: planes, motorcycles, and classic cars. We’ll come back to how to write this kind of code yourself in Chapter 5. For now, see if you can follow the logic of what the code is doing via the code comments.\nCreate a new code chunk, then copy, paste and run the following code and then view sales_counts by clicking on the object in the environment pane. Note that it doesn’t really matter whether you use sales_local or sales_online in the first line as they’re identical.\n\n\n\nsales_data.Rmd\n\n# keep only the data from planes, motorcycles, and cars\nsales_pmc <- filter(sales_online,\n         PRODUCTLINE %in% c(\"Planes\", \"Motorcycles\", \"Classic Cars\"))\n\n# count how many are in each PRODUCTLINE\nsales_counts <-count(sales_pmc, PRODUCTLINE)\n\n\nBecause each row of the dataset is a sale, this code gives us a nice and easy way of seeing how many sales were made of each type of vehicle; it just counts the number of rows in each group.\n\n\n\n\n\n\n\nPRODUCTLINE\n\n\nn\n\n\n\n\n\nClassic Cars\n\n\n967\n\n\n\n\nMotorcycles\n\n\n331\n\n\n\n\nPlanes\n\n\n306\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nJust putting an object by itself on a line “prints” it. Section 2.5.5 will show you how to print the table in different formats for your report.\n\n\n\n2.5.2 Text formatting\nYou can use the visual markdown editor if you have RStudio version 1.4 or higher. This will be a button at the top of the source pane with a pen tip and the menu options should be very familiar to anyone who has worked with software like Microsoft Word.\n\n\n\n\nFigure 2.6: The visual editor.\n\n\n\n\nThis is useful for complex styling, but you can also use these common plain-text style markups:\n\nHeaders are created by prefacing subtitles with one or more hashes (#) and a space (do not exclude the space). If you include a table of contents, this will be created from your document headers.\nFormat text with italics or bold by surrounding the text with one or two asterisks or underscores.\nMake lists using numbers, asterisks or dashes before items. Indent items to make nested lists.\nMake links like this: [psyTeachR](https://psyteachr.github.io/)\n\nDownload the R Markdown Cheat Sheet to learn more.\n\nCopy and paste the below text into the white space below the code chunk that loads in the data. Save the file and then click knit to view the results. It will look a bit messy for now as it contains the code and messages from loading the data but don’t worry, we’ll get rid of that soon.\n## Sample sales report\n\nThis report summarises the sales data for different types of vehicles sold between 2003 and 2005. This data is from [Kaggle](https://www.kaggle.com/kyanyoga/sample-sales-data).\n\n### Sales by type\n\nThe *total* number of **planes** sold was `r sales_counts$n[3]`\n\nThe *total* number of **classic cars** sold was `r sales_counts$n[1]`.\n\n\n\n\n\n\nWarning\n\n\n\nThe example markdown above (and in the rest of this book) is shown for the regular editor, not the visual editor. In the visual editor, you won’t see the hashes that create headers, or the asterisks that create bold and italic text. You also won’t see the backticks that demarcate inline code.\n\n\n\n\nThe example code above shown in the visual editor.\n\n\n\n\nIf you try to add the hashes, asterisks and backticks to the visual editor, you will get frustrated as they disappear. If you succeed, your code in the regular editor will look mangled like this:\n\\#\\#\\# Sales by type\n\nThe \\*total\\* number of \\*\\*planes\\*\\* sold was \\`r sales_counts\\$n\\[3]\\`\n\n\nTry and match up the inline code with what is in the sales_counts table. Of note:\n\nThe $ sign is used to indicate specific variables (or columns) in an object using the object$variable syntax.\nSquare brackets with a number e.g., [3] indicate a particular observation\nSo sales_counts$n[3] asks the inline code to display the third observation of the variable n in the dataset sales_online.\n\n\n\n\n\n\n\nFurther Practice\n\n\n\nAdd another line that reports the total numbers of motorcycles using inline code. Using either the visual editor or text markups, add in bold and italics so that it matches the others.\n\n\nSolution\n\nThe *total* number of **motorcycles** sold was `r sales_counts$n[2]`.\n\n\n\n\n2.5.3 Code comments\nIn the above code we’ve used code comments and it’s important to highlight how useful these are. You can add comments inside R chunks with the hash symbol (#). R will ignore characters from the hash to the end of the line.\n\n# important numbers\n\nn <- nrow(sales_online) # the total number of sales (number of rows)\nfirst <- min(sales_online$YEAR_ID) # the first (minimum) year\nlast <- max(sales_online$YEAR_ID) # the last (maximum) year\n\nIt’s usually good practice to start a code chunk with a comment that explains what you’re doing there, especially if the code is not explained in the text of the report.\nIf you name your objects clearly, you often don’t need to add clarifying comments. For example, if I’d named the three objects above total_number_of_sales, first_year and last_year, I would omit the comments. It’s a bit of an art to comment your code well, but try to add comments as you’re working through this book - it will help consolidate your learning and when future you comes to review your code, you’ll thank past you for being so clear.\n\n2.5.4 Images\nAs the saying goes, a picture paints a thousand words and sometimes you will want to communicate your data using visualisations.\nCreate a code chunk to display a graph of the data in your document after the text we’ve written so far. We’ll use some code that you’ll learn more about in Chapter 3 to make a simple bar chart that represents the sales data – focus on trying to follow how bits of the code map on to the plot that is created.\nCopy and paste the below code. Run the code in your Markdown to see the plot it creates and then knit the file to see how it is displayed in your document.\n\n\n\nsales_data.Rmd\n\nggplot(data = sales_counts, \n       mapping = aes(x = PRODUCTLINE, \n                     y = n, \n                     fill = PRODUCTLINE)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Type of vehicle\",\n       y = \"Number of sales\",\n       title = \"Sales by vehicle type\",\n       subtitle = \"2003 - 2005\")\n\n\n\n\n\n\n\n\nYou can also include images that you did not create in R using the markdown syntax for images or knitr::include_graphics(). This is very similar to loading data in that you can either use an image that is stored on your computer, or via a url.\nCreate a new code chunk underneath each of the sales figures for planes, classic cars, and motorcycles and add in an image from Google or Wikipedia for each (right click on an image and select copy image address to get a url). See the section on chunk defaults to see how to change the display size.\n\n\n\nsales_data.Rmd\n\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/3/3f/P-51_Mustang_edit1.jpg\")\n\n\n\n\n\n\n\n\nImage Licenses\n\n\n\n\n\nMost images on Wikipedia are public domain or have an open license. You can search for images by license on Google Images by clicking on the Tools button and choosing “Creative Commons licenses” from the “Usage Rights” menu.\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can use the markdown notation ![caption](url) to show an image. This goes in the markdown text section of the document, not inside is grey code block. The caption is optional; you can omit it like this:\n![](images/reports/google-images.png)\n\n2.5.5 Tables\nRather than a figure, we might want to display our data in a table.\n\nAdd a new level 2 heading (two hashtags) to your document, name the heading “Data in table form” and then create a new code chunk below this.\n\nFirst, let’s see what the table looks like if we don’t make any edits. Simply write the name of the table you want to display in the code chunk (in our case sales_counts) and then click knit to see what it looks like.\n\n\n\nsales_data.Rmd\n\nsales_counts\n\n\n## # A tibble: 3 × 2\n## # Groups:   PRODUCTLINE [3]\n##   PRODUCTLINE      n\n##   <chr>        <int>\n## 1 Classic Cars   967\n## 2 Motorcycles    331\n## 3 Planes         306\nIt’s just about readable but it’s not great.\nAnother way to customise tables uses the function kable() from the kableExtra package.\nAmend your code to load the kableExtra package and apply the kable() function to the table. Once you’ve done this, knit the file again to see the output.\n\n\n\nsales_data.Rmd\n\nlibrary(kableExtra) # for table display\n\nkable(sales_counts) # apply the kable function\n\n\n\n\n\n PRODUCTLINE \n    n \n  \n\n\n Classic Cars \n    967 \n  \n\n Motorcycles \n    331 \n  \n\n Planes \n    306 \n  \n\n\n\n\nIt’s better, but it’s still not amazing. So let’s make a few adjustments. We can change the names of the columns, add a caption, and also change the alignment of the cell contents using arguments to kable().\nWe can also add a theme to change the overall style. In this example we’ve used kable_classic but there are 5 others: kable_paper, kable_classic_2, kable_minimal, kable_material and kable_material_dark. Try them all and see which one you prefer.\nFinally, we can change the formatting of the first row using row_spec. Look up the help documentation for row_spec to see what other options are available. Try changing the value of any of the arguments below to figure out what they do.\n\n\n\nsales_data.Rmd\n\nk <- kable(sales_counts, \n      col.names = c(\"Product\", \"Sales\"),\n      caption = \"Number of sales per product line.\", \n      align = \"c\")\nk_style <- kable_classic(k, full_width = FALSE) \nk_highlighted <- row_spec(k_style, row = 0, bold = TRUE, color = \"red\") \n\nk_highlighted\n\n\n\n\nNumber of sales per product line.\n \n Product \n    Sales \n  \n\n\n Classic Cars \n    967 \n  \n\n Motorcycles \n    331 \n  \n\n Planes \n    306 \n  \n\n\n\n\n\n\n\n\n\n\nCaption placement\n\n\n\nThe appearance and placement of the table caption depends on the type of document you are creating. Your captions may look different to those in this book because you are creating a single-page html_document, while this book uses the html style from quarto, which is a newer alternative to R Markdown. You’ll learn more about other document output types in Section 10.2.\n\n\n\n\n\n\n\n\nAdvanced table customisation\n\n\n\n\n\nIf you’re feeling confident with what we have covered so far, the kableExtra vignette gives a lot more detail on how you can edit your tables using kableExtra.\nYou can also explore the gt package, which is complex, but allows you to create beautiful customised tables. Riding tables with {gt} and {gtExtras} is an outstanding tutorial."
  },
  {
    "objectID": "02-reports.html#refining-your-report",
    "href": "02-reports.html#refining-your-report",
    "title": "2  Reports with R Markdown",
    "section": "\n2.6 Refining your report",
    "text": "2.6 Refining your report\n\n2.6.1 Chunk defaults\nLet’s finish by tidying up the report and organising our code a bit better. When you create a new R Markdown file in RStudio, a setup chunk is automatically created - we’ve mostly ignored this chunk until now.\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\nYou can set more default options for your document here. Type the following code into the console to see the full list of options that you can set and their default values. However, the most useful and common options to change for the purposes of writing reports revolve around whether you want to show your code and the size of your images.\n\n\n\nRun in the console\n\n# list option default values\nstr(knitr::opts_chunk$get())\n\n\nReplace the code in your setup chunk with the below code and then try changing each option from FALSE to TRUE and changing the numeric values then knit the file again to see the difference it makes.\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo       = FALSE, # whether to show code chunks\n  message    = FALSE, # whether to show messages from your code\n  warning    = FALSE, # whether to show warnings from your code\n  fig.width  = 8,     # figure width in inches (at 96 dpi)\n  fig.height = 5,     # figure height in inches (at 96 dpi)\n  out.width = \"50%\"   # figures/images span 50% of the page width\n)\n```\n\n\n\n\n\n\n\n\nFigure versus output dimensions\n\n\n\n\n\nNote that fig.width and fig.height control the original size and aspect ratio of images generated by R, such as plots. This will affect the relative size of text and other elements in plots. It does not affect the size of existing images at all. However, out.width controls the display size of both existing images and figures generated by R. This is usually set as a percentage of the page width.\n\n\n\n\nFigure 2.7: A plot with the default values of fig.width = 8, fig.height = 5, out.width = “100%”\n\n\n\n\n\n\n\n\nFigure 2.8: The same plot with half the default width and height: fig.width = 4, fig.height = 2.5, out.width = “100%”\n\n\n\n\n\n\n\n\nFigure 2.9: The same plot as above at half the output width: fig.width = 4, fig.height = 2.5, out.width = “50%”\n\n\n\n\n\n\n\n\n2.6.2 Override defaults\nThese setup options change the behaviour for the entire document, however, you can override the behaviour for individual code chunks.\nFor example, by default you might want to hide your code but there also might be an occasion where you want to show the code you used to analyse your data. You can set echo = FALSE in your setup chunk to make hiding code the default but in the individual code chunk for your plot set echo = TRUE. Try this now and knit the file to see the results.\n\n\n```{r, echo = TRUE}\nggplot(data = sales_counts, \n       mapping = aes(x = PRODUCTLINE, \n                     y = n, \n                     fill = PRODUCTLINE)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Type of vehicle\",\n       y = \"Number of sales\",\n       title = \"Sales by vehicle type\",\n       subtitle = \"2003 - 2005\")\n```\n\n\nAdditionally, you can also override the default image display size or dimensions.\n\n\n```{r, out.width='25%'}\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/3/3f/P-51_Mustang_edit1.jpg\")\n```\n\n\n\n\n```{r, fig.width = 10, fig.height = 20}\nggplot(data = sales_counts, \n       mapping = aes(x = PRODUCTLINE, y = n, fill = PRODUCTLINE)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  labs(x = \"Type of vehicle\",\n       y = \"Number of sales\",\n       title = \"Sales by vehicle type\",\n       subtitle = \"2003 - 2005\")\n```\n\n\n\n2.6.3 Loading packages\nYou should add the packages you need in your setup chunk using library(). Often when you are working on a script, you will realize that you need to load another add-on package. Don’t bury the call to library(package_I_need) way down in the script. Put it in the setup chunk so the user has an overview of what packages are needed.\n\n\n\n\n\n\nMove library calls to the setup chunk\n\n\n\nMove the code that loads the tidyverse and kableExtra to the setup chunk.\n\n\n\n2.6.4 YAML header\nFinally, the YAML header is the bit at the very top of your Markdown document. You can set several options here as well.\n---\ntitle: \"Sales Data Report\"\nauthor: \"Your name\"\noutput:\n  html_document:\n    df_print: paged\n    theme: \n      version: 4\n      bootswatch: yeti\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 3\n    number_sections: false\n---\n\n\n\n\n\n\nTry\n\n\n\nTry changing the values from false to true to see what the options do.\n\n\nThe df_print: paged option prints data frames using rmarkdown::paged_table() automatically. You can use df_print: kable to default to the simple kable style, but you will need the code from Section 2.5.5 for more complex tables with kableExtra.\nThe built-in bootswatch themes are: default, cerulean, cosmo, darkly, flatly, journal, lumen, paper, readable, sandstone, simplex, spacelab, united, and yeti. You can view and download more themes. Try changing the theme to see which one you like best.\n\n\n\n\nFigure 2.10: Light themes in versions 3 and 4.\n\n\n\n\n\n\n\n\n\n\nYAML formatting\n\n\n\nYAML headers can be very picky about spaces and semicolons (the rest of R Markdown is much more forgiving). For example, if you put a space before “author”, you will get an error that looks like:\nError in yaml::yaml.load(..., eval.expr = TRUE) : \n  Parser error: while parsing a block mapping at line 1, \n  column 1 did not find expected key at line 2, column 2\nThe error message will tell you exactly where the problem is (the second character of the second line of the YAML header), and it’s usually a matter of fixing typos or making sure that the indenting is exactly right.\n\n\n\n2.6.5 Table of Contents\nThe table of contents is created by setting toc: true. It will be displayed at the top of your document unless you set toc_float: true or include toc_float: with its options collapsed and smooth_scroll (options for a setting are indented under it).\n---\noutput:\n  html_document:\n    toc: true\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    toc_depth: 3\n---\nThis will use the markdown header structure to create the table of contents. toc_depth: 3 means that the table of contents will only display headers up to level 3 (i.e., those that start with three hashes: ###). Add {-} after the header title to remove it from the table of contents (e.g., ### Overview {-}).\n\n\n\n\n\n\nMalformated ToC\n\n\n\nIf your table of contents isn’t showing up correctly, this probably means that your headers are not set up right. Make sure that headers have no spaces before the hashes and at least one space after the hashes. For example, ##Analysis won’t display as a header and be added to the table of contents, but ## Analysis will.\n\n\n\n2.6.6 Summary\nThis chapter has covered a lot but hopefully now you have a much better idea of what Markdown is able to do. Whilst working in Markdown takes longer in the initial set-up stage, once you have a fully reproducible report you can plug in new data each week or month and simply click knit, reducing duplication of effort, and the human error that comes with it.\nYou can access a working R Markdown file with the code from the example above to compare to your own code.\nAs you continue to work through the book you will learn how to wrangle and analyse your data and how to use Markdown to present it. We’ll slowly build on the available customisation options so over the course of next few weeks, you’ll find your Markdown reports start to look more polished and professional."
  },
  {
    "objectID": "02-reports.html#sec-exercises-reports",
    "href": "02-reports.html#sec-exercises-reports",
    "title": "2  Reports with R Markdown",
    "section": "\n2.7 Exercises",
    "text": "2.7 Exercises\nBelow are some additional exercises that will let you apply what you have learned in this chapter. We would suggest taking a break before you do these - it might feel slightly more effortful, but spreading out your practice will help you learn more in the long run.\n\n2.7.1 New project\nCreate a new project called “demo_report” (Section 2.2).\n\n2.7.2 New script\nIn the “demo_report” project, create a new Rmarkdown document called “job.Rmd” (Section 2.3). Edit the YAML header to output tables using kable and set a custom theme (Section 2.6.4).\n\n\nSolution\n\n---\ntitle: \"My Job\"\nauthor: \"Me\"\noutput:\n  html_document:\n    df_print: kable\n    theme: \n      version: 4\n      bootswatch: sandstone\n---\n\n\n2.7.3 R Markdown\nWrite a short paragraph describing your job or a job you might like to have in the future (Section 2.5.2). Include a bullet-point list of links to websites that are useful for that job (Section 2.5.2).\n\n\nSolution\n\nI am a research psychologist who is interested in open science \nand teaching computational skills.\n\n* [psyTeachR books](https://psyteachr.github.io/)\n* [Google Scholar](https://scholar.google.com/)\n\n\n2.7.4 Tables\nUse the following code to load a small table of tasks (Section 2.3.2). Edit it to be relevant to your job (you can change the categories entirely if you want).\n\n\n\njob.Rmd\n\ntasks <- tibble::tribble(\n  ~task,                   ~category,      ~frequency,\n  \"Respond to tweets\",     \"social media\", \"daily\",\n  \"Create a twitter poll\", \"social media\", \"weekly\",\n  \"Make the sales report\", \"reporting\",    \"montly\"\n)\n\n\nFigure out how to make it so that code chunks don’t show in your knitted document (Section 2.6.1).\n\n\nSolution\n\nYou can set the default to echo = FALSE in the setup chunk at the top of the script.\n\nknitr::opts_chunk$set(echo = FALSE)\n\nTo set visibility for a specific code chunk, put echo = FALSE inside the curly brackets.\n\n\n```{r, echo=FALSE}\n# code to hide\n```\n\n\n\nDisplay the table with purple italic column headers. Try different styles using kableExtra (Section 2.5.5).\n\n\n\nSolution\n\nk <- kableExtra::kable(tasks)\nk_style <- kableExtra::kable_minimal(k)\nk_highlight <- kableExtra::row_spec(k_style,\n                                    row = 0, \n                                    italic = TRUE, \n                                    color = \"purple\")\nk_highlight\n\n\n\n\n task \n    category \n    frequency \n  \n\n\n Respond to tweets \n    social media \n    daily \n  \n\n Create a twitter poll \n    social media \n    weekly \n  \n\n Make the sales report \n    reporting \n    montly \n  \n\n\n\n\n\n\n2.7.5 Images\nAdd an image of anything relevant (Section 2.5.4).\n\n\n\nSolution\n\nknitr::include_graphics(\"https://psyteachr.github.io/ads-v1/images/logos/logo.png\")\n\n\n\n\n\n\n\n\n\n\nAlternative Solution\n\nYou can add an image from the web using its URL:\n![Applied Data Skills](https://psyteachr.github.io/ads-v1/images/logos/logo.png)\nOr save an image into your project directory (e.g., in the images folder) and add it using the relative path:\n![Applied Data Skills](images/logos/logo.png)\n\n\n2.7.6 Inline R\nUse inline R to include the version of R you are using in the following sentence: “This report was created using R version 4.2.1 (2022-06-23).” You can get the version using the object R.version.string (Section 2.3.4).\n\n\nSolution\n\nThis report was created using R version 4.2.1 (2022-06-23).\n\n\n2.7.7 Knit\nKnit this document to html (Section 2.3.5).\n\n\nSolution\n\nClick on the knit button or run the following code in the console. (Do not put it the Rmd script!)\n\nrmarkdown::render(\"job.Rmd\")"
  },
  {
    "objectID": "02-reports.html#sec-glossary-reports",
    "href": "02-reports.html#sec-glossary-reports",
    "title": "2  Reports with R Markdown",
    "section": "\n2.8 Glossary",
    "text": "2.8 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\ndirectory\n\n\nA collection or “folder” of files on a computer.\n\n\n\n\nextension\n\n\nThe end part of a file name that tells you what type of file it is (e.g., .R or .Rmd).\n\n\n\n\nknit\n\n\nTo create an HTML, PDF, or Word document from an R Markdown (Rmd) document\n\n\n\n\nmarkdown\n\n\nA way to specify formatting, such as headers, paragraphs, lists, bolding, and links.\n\n\n\n\nproject\n\n\nA way to organise related files in RStudio\n\n\n\n\nr markdown\n\n\nThe R-specific version of markdown: a way to specify formatting, such as headers, paragraphs, lists, bolding, and links, as well as code blocks and inline code.\n\n\n\n\nscript\n\n\nA plain-text file that contains commands in a coding language, such as R.\n\n\n\n\nworking directory\n\n\nThe filepath where R is currently reading and writing files.\n\n\n\n\nyaml\n\n\nA structured format for information"
  },
  {
    "objectID": "02-reports.html#sec-resources-reports",
    "href": "02-reports.html#sec-resources-reports",
    "title": "2  Reports with R Markdown",
    "section": "\n2.9 Further Resources",
    "text": "2.9 Further Resources\n\n\nR Markdown Cheat Sheet \n\nR Markdown Tutorial\n\nR Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, & Garrett Grolemund\n\nChapter 27: R Markdown of R for Data Science\n\n\nProject Structure by Danielle Navarro\n\nHow to name files by Jenny Bryan\nkableExtra\ngt"
  },
  {
    "objectID": "03-viz.html#sec-ilo-viz",
    "href": "03-viz.html#sec-ilo-viz",
    "title": "3  Data Visualisation",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to identify categorical versus continuous data\nBe able to create plots in layers using ggplot\nBe able to choose appropriate plots for data"
  },
  {
    "objectID": "03-viz.html#sec-walkthrough-viz",
    "href": "03-viz.html#sec-walkthrough-viz",
    "title": "3  Data Visualisation",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "03-viz.html#sec-setup-viz",
    "href": "03-viz.html#sec-setup-viz",
    "title": "3  Data Visualisation",
    "section": "\n3.1 Set-up",
    "text": "3.1 Set-up\nCreate a new project for the work we’ll do in this chapter:\n\nFile > New Project…\nName the project 03-visualisation\n\nSave it inside your ADS directory.\n\nThen, create and save a new R Markdown document named plots.Rmd, get rid of the default template text, and load the packages in the set-up code chunk. You should have all of these packages installed already, but if you get the message Error in library(x) : there is no package called ‘x’, please refer to Section 1.3.1.\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse) # includes ggplot2\nlibrary(patchwork) # for multi-part plots\nlibrary(ggthemes)  # for plot themes\nlibrary(lubridate) # for manipulating dates\n```\n\n\nWe’d recommend making a new code chunk for each different activity, and using the white space to make notes on any errors you make, things you find interesting, or questions you’d like to ask the course team.\nDownload the ggplot2 cheat sheet."
  },
  {
    "objectID": "03-viz.html#variable-types",
    "href": "03-viz.html#variable-types",
    "title": "3  Data Visualisation",
    "section": "\n3.2 Variable types",
    "text": "3.2 Variable types\nIf a spreadsheet is in a tidy data format, each row is an observation, each column is a variable, and the information in each cell is a value. We’ll learn more about how to get our data into this format in Chapter 8, but to get started we’ll use datasets with the right format.\nFor example, the table below lists pets owned by members of the psyTeachR team. Each row is an observation of one pet. There are 6 variables for each pet, their name, owner, species, birthdate, weight (in kg), and rating (on a 5-point scale from “very evil” to “very good”).\n\n\n\n\n  \n\n\n\nVariables can be classified as continuous (numbers) or categorical (labels). When you’re plotting data, it’s important to know what kind of variables you have, which can help you decide what types of plots are most appropriate. Each variable also has a data type, such as numeric (numbers), character (text), or logical (TRUE/FALSE values). Some plots can only work on some data types. Make sure you have watched the mini-lecture on types of data from last week before you work through this chapter. Additionally, Appendix G has more details, as this concept will be relevant repeatedly.\n\n\n\n\nData types are like the categories when you format cells in Excel.\n\n\n\n\n\n3.2.1 Continuous\nContinuous variables are properties you can measure, like weight. You can use continuous variables in mathematical operations, like calculating the sum total of a column of prices or the average number of social media likes per day. They may be rounded to the nearest whole number, but it should make sense to have a measurement halfway between.\nContinuous variables always have a numeric data type. They are either integers like 42 or doubles like 3.14159.\n\n3.2.2 Categorical\nCategorical variables are properties you can count, like the species of pet. Categorical variables can be nominal, where the categories don’t really have an order, like cats, dogs and ferrets (even though ferrets are obviously best), or ordinal, where they have a clear order but the distance between the categories isn’t something you could exactly equate, like points on a Likert rating scale. Even if a data table uses numbers like 1-7 to represent ordinal variables, you shouldn’t treat them like continuous variables.\nCategorical data can have a character data type, also called strings. These are made by putting text inside of quotes. That text can be letters, punctuation, or even numbers. For example, \"January\" is a character string, but so is \"1\" if you put it in quotes. The character data type is best for variables that can have a lot of different values that you can’t predict ahead of time.\nCategorical data can also be factors, a specific type of integer that lets you specify the category names and their order. This is useful for making plots display with categories in the order you want (otherwise they default to alphabetical order). The factor data type is best for categories that have a specific number of levels.\n\n\n\n\n\n\nDo not factor numbers\n\n\n\nIf you factor numeric data, it gets converted to the integers 1 to the number of unique values, no matter what the values are. Additionally, you can no longer use the values as numbers, such as calculating the mean.\n\n\n\nExample\n\nx <- c(-3, 0, .5)  # numeric vector\nf <- factor(x)     # convert to factor\nx == as.numeric(f) # does not convert back to numeric \nm <- mean(f)       # you cannot average a factor\n\n\nWarning in mean.default(f): argument is not numeric or logical: returning NA\n\n\n[1] FALSE FALSE FALSE\n\n\n\n\nSometimes people represent categorical variables with numbers that correspond to names, like 0 = “no” and 1 = “yes”, but values in between don’t have a clear interpretation. If you have control over how the data are recorded, it’s better to use the character names for clarity. You’ll learn how to recode columns in Chapter 9.\n\n3.2.3 Dates and times\nDates and times are a special case of variable. They can act like categorical or continuous variables, and there are special ways to plot them. Dates and times can be hard to work with, but the lubridate”, “https://lubridate.tidyverse.org/ package provides functions to help you with this.\n\n# the current date\nlubridate::today()\n\n[1] \"2022-12-21\"\n\n\n\n# the current date and time in the GMT timezone\nlubridate::now(tzone = \"GMT\")\n\n[1] \"2022-12-21 21:01:53 GMT\"\n\n\n\n3.2.4 Test your understanding\nComing back to the pets dataset, what type of variable is in each column? You can use the function glimpse() to show a list of the column names, their data types, and the first few values in each column - here is the output of running glimpse() on the pets dataset.\n\nglimpse(pets)\n\nRows: 4\nColumns: 6\n$ name      <chr> \"Darwin\", \"Oy\", \"Khaleesi\", \"Bernie\"\n$ owner     <chr> \"Lisa\", \"Lisa\", \"Emily\", \"Phil\"\n$ species   <fct> ferret, ferret, cat, dog\n$ birthdate <date> 1998-04-02, NA, 2014-10-01, 2017-06-01\n$ weight    <dbl> 1.2, 2.9, 4.5, 32.0\n$ rating    <fct> a little evil, very good, very good, very good\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nColumn\nVariable type\nData type\n\n\n\nname\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nowner\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nspecies\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nbirthdate\n\ncontinuous\nnominal\nordinal\ndate\n\ncontinuous\nnominal\nordinal\ndate\n\n\nweight\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nrating\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate"
  },
  {
    "objectID": "03-viz.html#building-plots",
    "href": "03-viz.html#building-plots",
    "title": "3  Data Visualisation",
    "section": "\n3.3 Building plots",
    "text": "3.3 Building plots\nThere are multiple approaches to data visualisation in R; in this course we will use the popular package ggplot2, which is part of the larger tidyverse collection of packages. A grammar of graphics (the “gg” in “ggplot”) is a standardised way to describe the components of a graphic. ggplot2 uses a layered grammar of graphics, in which plots are built up in a series of layers. It may be helpful to think about any picture as having multiple elements that sit semi-transparently over each other. A good analogy is old Disney movies where artists would create a background and then add moveable elements on top of the background via transparencies.\n\ndisplays the evolution of a simple scatterplot using this layered approach. First, the plot space is built (layer 1); the variables are specified (layer 2); the type of visualisation (known as a geom) that is desired for these variables is specified (layer 3) - in this case geom_point() is called to visualise individual data points; a second geom is added to include a line of best fit (layer 4), the axis labels are edited for readability (layer 5), and finally, a theme is applied to change the overall appearance of the plot (layer 6).\n\n\n\n\n\nEvolution of a layered plot\n\n\n\n\nImportantly, each layer is independent and independently customisable. For example, the size, colour and position of each component can be adjusted, or one could, for example, remove the first geom (the data points) to only visualise the line of best fit, simply by removing the layer that draws the data points (Figure 3.1). The use of layers makes it easy to build up complex plots step-by-step, and to adapt or extend plots from existing code.\n\n\n\n\nFigure 3.1: Final plot with scatterplot layer removed.\n\n\n\n\n\n3.3.1 Plot Data\nLet’s build up the plot above, layer by layer. First we need to get the data. We’ll learn how to load data from different sources in Chapter 4, but this time we’ll use the same method as we did in Section 2.4.1 and load it from an online source.\nWhen you load the data, read_csv() will produce a message that gives you information about the data it has imported and what assumptions it has made. The “column specification” tells you what each column is named and what type of data R has categorised each variable as. The abbreviation “chr” is for character columns, “dbl” is for double columns, and “dttm” is a date/time column.\n\nsurvey_data <- read_csv(\"https://psyteachr.github.io/ads-v1/data/survey_data.csv\")\n\nRows: 707 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): caller_id, employee_id, issue_category\ndbl  (3): wait_time, call_time, satisfaction\ndttm (1): call_start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis data is simulated data for a call centre customer satisfaction survey. The first thing you should do when you need to plot data is to get familiar with what all of the rows (observations) and columns (variables) mean. Sometimes this is obvious, and sometimes it requires help from the data provider. Here, each row represents one call to the centre.\n\n\ncaller_id is a unique ID for each caller\n\nemployee_id is a unique ID for each employee taking calls\n\ncall_start is the date and time that the call arrived\n\nwait_time is the number of seconds the caller had to wait\n\ncall_time is the number of seconds the call lasted after the employee picked up\n\nissue_category is whether the issue was tech, sales, returns, or other\n\nsatisfaction is the customer satisfaction rating on a scale from 1 (very unsatisfied) to 5 (very satisfied)\n\nUnless you specify the column types, data importing functions will just guess the types and usually default to double for columns with numbers and character for columns with letters. Use the function spec() to find out all of the column types and edit them if needed.\n\nspec(survey_data)\n\ncols(\n  caller_id = col_character(),\n  employee_id = col_character(),\n  call_start = col_datetime(format = \"\"),\n  wait_time = col_double(),\n  call_time = col_double(),\n  issue_category = col_character(),\n  satisfaction = col_double()\n)\n\n\nLet’s set issue_category as a factor and set the order of the levels. By default, R will order the levels of a factor alphanumerically, however in many cases you will want or need to set your own order. For example, in this data, it makes most sense for the category “other” to come at the end of the list. After you update the column types, you have to re-import the data by adjusting the read_csv() code to set the col_types argument to the new column types.\nNote that because read_csv() is going to use the object survey_col_types, you must create survey_col_types before you run the adjusted read_csv() code. If you ever need to adjust your code, try to think about the order that the code will run in if you start from scratch and make sure it’s organised appropriately.\n\n# updated column types\nsurvey_col_types <- cols(\n  caller_id = col_character(),\n  employee_id = col_character(),\n  call_start = col_datetime(format = \"\"),\n  wait_time = col_double(),\n  call_time = col_double(),\n  issue_category = col_factor(levels = c(\"tech\", \"sales\", \"returns\", \"other\")),\n  satisfaction = col_integer()\n)\n\n# re-import data with correct column  types\nsurvey_data <- read_csv(\"https://psyteachr.github.io/ads-v1/data/survey_data.csv\",\n                        col_types = survey_col_types)\n\n\n3.3.2 Plot setup\nDefault theme\nPlots in this book use the black-and-white theme, not the default grey theme, so set your default theme to the same so your plots will look like the examples below. At the top of your script, in the setup chunk after you’ve loaded the tidyverse package, add the following code and run it. You’ll learn more ways to customise your theme in Section 3.3.3.4 and Section I.3.\n\ntheme_set(theme_bw()) # set the default theme\n\nData\nEvery plot starts with the ggplot() function and a data table. If your data are not loaded or you have a typo in your code, this will give you an error message. It’s best to check your plot after each step, so that you can figure out where errors are more easily.\n\nggplot(data = survey_data)\n\n\n\nA blank ggplot.\n\n\n\n\nMapping\nThe next argument to ggplot() is the mapping. This tells the plot which columns in the data should be represented by, or “mapped” to, different aspects of the plot, such as the x-axis, y-axis, line colour, object fill, or line style. These aspects, or “aesthetics”, are listing inside the aes() function.\nSet the arguments x and y to the names of the columns you want to be plotted on those axes. Here, we want to plot the wait time on the x-axis and the call time on the y-axis.\n\n# set up the plot with mapping\nggplot(\n  data = survey_data, \n  mapping = aes(x = wait_time, y = call_time)\n)\n\n\n\nFigure 3.2: A blank plot with x- and y- axes mapped.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the example above, we wrote out the names of the arguments data and mapping, but in practice, almost everyone omits them. Just make sure you put the data and mapping in the right order.\n\nggplot(survey_data,  aes(x = wait_time, y = call_time))\n\n\n\nGeoms\nNow we can add our plot elements in layers. These are referred to as geoms and their functions start with geom_. You add layers onto the base plot created by ggplot() with a plus (+).\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point() # scatterplot\n\n\n\nFigure 3.3: Adding a scatterplot with geom_point().\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSomewhat annoyingly, the plus has to be on the end of the previous line, not at the start of the next line. If you do make this mistake, it will run the first line of code to produce the base layer but then you will get the following error message rather than adding on geom_point().\n\nggplot(survey_data, aes(x = wait_time, y = call_time))\n\n\n\n\n\n\n+ geom_point() # scatterplot\n\nError:\n! Cannot use `+.gg()` with a single argument. Did you accidentally put + on a new line?\n\n\n\n\nMultiple geoms\nPart of the power of ggplot2 is that you can add more than one geom to a plot by adding on extra layers and so it quickly becomes possible to make complex and informative visualisation. Importantly, the layers display in the order you set them up. The code below uses the same geoms to produce a scatterplot with a line of best fit but orders them differently.\n\n# Points first\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point() + # scatterplot\n  geom_smooth(method = lm) # line of best fit\n\n# Line first\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_smooth(method = lm) + # line of best fit\n  geom_point() # scatterplot\n\n\n\n\n\nFigure 3.4: Points first versus line first.\n\n\n\n\nSaving plots\nJust like you can save numbers and data tables to objects, you can also save the output of ggplot(). The code below produces the same plots we created above but saves them to objects named point_first and line_first. If you run this code, the plots won’t display like they have done before. Instead, you’ll see the object names appear in the environment pane.\n\npoint_first <- \n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point() + # scatterplot\n  geom_smooth(method = lm) # line of best fit\n  \nline_first <-\n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_smooth(method = lm) + # line of best fit\n  geom_point() # scatterplot\n\nTo view the plots, call the objects by name. This will output each plot separately.\n\npoint_first # view first plot\nline_first # view second plot\n\nCombining plots\nOne of the reasons to save your plots to objects is so that you can combine multiple plots using functions from the patchwork package. The below code produces the above plot by combining the two plots with + and then specifying that we want the plots produced on a single row with the nrow argument in plot_layout().\n\n# add plots together in 1 row; try changing nrow to 2\npoint_first + line_first + plot_layout(nrow = 1)\n\n\n\nFigure 3.5: Combining plots with patchwork.\n\n\n\n\n\n3.3.3 Customising plots\nStyling geoms\nWe should definitely put the line in front of the points, but the points are still a bit dark. If you want to change the overall style of a geom, you can set the arguments colour, alpha, shape, size and linetype inside the geom function. There are many different values that you can set these to; Appendix I) gives details of these. Play around with different values below and figure out what the default values are for shape and size.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2, # 20% transparency\n             shape = 18,  # solid diamond\n             size = 2) + \n  geom_smooth(method = lm, \n              formula = y~x, # formula used to draw line, \n              # setting the default formula avoids an annoying message\n              colour = rgb(0, .5, .8),\n              linetype = 3) \n\n\n\nFigure 3.6: Changing geom styles.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis method is only for changing the style of all the shapes made with that geom. If you want, for example, points to have different colours depending on which issue category they are from, you set the argument colour = issue_category inside the aes() function for the mapping. You can customise the colours used with scale_ functions, which you will learn about below and in Appendix I).\n\n\nFormat axes\nNow we need to make the axes look neater. There are several functions you can use to change the axis labels, but the most powerful ones are the scale_ functions. You need to use a scale function that matches the data you’re plotting on that axis and this is where it becomes particularly important to know what type of data you’re working with. Both of the axes here are continuous, so we’ll use scale_x_continuous() and scale_y_continuous().\nThe name argument changes the axis label. The breaks argument sets the major units and needs a vector of possible values, which can extend beyond the range of the data (e.g., wait time only goes up to 350, but we can specify breaks up to 600 to make the maths easier). The seq() function creates a sequence of numbers from one to another by specified steps.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 30))\n\n\n\nFigure 3.7: Formatting plot axes with scale_ functions.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheck the help for ?scale_x_continuous to see how you would set the minor units or specify how many breaks you want instead.\n\n\nAxis limits\nIf you want to change the minimum and maximum values on an axis, use the coord_cartesian() function. Many plots make more sense if the minimum and maximum values represent the range of possible values, even if those values aren’t present in the data. Here, wait and call times can’t be less than 0 seconds, so we’ll set the minimum values to 0 and the maximum values to the first break above the highest value.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 30)) +\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180))\n\n\n\nFigure 3.8: Changing the axis limits.\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nYou can also set the limits argument inside the scale_ functions, but this actually removes any data that falls outside these limits, rather than cropping your plot, and this can change the appearance of certain types of plots like violin plots and density plots.\n\n\nThemes\nggplot2 comes with several built-in themes, such as theme_minimal() and theme_bw(), but the ggthemes package provides even more themes to match different software, such as GoogleDocs or Stata, or publications, such as the Economist or the Wall Street Journal. Let’s add the GoogleDocs theme, but change the font size to 20 with the base_size argument.\nIt’s also worth highlighting that this code is starting to look quite complicated because of the number of layers, but because we’ve built it up slowly it should (hopefully!) make sense. If you see examples of ggplot2` code online that you’d like to adapt, build the plot up layer by layer and it will make it easier to understand what each layer adds.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 30)) +\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180)) +\n  ggthemes::theme_gdocs(base_size = 20)\n\n\n\nFigure 3.9: Changing the theme.\n\n\n\n\nTheme tweaks\nIf you’re still not quite happy with a theme, you can customise it even further with the themes() function. Check the help for this function to see all of the possible options. The most common thing you’ll want to do is to remove an element entirely. You do this by setting the relevant argument to element_blank(). Below, we’re getting rid of the x-axis line and the plot background, which removes the line around the plot.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = x_breaks) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = y_breaks) +\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180)) +\n  theme_gdocs(base_size = 11) +\n  theme(axis.line.x = element_blank(),\n        plot.background = element_blank())\n\n\n\nFigure 3.10: Customising the theme."
  },
  {
    "objectID": "03-viz.html#appropriate-plots",
    "href": "03-viz.html#appropriate-plots",
    "title": "3  Data Visualisation",
    "section": "\n3.4 Appropriate plots",
    "text": "3.4 Appropriate plots\nNow that you know how to build up a plot by layers and customise its appearance, you’re ready to learn about some more plot types. Different types of data require different types of plots, so this section is organised by data type.\nThe ggplot2 cheat sheet is a great resource to help you find plots appropriate to your data, based on how many variables you’re plotting and what type they are. The examples below all use the same customer satisfaction data, but each plot communicates something different.\nWe don’t expect you to memorise all of the plot types or the methods for customising them, but it will be helpful to try out the code in the examples below for yourself, changing values to test your understanding.\n\n3.4.1 Counting categories\nBar plot\nIf you want to count the number of things per category, you can use geom_bar(). You only need to provide a x mapping to geom_bar() because by default geom_bar() uses the number of observations in each group of x and the value for y, so you don’t need to tell it what to put on the y-axis.\n\nggplot(survey_data, aes(x = issue_category)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou probably want to customise some things, like the colours, order of the columns, and their labels. Inspect the code below and try running it layer by layer to figure out where these things change. The functions scale_fill_manual() and scale_x_discrete() are new, but work in the same way as the other scale_ functions. You’ll learn more about this in Section 10.1.\n\n\n\nCode\n\nggplot(survey_data, aes(x = issue_category, \n                        fill = issue_category)) +\n  geom_bar() +\n  scale_x_discrete(\n    # change axis title\n    name = \"Issue Category\", \n    # change order\n    limits = c(\"tech\", \"returns\", \"sales\", \"other\"), \n    # change labels\n    labels = c(\"Technical\", \"Returns\", \"Sales\", \"Other\") \n  ) +\n  scale_fill_manual(\n    # change colours\n    values = c(tech = \"goldenrod\", \n                returns = \"darkgreen\", \n                sales = \"dodgerblue3\", \n                other = \"purple3\"),\n    # remove the legend\n    guide = \"none\" \n  ) +\n  scale_y_continuous(\n    name = \"\", # remove axis title\n    # remove the space above and below the y-axis\n    expand = expansion(add = 0)\n  ) +\n  # minimum = 0, maximum = 350\n  coord_cartesian(ylim = c(0, 350)) + \n  ggtitle(\"Number of issues per category\") # add a title\n\n\n\nFigure 3.11: ?(caption)\n\n\n\n\n\n\n\nColumn plot\nIf your data already have a column with the number you want to plot, you can use geom_col() to plot it. We can use the count() function to make a table with a row for each issue_category and a column called n with the number of observations in that category.\n\ncount_data <- count(x = survey_data, issue_category)\n\n\n\n\n\nThe mapping for geom_col() requires you to set both the x and y aesthetics. Set y = n because we want to plot the number of issues in each category, and that information is in the column called n.\n\nggplot(count_data, aes(x = issue_category, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\nPie chart\nPie charts are a misleading form of data visualisation, so we won’t cover them. We’ll cover options for visualising proportions, like waffle, lollipop and treemap plots, in Section 10.1.4).\nTest your understanding\nHere is a small data table.\n\n\ncountry\npopulation\nisland\n\n\n\nNorthern Ireland\n1,895,510\nIreland\n\n\nWales\n3,169,586\nGreat Britain\n\n\nRepublic of Ireland\n4,937,786\nIreland\n\n\nScotland\n5,466,000\nGreat Britain\n\n\nEngland\n56,550,138\nGreat Britain\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWhat geom would you use to plot the population for each of the 5 countries? \ngeom_bar\ngeom_col\n\n\n\n\nWhat mapping would you use?\n\naes(x = country, y = population)aes(x = population, y = country)aes(x = country)aes(x = island)aes(y = population)\n\n\n\n\n\nWhat geom would you use to plot the number of countries on each island? \ngeom_bar\ngeom_col\n\n\n\n\nWhat mapping would you use?\n\naes(x = country, y = population)aes(x = population, y = country)aes(x = country)aes(x = island)aes(y = population)\n\n\n\n\n\n\n\n3.4.2 One continuous variable\nIf you have a continuous variable, like the number of seconds callers have to wait, you can use geom_histogram() or geom_density() to show the distribution. Just like geom_bar() you are only required to specify the x variable.\nHistogram\nA histogram splits the data into “bins” along the x-axis and shows the count of how many observations are in each bin along the y-axis.\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nHistogram of wait times.\n\n\n\n\nYou should always set the binwidth or number of bins to something meaningful for your data (otherwise you get an annoying message). You might need to try a few options before you find something that looks good and conveys the meaning of your plot – try changing the values of binwidth and bins below to see what works best.\n\n# adjust width of each bar\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(binwidth = 15)\n\n# adjust number of bars\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(bins = 5)\n\nBy default, the bars start centered on 0, so if binwidth is set to 15, the first bar would include -7.5 to 7.5 seconds, which doesn’t make much sense. We can set boundary = 0 so that each bar represents increments of 15 seconds starting from 0.\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(binwidth = 15, boundary = 0)\n\n\n\n\n\n\n\nFinally, the default style of grey bars is ugly, so you can change that by setting the fill and colour, as well as using scale_x_continuous() to update the axis labels.\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(binwidth = 15, \n                 boundary = 0, \n                 fill = \"white\", \n                 color = \"black\") +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60))\n\n\n\nHistogram with custom styles.\n\n\n\n\nFrequency plot\nRather than plotting each bin as a bar, you can connect a line across the top of each bin using a frequency plot. The function geom_freqpoly() works the same as geom_histogram(), except it can’t take a fill argument because it’s just a line.\n\nggplot(survey_data, aes(x = wait_time)) +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60)) +\n  geom_freqpoly(boundary = 0, binwidth = 15, \n                color = \"black\")\n\n\n\n\n\n\n\nDensity plot\nIf the distribution is smooth, a density plot is often a better way to show the distribution. A density plot doesn’t need the binwidth or boundary arguments because it doesn’t split the data into bins, but it can have fill.\n\nggplot(survey_data, aes(x = wait_time)) +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60)) +\n  geom_density(fill = \"purple\", color = \"black\")\n\n\n\n\n\n\n\nTest your understanding\nImagine you have a table of the population for each country in the world with the columns country and population. We’ll just look at the 76 countries with populations of less than a million.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWhat kind of plot is A? \ngeom_histogram\ngeom_freqpoly\ngeom_density\n\nWhat kind of plot is B? \ngeom_histogram\ngeom_freqpoly\ngeom_density\n\nWhat kind of plot is C? \ngeom_histogram\ngeom_freqpoly\ngeom_density\n\n\n\n\nHow would you set the mapping for these plots?\n\naes(x = country, y = population)aes(x = population, y = country)aes(x = population)aes(x = population, y = count)\n\n\n\n\n\nWhat is the binwidth of the histogram? \n1\n100\n100K\n1M\n\n\n\n\n\n3.4.3 Grouped continuous variables\nThere are several ways to compare continuous data across groups. Which you choose depends on what point you are trying to make with the plot.\nSubdividing distributions\nIn previous plots, we have used fill purely for visual reasons, e.g., we have changed the colour of the histogram bars to make them look nicer. However, you can also use fill to represent another variable so that the colours become meaningful.\nSetting the fill aesthetic in the mapping will produce different coloured bars for each category of the fill variable, in this case issue_category.\n\nggplot(survey_data, aes(x = wait_time, fill = issue_category)) +\n  geom_histogram(boundary = 0, \n                 binwidth = 15,\n                 color = \"black\")\n\n\n\nHistogram with categories represented by fill.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you set an aspect to represent the data, you do this inside the aes() function for the mapping, not as an argument to the geom. If you try to set this in a geom, you’ll get the following error (unless you coincidentally have an object named issue_category that is a colour word).\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(boundary = 0, \n                 binwidth = 15, \n                 color = \"black\",\n                 fill = issue_category)\n\nError in layer(data = data, mapping = mapping, stat = stat, geom = GeomBar, : object 'issue_category' not found\n\n\n\n\nBy default, the categories are positioned stacked on top of each other. The function geom_area() gives a similar effect when stat = \"bin\".\n\n# area plot\nggplot(survey_data, mapping = aes(x = wait_time, fill = issue_category)) +\n  geom_area(stat = \"bin\", \n            boundary = 0, \n            binwidth = 15, \n            color = \"black\")\n\n\n\nStacked area plot.\n\n\n\n\nComparing distributions\nIf you want to compare more than one distribution, you can set the position argument of geom_histogram() to “dodge” to put the bars for each group next to each other instead of stacking them. However, this can look confusing with several categories. Instead, usegeom_freqpoly() to plot a line connecting the top of each bin.\n\n# dodged histogram\nhistogram_dodge <- \n  ggplot(survey_data, aes(x = wait_time, \n                          fill = issue_category,\n                          colour = issue_category))+\n  geom_histogram(boundary = 0, \n                 binwidth = 15, \n                 position = \"dodge\") +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60)) +\n  ggtitle(\"Dodged Histogram\")\n\n# frequency plot\nfreqpoly_plot <- \n  ggplot(survey_data, aes(x = wait_time,\n                          fill = issue_category,\n                          colour = issue_category)) +\n  geom_freqpoly(binwidth = 15, \n                boundary = 0,\n                size = 1) +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60)) +\n  ggtitle(\"Frequency\")\n\n# put plots together\nhistogram_dodge + freqpoly_plot + \n  plot_layout(nrow = 1, guides = \"collect\") # collects the legends together, try removing this\n\n\n\nFigure 3.12: Different ways to plot the distribution of a continuous variable for multiple groups.\n\n\n\n\nViolin plot\nAnother way to compare groups of continuous variables is the violin plot. This is like a density plot, but rotated 90 degrees and mirrored - the fatter the violin, the larger proportion of data points there are at that value.\n\nviolin_area <- \n  ggplot(survey_data, aes(x = issue_category, y = wait_time)) +\n  geom_violin() +\n  ggtitle('scale = \"area\"')\n\nviolin_count <- \n  ggplot(survey_data, aes(x = issue_category, y = wait_time)) +\n  geom_violin(scale = \"count\") +\n  ggtitle('scale = \"count\"')\n\nviolin_area + violin_count\n\n\n\nFigure 3.13: The default violin plot gives each shape the same area. Set scale=‘count’ to make the size proportional to the number of observations.\n\n\n\n\nBoxplot\nBoxplots serve a similar purpose to violin plots (without the giggles from the back row). They don’t show you the shape of the distribution, but rather some statistics about it. The middle line represents the median; half the data are above this line and half below it. The box encloses the 25th to 75th percentiles of the data, so 50% of the data falls inside the box. The “whiskers” extending above and below the box extend 1.5 times the height of the box, although you can change this with the coef argument. The points show outliers – individual data points that fall outside of this range.\n\nggplot(survey_data, aes(x = issue_category, y = wait_time)) +\n geom_boxplot()\n\n\n\nFigure 3.14: Basic boxplot.\n\n\n\n\nCombo plots\nViolin plots are frequently layered with other geoms that represent the mean or median values in the data. This is a lot of code, to help your understanding a) run it layer by layer to see how it builds up and b) change the values throughout the code\n\n# add fill and colour to the mapping\n\nggplot(survey_data,  aes(x = issue_category, \n                         y = wait_time,\n                         fill = issue_category,\n                         colour = issue_category)) +\n  scale_x_discrete(name = \"Issue Category\") +\n  scale_y_continuous(name = \"Wait Time (seconds)\",\n                     breaks = seq(0, 600, 60)) +\n  coord_cartesian(ylim = c(0, 360)) +\n  guides(fill = \"none\", colour = \"none\") + \n  # add a line at median (50%) score\n  geom_violin(alpha = 0.4, \n              draw_quantiles = 0.5) + \n  # add a boxplot\n  geom_boxplot(width = 0.25, \n               fill = \"white\", \n               alpha = 0.75, \n               outlier.alpha = 0) + \n  # add a point that represents the mean\n  stat_summary(fun = mean, \n               geom = \"point\", \n               size = 2) + \n  ggtitle(\"ViolinBox\")\n\n\n\nFigure 3.15: Violin plots combined with different methods to represent means and medians.\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nA very common type of plot is to produce a bar chart of means, however, the example below demonstrates just how misleading this is. It communicates the mean value for each category, but the bars hide the distribution of the actual data. You can’t tell if most wait times are close to 3 minutes, or spread from 0 to 6 minutes, or if the vast majority are less than 2 minutes, but the mean is pulled up by some very high outliers.\n\n\n\n\nFigure 3.16: Don’t plot continuous data with column plots. They are only appropriate for count data.\n\n\n\n\nColumn plots can also be very misleading. The plot on the left starts the y-axis at 0, which makes the bar heights proportional, showing almost no difference in average wait times. Since the differences are hard to see, you may be tempted to start the y-axis higher, but that makes it look like the average wait time for returns is double that for tech.\n\n\nTest your understanding\n\n\n\n\nFigure 3.17: ?(caption)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nHow would you create plot A? \ngeom_box()\ngeom_boxplot()\ngeom_violin()\ngeom_violinplot()\n\nHow would you create plot B? \ngeom_box()\ngeom_boxplot()\ngeom_violin()\ngeom_violinplot()\n\nWhat does the mapping look like for both plots?\n\naes(x = employee_id, y = call_time, fill = employee_id)aes(x = employee_id, y = call_time, colour = call_time)aes(x = employee_id, y = call_time, fill = call_time)aes(x = employee_id, y = call_time, colour = employee_id)\n\n\nWhich employee tends to have the longest calls? \ne01\ne02\ne03\ne04\ne05\ne06\ne07\ne08\ne09\ne10\n\nWhich employee has the record longest call? \ne01\ne02\ne03\ne04\ne05\ne06\ne07\ne08\ne09\ne10\n\n\n\n\n\n3.4.4 Two continuous variables\nWhen you want to see how two continuous variables are related, set one as the x-axis and the other as the y-axis. Usually, if one variable causes the other, you plot the cause on the x-axis and the effect on the y-axis. Here, we want to see if longer wait times cause the calls to be longer.\nScatterplot\nThe function to create a scatterplot is called geom_point().\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point()\n\n\n\nFigure 3.18: Scatterplot with geom_point().\n\n\n\n\nTrendlines\nIn 1, we emphasised the relationship between wait time and call time with a trendline created by geom_smooth() using the argument method = lm (“lm” stands for “linear model” or a straight line relationship). You can also set method = loess to visualise a non-linear relationship.\n\nlm_plot <- \n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = lm, formula = y~x) +\n  ggtitle(\"method = lm\")\n\nloess_plot <- \n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = loess, formula = y~x) +\n  ggtitle(\"method = loess\")\n\nlm_plot + loess_plot\n\n\n\nFigure 3.19: Different ways to show the relationship between two continuous variables.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf there isn’t much data at the extremes of the x-axis, the curve can be very uncertain. This is represented by the wider shaded area, which means that the true relationship might be anywhere within that area. Add the argument se = FALSE to geom_smooth() to remove this “standard error” shading.\n\n\nDates\nThe call_start column contains both a date and a time, so use the date() function from lubridate` to convert it to just a date. We’ll need it in this format to be able to transform the x-axis below.\n\nggplot(survey_data, aes(x = lubridate::date(call_start), \n                        y = satisfaction)) + \n  geom_smooth(method = lm, formula = y~x)\n\n\n\n\n\n\n\nWe can use scale_x_date() to set the date_breaks to be “1 month” apart. The date_labels argument uses a code for different date formats; you can see the full list of possibilities in the help for ?strptime. For example, %b means “Abbreviated month name”, whilst if you wanted to use a format like “2020/01/31” you could try \"%Y/%m/%d\".\n\nggplot(survey_data, aes(x = lubridate::date(call_start), \n                        y = satisfaction)) +\n  geom_smooth(method = lm, formula = y~x) +\n  scale_x_date(name = \"\",\n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  scale_y_continuous(name = \"Caller Satisfaction\") +\n  ggtitle(\"2020 Caller Satisfaction\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt looks like customer satisfaction declined across the year, but is this change meaningful? See what the plot looks like when the y-axis spans the full range of possible satisfaction values from 1 to 5. You can also plot the individual data points to emphasise the range of values.\n\n\n\nSolution\n\nggplot(survey_data, aes(x = lubridate::date(call_start), \n                        y = satisfaction)) +\n  # show individual data, jitter the height to avoid overlap\n  geom_jitter(width = 0, height = .1, alpha = 0.2) + \n  geom_smooth(method = lm,  formula = y~x) +\n  scale_x_date(name = \"\",\n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  scale_y_continuous(name = \"Caller Satisfaction\",\n                     breaks = 1:5) +\n  coord_cartesian(ylim = c(1, 5)) + # changes limits\n  ggtitle(\"2020 Caller Satisfaction\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.5 Ordinal variables\nWhen you have a limited range of numeric values, such as an ordinal rating scale, sometimes overlapping data makes it difficult to see what is going on in a point plot. For example, the plot below shows satisfaction ratings by call time but because all the ratings are 1, 2, 3, 4 or 5, it makes it hard to see exactly how many data points there are at each point.\nIn this section, we’ll explore a few options for dealing with this.\n\nggplot(survey_data, aes(x = call_time, y = satisfaction)) + \n  geom_point()\n\n\n\nFigure 3.20: Overlapping data.\n\n\n\n\nJitter plot\nYou can use geom_jitter() to move the points around a bit to make them easier to see. You can also set alpha transparency. Here, the x-axis is continuous, so there is no need to jitter the width, but the y-axis is ordinal categories, so the height is jittered between -0.2 and +0.2 away from the true y-value. It can help to play around with these values to understand what the jitter is doing.\n\nggplot(survey_data, aes(x = call_time, y = satisfaction)) +\n  geom_jitter(width = 0, height = .2, alpha = 0.5)\n\n\n\nFigure 3.21: Jitter plot.\n\n\n\n\nFacets\nAlternatively, you can use facet_wrap() to create a separate plot for each level of satisfaction. facet_wrap() uses the tilde (~) symbol, which you can roughly translate as “by”, e.g., facet the plot by satisfaction rating. The labeller function controls the labels above each plot. label_both specifies that we want both the variable name (satisfaction) and the value (e.g., 1) printed on the plot to make it easier to read.\n\nggplot(survey_data, aes(x = call_time)) +\n  geom_histogram(binwidth = 10, \n                 boundary = 0, \n                 fill = \"dodgerblue\", \n                 color = \"black\") +\n  facet_wrap(~satisfaction, \n             ncol = 1, # try changing this to 2 \n             labeller = label_both) +\n  scale_x_continuous(name = \"Call Time (seconds)\",\n                     breaks = seq(0, 600, 30))\n\n\n\nFigure 3.22: A histogram with facets.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are not, by any means, all the plot types that you can make in R. This chapter just gave you a basic overview, and we will go into more detail in Section 10.1). The further resources section at the end of this chapter lists many resources, but the R Graph Gallery is an especially useful one to get inspiration for the kinds of beautiful plots you can make in R."
  },
  {
    "objectID": "03-viz.html#exercises",
    "href": "03-viz.html#exercises",
    "title": "3  Data Visualisation",
    "section": "\n3.5 Exercises",
    "text": "3.5 Exercises\nFor the final step in this chapter, we will create a report of data visualisations. You may need to refer back to Chapter 2) to help you complete these exercises and you may also want to take a break before you work through this section. We’d also recommend you knit at every step so that you can see how your output changes.\n\n3.5.1 New Markdown\nCreate and save a new R Markdown document named plots_report.Rmd and give it the title “Customer satisfaction report”. Remove the default template text and then load the packages and code below in the set-up code chunk:\n\nlibrary(tidyverse) \nlibrary(patchwork) \nlibrary(ggthemes)  \nlibrary(lubridate) \nlibrary(knitr)\nlibrary(kableExtra)\n\nsurvey_data <- read_csv(\"https://psyteachr.github.io/ads-v1/data/survey_data.csv\")\n\n\n3.5.2 Summary\nCreate a level 1 heading titled “Overview”. Underneath this heading, write a short summary of what the data set contains and what each of the variables means (you can use the information from Section 3.3.1 if you like).\n\n3.5.3 Presenting plots\nPick your two favourites plots from all the examples we’ve made in this chapter. For each plot:\n\nCreate a level 2 heading in your R Markdown document and give it an informative title.\nWrite a short summary that interprets the data shown in the plots - it’s not enough just to present visualisations, effective reports will also help the reader understand the conclusions they should draw from the plots you’ve presented.\nLook through the different themes available with ggtheme and choose one to apply to your plots.\nMake sure each plot has a figure caption (either by adding this to the ggplot() code or adding it to the code chunk options).\nOrganise your Markdown so that the plots are shown after the text summary.\n\n3.5.4 Combining plots\nNow, pick your two least favourite plots:\n\nAdjust the visual aesthetics to make them look as bad and as difficult to read as possible.\nCombine the plots using patchwork functions.\nWrite a short summary that explains why you think these plots are so bad.\nOrganise your Markdown so that the plots are shown after the text summary.\n\n3.5.5 Editing your Markdown display\nAdjust the set-up of your Markdown so that your knitted report does not show any code, messages, or warnings, and all figures are 8 x 5 (see Section 2.6.1).\n\n3.5.6 Change the output\nSo far we’ve just knitted to html. To generate PDF reports, you need to install tinytex(Xie, 2021) and run the following code in the console (do not add this to your Rmd file):\n\n# run this in the console\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nOnce you’ve done this, update your YAML heading to include the following:\n---\ntitle: \"Customer Satisfaction Report\"\nauthor: \"Your name\"\noutput:\n  pdf_document:\n---\nThis will knit a PDF document. You will likely encounter errors - knitting to pdf really is the seventh circle of hell. If this happens, ask on Teams for help.\nAs an alternative, you can also knit to a Word document.\n---\ntitle: \"Customer Satisfaction Report\"\nauthor: \"Your name\"\noutput:\n  word_document:\n---\n\n3.5.7 Share your work\nOnce you’ve completed this activity, post it in the Week 3 channel on Teams so that you can compare which plots you chose and visual style with other learners on the course."
  },
  {
    "objectID": "03-viz.html#sec-glossary-viz",
    "href": "03-viz.html#sec-glossary-viz",
    "title": "3  Data Visualisation",
    "section": "\n3.6 Glossary",
    "text": "3.6 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nargument\n\n\nA variable that provides input to a function.\n\n\n\n\ncategorical\n\n\nData that can only take certain values, such as types of pet.\n\n\n\n\ncharacter\n\n\nA data type representing strings of text.\n\n\n\n\nchunk\n\n\nA section of code in an R Markdown file\n\n\n\n\ncontinuous\n\n\nData that can take on any values between other existing values.\n\n\n\n\ndata type\n\n\nThe kind of data represented by an object.\n\n\n\n\ndefault value\n\n\nA value that a function uses for an argument if it is skipped.\n\n\n\n\ndouble\n\n\nA data type representing a real decimal number\n\n\n\n\nfactor\n\n\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\n\n\ngeom\n\n\nThe geometric style in which data are displayed, such as boxplot, density, or histogram.\n\n\n\n\ninteger\n\n\nA data type representing whole numbers.\n\n\n\n\nknit\n\n\nTo create an HTML, PDF, or Word document from an R Markdown (Rmd) document\n\n\n\n\nlikert\n\n\nA rating scale with a small number of discrete points in order\n\n\n\n\nlogical\n\n\nA data type representing TRUE or FALSE values.\n\n\n\n\nmedian\n\n\nThe middle number in a distribution where half of the values are larger and half are smaller.\n\n\n\n\nnominal\n\n\nCategorical variables that don't have an inherent order, such as types of animal.\n\n\n\n\nnumeric\n\n\nA data type representing a real decimal number or integer.\n\n\n\n\nobservation\n\n\nAll of the data about a single trial or question.\n\n\n\n\nordinal\n\n\nDiscrete variables that have an inherent order, such as level of education or dislike/like.\n\n\n\n\noutlier\n\n\nA data point that is extremely distant from most of the other data points\n\n\n\n\nr markdown\n\n\nThe R-specific version of markdown: a way to specify formatting, such as headers, paragraphs, lists, bolding, and links, as well as code blocks and inline code.\n\n\n\n\nstring\n\n\nA piece of text inside of quotes.\n\n\n\n\ntidy data\n\n\nA format for data that maps the meaning onto the structure.\n\n\n\n\nvalue\n\n\nA single number or piece of data.\n\n\n\n\nvariable\n\n\n(coding): A word that identifies and stores the value of some data for later use; (stats): An attribute or characteristic of an observation that you can measure, count, or describe\n\n\n\n\nvector\n\n\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings."
  },
  {
    "objectID": "03-viz.html#sec-resources-viz",
    "href": "03-viz.html#sec-resources-viz",
    "title": "3  Data Visualisation",
    "section": "\n3.7 Further Resources",
    "text": "3.7 Further Resources\n\nggplot2 cheat sheet\n\nData visualisation using R, for researchers who don’t use R (Nordmann et al., 2021)\n\n\nChapter 3: Data Visualisation of R for Data Science\n\nggplot2 FAQs\nggplot2 documentation\n\nHack Your Data Beautiful workshop by University of Glasgow postgraduate students\n\nChapter 28: Graphics for communication of R for Data Science\n\n\ngganimate: A package for making animated plots\n\n\n\n\n\nNordmann, E., McAleer, P., Toivo, W., Paterson, H., & DeBruine, L. M. (2021). Data visualisation using R, for researchers who don’t use R. PsyArXiv. https://doi.org/10.31234/osf.io/4huvw\n\n\nXie, Y. (2021). Tinytex: Helper functions to install and maintain TeX live, and compile LaTeX documents. https://github.com/yihui/tinytex"
  },
  {
    "objectID": "04-data.html#sec-ilo-data",
    "href": "04-data.html#sec-ilo-data",
    "title": "4  Data Import",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to inspect data\nBe able to import data from a range of sources\nBe able to identify and handle common problems with data import"
  },
  {
    "objectID": "04-data.html#sec-walkthrough-data",
    "href": "04-data.html#sec-walkthrough-data",
    "title": "4  Data Import",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "04-data.html#sec-setup-data",
    "href": "04-data.html#sec-setup-data",
    "title": "4  Data Import",
    "section": "\n4.1 Set-up",
    "text": "4.1 Set-up\nCreate a new project for the work we’ll do in this chapter named 04-data. Then, create and save a new R Markdown document named data.Rmd, get rid of the default template text, and load the packages in the set-up code chunk. You should have all of these packages installed already, but if you get the message Error in library(x) : there is no package called ‘x’, please refer to Section 1.3.1.\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)     # includes readr & tibble\nlibrary(rio)           # for almost any data import/export\nlibrary(haven)         # for SPSS, Stata,and SAS files\nlibrary(readxl)        # for Excel files\nlibrary(googlesheets4) # for Google Sheets\n```\n\n\nWe’d recommend making a new code chunk for each different activity, and using the white space to make notes on any errors you make, things you find interesting, or questions you’d like to ask the course team.\nDownload the Data import cheatsheet."
  },
  {
    "objectID": "04-data.html#sec-builtin",
    "href": "04-data.html#sec-builtin",
    "title": "4  Data Import",
    "section": "\n4.2 Built-in data",
    "text": "4.2 Built-in data\nYou’ll likely want to import you own data to work with, however, Base R also comes with built-in datasets and these can be very useful for learning new functions and packages. Additionally, some packages, like tidyr, also contain data. The data() function lists the datasets available.\n\n# list datasets built in to base R\ndata()\n\n# lists datasets in a specific package\ndata(package = \"tidyr\")\n\nType the name of a dataset into the console to see the data. For example, type ?table1 into the console to see the dataset description for table1, which is a dataset included with tidyr.\n\n?table1\n\nYou can also use the data() function to load a dataset into your global environment.\n\n# loads table1 into the environment\ndata(\"table1\")"
  },
  {
    "objectID": "04-data.html#looking-at-data",
    "href": "04-data.html#looking-at-data",
    "title": "4  Data Import",
    "section": "\n4.3 Looking at data",
    "text": "4.3 Looking at data\nNow that you’ve loaded some data, look the upper right hand window of RStudio, under the Environment tab. You will see the object table1 listed, along with the number of observations (rows) and variables (columns). This is your first check that everything went OK.\nAlways, always, always, look at your data once you’ve created or loaded a table. Also look at it after each step that transforms your table. There are three main ways to look at your table: View(), print(), tibble::glimpse().\n\n4.3.1 View()\nA familiar way to look at the table is given by View() (uppercase ‘V’), which opens up a data table in the console pane using a viewer that looks a bit like Excel. This command can be useful in the console, but don’t ever put this one in a script because it will create an annoying pop-up window when the user runs it. You can also click on an object in the environment pane to open it in the same interface. You can close the tab when you’re done looking at it; it won’t remove the object.\n\nView(table1)\n\n\n4.3.2 print()\nThe print() method can be run explicitly, but is more commonly called by just typing the variable name on a blank line. The default is not to print the entire table, but just the first 10 rows.\nLet’s look at the table1 table that we loaded above. Depending on how wide your screen is, you might need to click on an arrow at the right of the table to see the last column.\n\n# call print explicitly\nprint(table1)\n\n# more common method of just calling object name\ntable1\n\n\n\n\n\n  \n\n\n\n\n4.3.3 glimpse()\nThe function tibble::glimpse() gives a sideways version of the table. This is useful if the table is very wide and you can’t easily see all of the columns. It also tells you the data type of each column in angled brackets after each column name.\n\nglimpse(table1)\n\nRows: 6\nColumns: 4\n$ country    <chr> \"Afghanistan\", \"Afghanistan\", \"Brazil\", \"Brazil\", \"China\", …\n$ year       <int> 1999, 2000, 1999, 2000, 1999, 2000\n$ cases      <int> 745, 2666, 37737, 80488, 212258, 213766\n$ population <int> 19987071, 20595360, 172006362, 174504898, 1272915272, 12804…\n\n\n\n4.3.4 summary()\nYou can get a quick summary of a dataset with the summary() function, which can be useful for spotting things like if the minimum or maximum values are clearly wrong, or if R thinks that a nominal variable is numeric. For example, if you had labelled gender as 1, 2, and 3 rather than male, female, and non-binary, summary() would calculate a mean and median even though this isn’t appropriate for the data. This can be a useful flag that you need to take further steps to correct your data.\nNote that because population is a very, very large number, R will use scientific notation.\n\nsummary(table1)\n\n   country               year          cases          population       \n Length:6           Min.   :1999   Min.   :   745   Min.   :1.999e+07  \n Class :character   1st Qu.:1999   1st Qu.: 11434   1st Qu.:5.845e+07  \n Mode  :character   Median :2000   Median : 59112   Median :1.733e+08  \n                    Mean   :2000   Mean   : 91277   Mean   :4.901e+08  \n                    3rd Qu.:2000   3rd Qu.:179316   3rd Qu.:9.983e+08  \n                    Max.   :2000   Max.   :213766   Max.   :1.280e+09"
  },
  {
    "objectID": "04-data.html#sec-import_data",
    "href": "04-data.html#sec-import_data",
    "title": "4  Data Import",
    "section": "\n4.4 Importing data",
    "text": "4.4 Importing data\nBuilt-in data are nice for examples, but you’re probably more interested in your own data. There are many different types of files that you might work with when doing data analysis. These different file types are usually distinguished by the three-letter extension following a period at the end of the file name (e.g., .xls).\nDownload this directory of data files, unzip the folder, and save the data directory in the 04-data project directory.\n\n4.4.1 rio::import()\nThe type of data files you have to work with will likely depend on the software that you typically use in your workflow. The rio package has very straightforward functions for reading and saving data in most common formats: rio::import() and rio::export().\n\ndemo_tsv  <- import(\"data/demo.tsv\")  # tab-separated values\ndemo_csv  <- import(\"data/demo.csv\")  # comma-separated values\ndemo_xls  <- import(\"data/demo.xlsx\") # Excel format\ndemo_sav  <- import(\"data/demo.sav\")  # SPSS format\n\n\n4.4.2 File type specific import\nHowever, it is also useful to know the specific functions that are used to import different file types because it is easier to discover features to deal with complicated cases, such as when you need to skip rows, rename columns, or choose which Excel sheet to use.\n\ndemo_tsv <- readr::read_tsv(\"data/demo.tsv\")\ndemo_csv <- readr::read_csv(\"data/demo.csv\")\ndemo_xls <- readxl::read_excel(\"data/demo.xlsx\")\ndemo_sav <- haven::read_sav(\"data/demo.sav\")\n\n\n\n\n\n\n\nNote\n\n\n\nLook at the help for each function above and read through the Arguments section to see how you can customise import.\n\n\nIf you keep data in Google Sheets, you can access it directly from R using googlesheets4”, “https://googlesheets4.tidyverse.org/”). The code below imports data from a [public sheet](https://docs.google.com/spreadsheets/d/16dkq0YL0J7fyAwT1pdgj1bNNrheckAU_2-DKuuM6aGI){target=\"_blank\"}. You can set thess` argument to the entire URL for the target sheet, or just the section after “https://docs.google.com/spreadsheets/d/”.\n\ngs4_deauth() # skip authorisation for public data\n\ndemo_gs4  <- googlesheets4::read_sheet(\n  ss = \"16dkq0YL0J7fyAwT1pdgj1bNNrheckAU_2-DKuuM6aGI\"\n)\n\n\n4.4.3 Column data types\nUse glimpse() to see how these different functions imported the data with slightly different data types. This is because the different file types store data slightly differently. For example, SPSS stores factors as numbers, so the factor column contains the values 1, 2, 3 rather than low, med, high. It also stores logical values as 0 and 1 instead or TRUE and FALSE.\n\nglimpse(demo_csv)\n\nRows: 6\nColumns: 6\n$ character <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    <chr> \"high\", \"low\", \"med\", \"high\", \"low\", \"med\"\n$ integer   <dbl> 1, 2, 3, 4, 5, 6\n$ double    <dbl> 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   <lgl> TRUE, TRUE, FALSE, FALSE, NA, TRUE\n$ date      <date> 2022-12-21, 2022-12-20, 2022-12-19, 2022-12-18, 2022-12-17, …\n\n\n\nglimpse(demo_xls)\n\nRows: 6\nColumns: 6\n$ character <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    <chr> \"high\", \"low\", \"med\", \"high\", \"low\", \"med\"\n$ integer   <dbl> 1, 2, 3, 4, 5, 6\n$ double    <dbl> 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   <lgl> TRUE, TRUE, FALSE, FALSE, NA, TRUE\n$ date      <dttm> 2022-12-21, 2022-12-20, 2022-12-19, 2022-12-18, 2022-12-17, …\n\n\n\nglimpse(demo_sav)\n\nRows: 6\nColumns: 6\n$ character <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    <dbl+lbl> 3, 1, 2, 3, 1, 2\n$ integer   <dbl> 1, 2, 3, 4, 5, 6\n$ double    <dbl> 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   <dbl> 1, 1, 0, 0, NA, 1\n$ date      <date> 2022-12-21, 2022-12-20, 2022-12-19, 2022-12-18, 2022-12-17, …\n\n\n\nglimpse(demo_gs4)\n\nRows: 6\nColumns: 6\n$ character <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    <chr> \"high\", \"low\", \"med\", \"high\", \"low\", \"med\"\n$ integer   <dbl> 1, 2, 3, 4, 5, 6\n$ double    <dbl> 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   <lgl> TRUE, TRUE, FALSE, FALSE, NA, TRUE\n$ date      <dttm> 2021-11-22, 2021-11-21, 2021-11-20, 2021-11-19, 2021-11-18, …\n\n\nThe readr functions display a message when you import data explaining what data type each column is.\n\ndemo <- readr::read_csv(\"data/demo.csv\")\n\nRows: 6 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): character, factor\ndbl  (2): integer, double\nlgl  (1): logical\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe “Column specification” tells you which data type each column is. You can review data types in Appendix G. Options are:\n\n\nchr: character\n\n\ndbl: double\n\n\nlgl: logical\n\n\nint: integer\n\n\ndate: date\n\ndttm: date/time\n\nread_csv() will guess what type of data each variable is and normally it is pretty good at this. However, if it makes a mistake, such as reading the “date” column as a character, you can manually set the column data types.\nFirst, run spec() on the dataset which will give you the full column specification that you can copy and paste:\n\nspec(demo)\n\ncols(\n  character = col_character(),\n  factor = col_character(),\n  integer = col_double(),\n  double = col_double(),\n  logical = col_logical(),\n  date = col_date(format = \"\")\n)\n\n\nThen, we create an object using the code we just copied that lists the correct column types. Factor columns will always import as character data types, so you have to set their data type manually with col_factor() and set the order of levels with the levels argument. Otherwise, the order defaults to the order they appear in the dataset. For our demo dataset, we will tell R that the factor variable is a factor by using col_factor() and we can also specify the order of the levels so that they don’t just appear alphabetically. Additionally, we can also specify exactly what format our date variable is in using %Y-%m-%d.\nWe then save this column specification to an object, and then add this to the col_types argument when we call read_csv().\n\ncorrected_cols <- cols(\n  character = col_character(),\n  factor = col_factor(levels = c(\"low\", \"med\", \"high\")),\n  integer = col_integer(),\n  double = col_double(),\n  logical = col_logical(),\n  date = col_date(format = \"%Y-%m-%d\")\n)\n\ndemo <- readr::read_csv(\"data/demo.csv\", col_types = corrected_cols)\n\n\n\n\n\n\n\nNote\n\n\n\nFor dates, you might need to set the format your dates are in. See ?strptime for a list of the codes used to represent different date formats. For example, \"%d-%b-%y\" means that the dates are formatted like 31-Jan-21.\n\n\nThe functions from readxl for loading .xlsx sheets have a different, more limited way to specify the column types. You will have to convert factor columns and dates using mutate(), which you’ll learn about in Chapter 9, so most people let read_excel() guess data types and don’t set the col_types argument.\nFor SPSS data, whilst rio::import() will just read the numeric values of factors and not their labels, the function read_sav() from haven reads both. However, you have to convert factors from a haven-specific “labelled double” to a factor (we have no idea why haven doesn’t do this for you).\n\ndemo_sav$factor <- haven::as_factor(demo_sav$factor)\n\nglimpse(demo_sav)\n\nRows: 6\nColumns: 6\n$ character <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    <fct> high, low, med, high, low, med\n$ integer   <dbl> 1, 2, 3, 4, 5, 6\n$ double    <dbl> 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   <dbl> 1, 1, 0, 0, NA, 1\n$ date      <date> 2022-12-21, 2022-12-20, 2022-12-19, 2022-12-18, 2022-12-17, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe way you specify column types for googlesheets4 is a little different from readr, although you can also use the shortcodes described in the help for read_sheet() with readr functions. There is currently no column specification for factors."
  },
  {
    "objectID": "04-data.html#creating-data",
    "href": "04-data.html#creating-data",
    "title": "4  Data Import",
    "section": "\n4.5 Creating data",
    "text": "4.5 Creating data\nIf you need to create a small data table from scratch in R, use the tibble::tibble() function, and type the data right in. The tibble package is part of the tidyverse package that we loaded at the start of this chapter.\nLet’s create a small table with the names of three Avatar characters and their bending type. The tibble() function takes arguments with the names that you want your columns to have. The values are vectors that list the column values in order.\nIf you don’t know the value for one of the cells, you can enter NA, which we have to do for Sokka because he doesn’t have any bending ability. If all the values in the column are the same, you can just enter one value and it will be copied for each row.\n\navatar <- tibble(\n  name = c(\"Katara\", \"Toph\", \"Sokka\"),\n  bends = c(\"water\", \"earth\", NA),\n  friendly = TRUE\n)\n\n# print it\navatar\n\n\n\n  \n\n\n\nYou can also use the tibble::tribble() function to create a table by row, rather than by column. You start by listing the column names, each preceded by a tilde (~), then you list the values for each column, row by row, separated by commas (don’t forget a comma at the end of each row).\n\navatar_by_row <- tribble(\n  ~name,    ~bends,  ~friendly,\n  \"Katara\", \"water\", TRUE,\n  \"Toph\",   \"earth\", TRUE,\n  \"Sokka\",  NA,      TRUE\n)\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t have to line up the columns in a tribble, but it can make it easier to spot errors.\n\n\nYou may not need to do this very often if you are primarily working with data that you import from spreadsheets, but it is useful to know how to do it anyway."
  },
  {
    "objectID": "04-data.html#writing-data",
    "href": "04-data.html#writing-data",
    "title": "4  Data Import",
    "section": "\n4.6 Writing data",
    "text": "4.6 Writing data\nIf you have data that you want to save, use rio::export(), as follows.\n\nexport(avatar, \"data/avatar.csv\")\n\nThis will save the data in CSV format to your working directory.\nWriting to Google Sheets is a little trickier (if you never use Google Sheets feel free to skip this section). Even if a Google Sheet is publicly editable, you can’t add data to it without authorising your account.\nYou can authorise interactively using the following code (and your own email), which will prompt you to authorise “Tidyverse API Packages” the first time you do this. If you don’t tick the checkbox authorising it to “See, edit, create, and delete all your Google Sheets spreadsheets”, the next steps will fail.\n\n# authorise your account \n# this only needs to be done once per script\ngs4_auth(email = \"myemail@gmail.com\")\n\n# create a new sheet\nsheet_id <- gs4_create(name = \"demo-file\", \n                       sheets = \"letters\")\n\n# define the data table to save\nletter_data <- tibble(\n  character = LETTERS[1:5],\n  integer = 1:5,\n  double = c(1.1, 2.2, 3.3, 4.4, 5.5),\n  logical = c(T, F, T, F, T),\n  date = lubridate::today()\n)\n\nwrite_sheet(data = letter_data, \n            ss = sheet_id, \n            sheet = \"letters\")\n\n## append some data\nnew_data <- tibble(\n  character = \"F\",\n  integer = 6L,\n  double = 6.6,\n  logical = FALSE,\n  date = lubridate::today()\n)\nsheet_append(data = new_data,\n             ss = sheet_id,\n             sheet = \"letters\")\n\n# read the data\ndemo <- read_sheet(ss = sheet_id, sheet = \"letters\")\n\n\n\n\n\n\n\nNote\n\n\n\n\nCreate a new table called family with the first name, last name, and age of your family members (biological, adopted, or chosen).\nSave it to a CSV file called “family.csv”.\nClear the object from your environment by restarting R or with the code remove(family).\nLoad the data back in and view it.\n\n\n\n\nSolution\n\n# create the table\nfamily <- tribble(\n  ~first_name, ~last_name, ~age,\n  \"Lisa\", \"DeBruine\", 45,\n  \"Robbie\", \"Jones\", 14\n)\n\n# save the data to CSV\nexport(family, \"data/family.csv\")\n\n# remove the object from the environment\nremove(family)\n\n# load the data\nfamily <- import(\"data/family.csv\")\n\n\n\n\nWe’ll be working with tabular data a lot in this class, but tabular data is made up of vectors, which groups together data with the same basic data type. Appendix G explains some of this terminology to help you understand the functions we’ll be learning to process and analyse data."
  },
  {
    "objectID": "04-data.html#troubleshooting",
    "href": "04-data.html#troubleshooting",
    "title": "4  Data Import",
    "section": "\n4.7 Troubleshooting",
    "text": "4.7 Troubleshooting\nWhat if you import some data and it guesses the wrong column type? The most common reason is that a numeric column has some non-numbers in it somewhere. Maybe someone wrote a note in an otherwise numeric column. Columns have to be all one data type, so if there are any characters, the whole column is converted to character strings, and numbers like 1.2 get represented as \"1.2\", which will cause very weird errors like \"100\" < \"9\" == TRUE. You can catch this by using glimpse() to check your data.\nThe data directory you downloaded contains a file called “mess.csv”. Let’s try loading this dataset.\n\nmess <- rio::import(\"data/mess.csv\")\n\nWarning in (function (input = \"\", file = NULL, text = NULL, cmd = NULL, :\nStopped early on line 5. Expected 7 fields but found 0. Consider fill=TRUE\nand comment.char=. First discarded non-empty line: <<junk,missing,0.72,b,1,2 -\n3,2020-01-2>>\n\n\nWhen importing goes wrong, it’s often easier to fix it using the specific importing function for that file type (e.g., use read_csv() rather than rio::import(). This is because the problems tend to be specific to the file format and you can look up the help for these functions more easily. For CSV files, the import function is readr::read_csv.\n\n# lazy = FALSE loads the data right away so you can see error messages\n# this default changed in late 2021 and might change back soon\nmess <- read_csv(\"data/mess.csv\", lazy = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 27 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): This is my messy dataset\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nYou’ll get a warning about parsing issues and the data table is just a single column. View the file data/mess.csv by clicking on it in the File pane, and choosing “View File”. Here are the first 10 lines. What went wrong?\nThis is my messy dataset\n\njunk,order,score,letter,good,min_max,date\njunk,1,-1,a,1,1 - 2,2020-01-1\n\njunk,missing,0.72,b,1,2 - 3,2020-01-2\n\njunk,3,-0.62,c,FALSE,3 - 4,2020-01-3\n\njunk,4,2.03,d,T,4 - 5,2020-01-4\nFirst, the file starts with a note: “This is my messy dataset” and then a blank line. The first line of data should be the column headings, so we want to skip the first two lines. You can do this with the argument skip in read_csv().\n\nmess <- read_csv(\"data/mess.csv\", \n                 skip = 2,\n                 lazy = FALSE)\n\nRows: 26 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): junk, order, letter, good, min_max, date\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(mess)\n\nRows: 26\nColumns: 7\n$ junk    <chr> \"junk\", \"junk\", \"junk\", \"junk\", \"junk\", \"junk\", \"junk\", \"junk\"…\n$ order   <chr> \"1\", \"missing\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",…\n$ score   <dbl> -1.00, 0.72, -0.62, 2.03, NA, 0.99, 0.03, 0.67, 0.57, 0.90, -1…\n$ letter  <chr> \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m…\n$ good    <chr> \"1\", \"1\", \"FALSE\", \"T\", \"1\", \"0\", \"T\", \"TRUE\", \"1\", \"T\", \"F\", …\n$ min_max <chr> \"1 - 2\", \"2 - 3\", \"3 - 4\", \"4 - 5\", \"5 - 6\", \"6 - 7\", \"7 - 8\",…\n$ date    <chr> \"2020-01-1\", \"2020-01-2\", \"2020-01-3\", \"2020-01-4\", \"2020-01-5…\n\n\nOK, that’s a little better, but this table is still a serious mess in several ways:\n\n\njunk is a column that we don’t need\n\norder should be an integer column\n\ngood should be a logical column\n\ngood uses all kinds of different ways to record TRUE and FALSE values\n\nmin_max contains two pieces of numeric information, but is a character column\n\ndate should be a date column\n\nWe’ll learn how to deal with this mess in Chapter 8 and Chapter 9, but we can fix a few things by setting the col_types argument in read_csv() to specify the column types for our two columns that were guessed wrong and skip the “junk” column. The argument col_types takes a list where the name of each item in the list is a column name and the value is from the table below. You can use the function, like col_double() or the abbreviation, like \"d\"; for consistency with earlier in this chapter we will use the function names. Omitted column names are guessed.\n\n\nfunction\n\nabbreviation\n\n\n\ncol_logical()\nl\nlogical values\n\n\ncol_integer()\ni\ninteger values\n\n\ncol_double()\nd\nnumeric values\n\n\ncol_character()\nc\nstrings\n\n\ncol_factor(levels, ordered)\nf\na fixed set of values\n\n\ncol_date(format = ““)\nD\nwith the locale’s date_format\n\n\ncol_time(format = ““)\nt\nwith the locale’s time_format\n\n\ncol_datetime(format = ““)\nT\nISO8601 date time\n\n\ncol_number()\nn\nnumbers containing the grouping_mark\n\n\ncol_skip()\n_, -\ndon’t import this column\n\n\ncol_guess()\n?\nparse using the “best” type based on the input\n\n\n\n\n# omitted values are guessed\n# ?col_date for format options\nct <- cols(\n  junk = col_skip(), # skip this column\n  order = col_integer(),\n  good = col_logical(),\n  date = col_date(format = \"%Y-%m-%d\")\n)\n\ntidier <- read_csv(\"data/mess.csv\", \n                   skip = 2,\n                   col_types = ct,\n                   lazy = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nYou will get a message about parsing issues when you run this that tells you to run the problems() function to see a table of the problems. Warnings look scary at first, but always start by reading the message.\n\nproblems()\n\n\n\n\n\n  \n\n\n\nThe output of problems() tells you what row (3) and column (2) the error was found in, what kind of data was expected (an integer), and what the actual value was (missing). If you specifically tell read_csv() to import a column as an integer, any characters (i.e., not numbers) in the column will produce a warning like this and then be recorded as NA. You can manually set what missing values are recorded as with the na argument.\n\ntidiest <- read_csv(\"data/mess.csv\", \n                   skip = 2,\n                   na = \"missing\",\n                   col_types = ct,\n                   lazy = FALSE)\n\nNow order is an integer variable where any empty cells contain NA. The variable good is a logical value, where 0 and F are converted to FALSE, while 1 and T are converted to TRUE. The variable date is a date type (adding leading zeros to the day). We’ll learn in later chapters how to fix other problems, such as the min_max column containing two different types of data."
  },
  {
    "objectID": "04-data.html#working-with-real-data",
    "href": "04-data.html#working-with-real-data",
    "title": "4  Data Import",
    "section": "\n4.8 Working with real data",
    "text": "4.8 Working with real data\nIt’s worth highlighting at this point that working with real data can be difficult because each dataset can be messy in its own way. Throughout this course we will show you common errors and how to fix them, but be prepared that when you start with working your own data, you’ll likely come across problems we don’t cover in the course and that’s just part of joy of learning programming. You’ll also get better at looking up solutions using sites like Stack Overflow and there’s a fantastic #rstats community on Twitter you can ask for help.\nYou may also be tempted to fix messy datasets by, for example, opening up Excel and editing them there. Whilst this might seem easier in the short term, there’s two serious issues with doing this. First, you will likely work with datasets that have recurring messy problems. By taking the time to solve these problems with code, you can apply the same solutions to a large number of future datasets so it’s more efficient in the long run. Second, if you edit the spreadsheet, there’s no record of what you did. By solving these problems with code, you do so reproducibly and you don’t edit the original data file. This means that if you make an error, you haven’t lost the original data and can recover."
  },
  {
    "objectID": "04-data.html#exercises",
    "href": "04-data.html#exercises",
    "title": "4  Data Import",
    "section": "\n4.9 Exercises",
    "text": "4.9 Exercises\nFor the final step in this chapter, we will create a report using one of the in-built datasets to practice the skills you have used so far. You may need to refer back to previous chapters to help you complete these exercises and you may also want to take a break before you work through this section. We’d also recommend you knit at every step so that you can see how your output changes.\n\n4.9.1 New Markdown\nCreate and save a new R Markdown document named starwars_report.Rmd. In the set-up code chunk load the packages tidyverse and rio.\nWe’re going to use the built-in starwars dataset that contains data about Star Wars characters. You can learn more about the dataset by using the ?help function.\n\n4.9.2 Import and export the dataset\n\nFirst, load the in-built dataset into the environment. Type and run the code to do this in the console; do not save it in your Markdown.\n\nThen, export the dataset to a .csv file and save it in your data directory. Again, do this in the console.\nFinally, import this version of the dataset using read_csv() to an object named starwars - you can put this code in your Markdown.\n\n\n\nSolution\n\n\ndata(starwars)\nexport(starwars, \"data/starwars.csv\")\nstarwars <- read_csv(\"data/starwars.csv\")\n\n\n\n4.9.3 Convert column types\n\nCheck the column specification of starwars.\nCreate a new column specification that lists the following columns as factors: hair_color, skin_color, eye_color, sex, gender, homeworld, and species and skips the following columns: films, vehicles, and starships (this is because these columns contain multiple values and are stored as lists, which we haven’t covered how to work with). You do not have to set the factor orders (although you can if you wish).\nRe-import the dataset, this time with the corrected column types.\n\n\n\nSolution\n\n\nspec(starwars)\ncorrected_cols <- cols(\n  name = col_character(),\n  height = col_double(),\n  mass = col_double(),\n  hair_color = col_factor(),\n  skin_color = col_factor(),\n  eye_color = col_factor(),\n  birth_year = col_double(),\n  sex = col_factor(),\n  gender = col_factor(),\n  homeworld = col_factor(),\n  species = col_factor(),\n  films = col_skip(),\n  vehicles = col_skip(),\n  starships = col_skip()\n)\n\nstarwars <- read_csv(\"data/starwars.csv\", col_types = corrected_cols)\n\n\n\n4.9.4 Plots\nProduce the following plots and one plot of your own choosing. Write a brief summary of what each plot shows and any conclusions you might reach from the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\nggplot(starwars, aes(height)) +\n  geom_density(colour = \"black\", alpha = .3) +\n  scale_x_continuous(breaks = seq(from = 50, to = 300, by = 25)) +\n  labs(title = \"Height (cm) distribution of Star Wars Characters\") +\n  theme_classic()\n\n\nggplot(starwars, aes(mass)) +\n  geom_histogram(colour = \"black\", binwidth = 10) +\n  scale_x_continuous(breaks = seq(from = 0, to = 2000, by = 100)) +\n  labs(title = \"Weight (kg) distribution of Star Wars Characters\") +\n  theme_classic()\n\n\nggplot(starwars, aes(x = gender, fill = gender)) +\n  geom_bar(show.legend = FALSE, colour = \"black\") +\n  scale_x_discrete(name = \"Gender of character\", labels = (c(\"Masculine\", \"Feminine\", \"Missing\"))) +\n  scale_fill_brewer(palette = 2) +\n  labs(title = \"Number of Star Wars characters of each gender\") +\n  theme_bw()\n\n\n\n4.9.5 Make it look nice\n\nAdd at least one Star Wars related image from an online source\nHide the code and any messages from the knitted output\nResize any images as you see fit\n\n\n\nSolution\n\n\n\n```{r, echo = FALSE, out.width = \"50%\", fig.cap=\"Adaptation of Star Wars logo created by Weweje; original logo by Suzy Rice, 1976. CC-BY-3.0\"}\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Star_wars2.svg/2880px-Star_wars2.svg.png\")\n```\n\n\n\n\n\n\nAdaptation of Star Wars logo created by Weweje; original logo by Suzy Rice, 1976. CC-BY-3.0\n\n\n\n\n\n\n4.9.6 Share your work\nOnce you’re done, share your knitted html file on the Week 4 Teams channel so other learners on the course can see how you approached the task."
  },
  {
    "objectID": "04-data.html#sec-glossary-data",
    "href": "04-data.html#sec-glossary-data",
    "title": "4  Data Import",
    "section": "\n4.10 Glossary",
    "text": "4.10 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nargument\n\n\nA variable that provides input to a function.\n\n\n\n\ncharacter\n\n\nA data type representing strings of text.\n\n\n\n\nconsole\n\n\nThe pane in RStudio where you can type in commands and view output messages.\n\n\n\n\ndata type\n\n\nThe kind of data represented by an object.\n\n\n\n\ndouble\n\n\nA data type representing a real decimal number\n\n\n\n\nextension\n\n\nThe end part of a file name that tells you what type of file it is (e.g., .R or .Rmd).\n\n\n\n\nglobal environment\n\n\nThe interactive workspace where your script runs\n\n\n\n\ninteger\n\n\nA data type representing whole numbers.\n\n\n\n\nlogical\n\n\nA data type representing TRUE or FALSE values.\n\n\n\n\nna\n\n\nA missing value that is “Not Available”\n\n\n\n\nnominal\n\n\nCategorical variables that don't have an inherent order, such as types of animal.\n\n\n\n\nnumeric\n\n\nA data type representing a real decimal number or integer.\n\n\n\n\npanes\n\n\nRStudio is arranged with four window “panes”.\n\n\n\n\ntabular data\n\n\nData in a rectangular table format, where each row has an entry for each column.\n\n\n\n\ntidyverse\n\n\nA set of R packages that help you create and work with tidy data\n\n\n\n\nurl\n\n\nThe address of a web page (uniform resource locator)\n\n\n\n\nvector\n\n\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings."
  },
  {
    "objectID": "04-data.html#sec-resources-data",
    "href": "04-data.html#sec-resources-data",
    "title": "4  Data Import",
    "section": "\n4.11 Further resources",
    "text": "4.11 Further resources\n\nData import cheatsheet\n\nChapter 11: Data Import in R for Data Science\n\nMulti-row headers"
  },
  {
    "objectID": "05-summary.html#sec-ilo-summary",
    "href": "05-summary.html#sec-ilo-summary",
    "title": "5  Data Summaries",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to summarise data by groups\nBe able to produce well-formatted tables\nUse pipes to chain together functions"
  },
  {
    "objectID": "05-summary.html#sec-walkthrough-summary",
    "href": "05-summary.html#sec-walkthrough-summary",
    "title": "5  Data Summaries",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "05-summary.html#sec-setup-summary",
    "href": "05-summary.html#sec-setup-summary",
    "title": "5  Data Summaries",
    "section": "\n5.1 Set-up",
    "text": "5.1 Set-up\nFirst, create a new project for the work we’ll do in this chapter named 05-summary. Second, download the data for this chapter (ncod_tweets.rds) and save it in your project data folder. Finally, open and save and new R Markdown document named summary.Rmd, delete the welcome text and load the required packages for this chapter.\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)   # data wrangling functions\nlibrary(rtweet)      # for searching tweets\nlibrary(kableExtra)  # for nice tables\n```\n\n\nDownload the Data transformation cheat sheet."
  },
  {
    "objectID": "05-summary.html#social-media-data",
    "href": "05-summary.html#social-media-data",
    "title": "5  Data Summaries",
    "section": "\n5.2 Social media data",
    "text": "5.2 Social media data\nIn this chapter we’re going to analyse social media data, specifically data from Twitter. There are two broad types of data you can obtain from Twitter; data scraped from Twitter using purpose-built packages such as rtweet, and data provided via Twitter Analytics for any accounts for which you have access.\nFor this chapter, we’ll use data scraped from Twitter using rtweet. In order to use these functions, you need to have a Twitter account. Don’t worry if you don’t have one; we’ll provide the data in the examples below for you.\nrtweet has a lot of flexibility, for example, you can search for tweets that contain a certain hashtag or word, tweets by a specific user, or tweets that meet certain conditions like location or whether the user is verified.\nFor the dataset for this chapter, we used the search_tweets() function to find the last 30K tweets with the hashtag #NationalComingOutDay. This is mainly interesting around October 11th (the date of National Coming Out Day), so we’ve provided the relevant data for you that we scraped at that time.\nIf you have a Twitter account, you can complete this chapter using your own data and any hashtag that interests you. When you run the search_tweets() function, you will be asked to sign in to your Twitter account.\n\ntweets <- search_tweets(q = \"#NationalComingOutDay\", \n                        n = 30000, \n                        include_rts = FALSE)\n\n\n5.2.1 R objects\nIf you’re working with live social media data, every time you run a query it’s highly likely you will get a different set of data as new tweets are added. Additionally, the Twitter API places limits on how much data you can download and searches are limited to data from the last 6-9 days. Consequently, it can be useful to save the results of your initial search. saveRDS is a useful function that allows you to save any object in your environment to disk.\n\nsaveRDS(tweets, file = \"data/ncod_tweets.rds\")\n\nAfter you run search_tweets() and save the results, set that code chunk to eval = FALSE or comment out that code so your script doesn’t run the search and overwrite your saved data every time you knit it.\nTo load an .rds file, you can use the readRDS() function. If you don’t have access to a Twitter account, or to ensure that you get the same output as the rest of this chapter, you can download ncod_tweets.rds and load it using this function.\n\ntweets <- readRDS(\"data/ncod_tweets.rds\")\n\nFirst, run glimpse(tweets) or click on the object in the environment to find out what information is in the downloaded data (it’s a lot!). Now let’s create a series of summary tables and plots with these data."
  },
  {
    "objectID": "05-summary.html#sec-summary-summarise",
    "href": "05-summary.html#sec-summary-summarise",
    "title": "5  Data Summaries",
    "section": "\n5.3 Summarise",
    "text": "5.3 Summarise\nThe summarise() function from the dplyr package is loaded as part of the tidyverse and creates summary statistics. It creates a new table with columns that summarise the data from a larger table using summary functions. Check the Data Transformation Cheat Sheet for various summary functions. Some common ones are: n(), min(), max(), sum(), mean(), and quantile().\n\n\n\n\n\n\nWarning\n\n\n\nIf you get the answer NA from a summary function, that usually means that there are missing values in the columns you were summarising. We’ll discuss this more in Section 9.3.2, but you can ignore missing values for many functions by adding the argument na.rm = TRUE.\n\nvalues <- c(1, 2, 4, 3, NA, 2)\nmean(values) # is NA\nmean(values, na.rm = TRUE) # removes NAs first\n\n[1] NA\n[1] 2.4\n\n\n\n\nThis function can be used to answer questions like: How many tweets were there? What date range is represented in these data? What are the mean and median number of favourites per tweet? Let’s start with a very simple example to calculate the mean, median, min, and max number of favourites (Twitter’s version of a “like”):\n\nThe first argument that summarise() takes is the data table you wish to summarise, in this case the object tweets.\n\nsummarise() will create a new table. The column names of this new table will be the left hand-side arguments, i.e., mean_favs, median_favs, min_favs and max_favs.\nThe values of these columns are the result of the summary operation on the right hand-side.\n\n\nfavourite_summary <- summarise(tweets,\n                           mean_favs = mean(favorite_count),\n                           median_favs = median(favorite_count),\n                           min_favs = min(favorite_count),\n                           max_favs = max(favorite_count))\n\n\n\n\n\n\n mean_favs \n    median_favs \n    min_favs \n    max_favs \n  \n\n 29.71732 \n    3 \n    0 \n    22935 \n  \n\n\n\nThe mean number of favourites is substantially higher than the median and the range is huge, suggesting there are r glossary(\"outlier\", \"outliers\"). A quick histogram confirms this - most tweets have few favourites but there are a few with a lot of likes that skew the mean.\n\nggplot(tweets, aes(x = favorite_count)) +\n  geom_histogram(bins = 25) +\n  scale_x_continuous(trans = \"pseudo_log\", \n                     breaks = c(0, 1, 10, 100, 1000, 10000))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlotting the logarithm of a very skewed value can often give you a better idea of what’s going on. Use scale_x_continuous(trans = \"pseudo_log\") to include zeros on the plot (just “log” converts 0 to negative infinity and removes it from the plot).\n\n\nYou can add multiple operations to a single call to summarise() so let’s try a few different operations. The n() function counts the number of rows in the data. The created_at column gives us the date each tweet were created, so we can use the min() and max() functions to get the range of dates.\n\ntweet_summary <- tweets %>%\n  summarise(mean_favs = mean(favorite_count),\n            median_favs = quantile(favorite_count, .5),\n            n = n(),\n            min_date = min(created_at),\n            max_date = max(created_at))\n\nglimpse(tweet_summary)\n\nRows: 1\nColumns: 5\n$ mean_favs   <dbl> 29.71732\n$ median_favs <dbl> 3\n$ n           <int> 28626\n$ min_date    <dttm> 2021-10-10 00:10:02\n$ max_date    <dttm> 2021-10-12 20:12:27\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuantiles are like percentiles. Use quantile(x, .50) to find the median (the number where 50% of values in x are above it and 50% are below it). This can be useful when you need a value like “90% of tweets get X favourites or fewer”.\n\nquantile(tweets$favorite_count, 0.90)\n\n90% \n 31 \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nHow would you find the largest number of retweets?\n\ntweets %>% summarise(max_retweets = max(retweets))tweets %>% summarise(max_retweets)tweets %>% summarise(max = retweets)tweets %>% max(retweets)\n\n\nHow would you calculate the mean display_text_width?\n\nwidth(mean(display_text_width))summarise(display_text_width = mean)summarise(width = mean(display_text_width))group_by(display_text_width)\n\n\n\n\n\n\n5.3.1 The $ operator\nWe need to take a couple of brief detours to introduce some additional coding conventions. First, let’s clear up what that $ notation is doing. The dollar sign allows you to select items from an object, such as columns from a table. The left-hand side is the object, and the right-hand side is the item. When you call a column like this, R will print all the observations in that column.\n\ntweet_summary$mean_favs\n\n[1] 29.71732\n\n\nIf your item has multiple observations, you can specify which ones to return using square brackets [] and the row number or a vector of row numbers.\n\ntweets$source[1] # select one observation\ntweets$display_text_width[c(20,30,40)] # select multiple with c()\n\n[1] \"Twitter for Android\"\n[1]  78 287 107\n\n\n\n5.3.2 Pipes\nFor our second detour, let’s formally introduce the pipe, that weird %>% symbol we’ve used occasionally. Pipes allow you to send the output from one function straight into another function. Specifically, they send the result of the function before %>% to be the first argument of the function after %>%. It can be useful to translate the pipe as “and then”. It’s easier to show than tell, so let’s look at an example.\nWe could write the above code using a pipe as follows:\n\ntweet_summary <- tweets %>% # start with the object tweets and then\n  summarise(mean_favs = mean(favorite_count), #summarise it\n            median_favs = median(favorite_count))\n\nNotice that summarise() no longer needs the first argument to be the data table, it is pulled in from the pipe. The power of the pipe may not be obvious now, but it will soon prove its worth.\n\n5.3.3 Inline coding\nTo insert those values into the text of a report you can use inline coding. First. we’ll create another set of objects that contain the first and last date of the tweets in our dataset. format() formats the dates to day/month/year.\n\ndate_from <- tweet_summary$min_date %>% \n  format(\"%d %B, %Y\")\ndate_to <- tweet_summary$max_date %>% \n  format(\"%d %B, %Y\")\n\nThen you can insert values from these objects and the tables you created with summarise() using inline R (note the dollar sign notation to get the value of the n column from the table tweet_summary).\nThere were `r tweet_summary$n` tweets between `r date_from` and `r date_to`.\nKnit your Markdown to see how the variables inside the inline code get replaced by their values.\n\nThere were 28626 tweets between 10 October, 2021 and 12 October, 2021.\n\nOk, let’s get back on track."
  },
  {
    "objectID": "05-summary.html#counting",
    "href": "05-summary.html#counting",
    "title": "5  Data Summaries",
    "section": "\n5.4 Counting",
    "text": "5.4 Counting\nHow many different accounts tweeted using the hashtag? Who tweeted most?\nYou can count categorical data with the count() function. Since each row is a tweet, you can count the number of rows per each different screen_name to get the number of tweets per user. This will give you a new table with each combination of the counted columns and a column called n containing the number of observations from that group.\nThe argument sort = TRUE will sort the table by n in descending order, whilst head() returns the first six lines of a data table and is a useful function to call when you have a very large dataset and just want to see the top values.\n\ntweets_per_user <- tweets %>%\n  count(screen_name, sort = TRUE)\n\nhead(tweets_per_user)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHow would you create the table of counts below?\n\n\n\n\n  \n\n\n\n\ntweets %>% count(is_quote, is_retweet)tweets %>% count(is_quote) %>% count(is_retweet)tweets %>% count(c(is_quote, is_retweet))tweets %>% select(is_quote, is_retweet) %>% count()\n\n\n\n\n5.4.1 Inline coding 2\nLet’s do another example of inline coding that writes up a summary of the most prolific tweeters to demonstrate a few additional functions. First, we need to create some additional objects to use with inline R:\n\n\nnrow() simply counts the number of rows in a dataset so if you have one user/participant/customer per row, this is an easy way to do a head count.\n\nslice() chooses a particular row of data, in this case the first row. Because we sorted our data, this will therefore be the user with the most tweets.\n\npull() pulls out a single variable.\nThe combination of slice() and pull() allows you to choose a single observation from a single variable.\n\n\nunique_users <- nrow(tweets_per_user)\nmost_prolific <- slice(tweets_per_user, 1) %>% \n  pull(screen_name)\nmost_prolific_n <- slice(tweets_per_user, 1) %>% \n  pull(n)\n\nThen add the inline code to your report…\nThere were `r unique_users` unique accounts tweeting about #NationalComingOutDay. `r most_prolific` was the most prolific tweeter, with `r most_prolific_n` tweets.\n…and knit your Markdown to see the output:\nThere were 25189 unique accounts tweeting about #NationalComingOutDay. interest_outfit was the most prolific tweeter, with 35 tweets."
  },
  {
    "objectID": "05-summary.html#sec-grouping",
    "href": "05-summary.html#sec-grouping",
    "title": "5  Data Summaries",
    "section": "\n5.5 Grouping",
    "text": "5.5 Grouping\nYou can also create summary values by group. The combination of group_by() and summarise() is incredibly powerful, and it is also a good demonstration of why pipes are so useful.\nThe function group_by() takes an existing data table and converts it into a grouped table, where any operations that are performed on it are done “by group”.\nThe first line of code creates an object named tweets_grouped, that groups the dataset according to whether the user is a verified user. On the surface, tweets_grouped doesn’t look any different to the original tweets. However, the underlying structure has changed and so when we run summarise(), we now get our requested summaries for each group (in this case verified or not).\n\ntweets_grouped <- tweets %>%\n  group_by(verified)\n\nverified <- tweets_grouped %>%\n  summarise(count = n(),\n            mean_favs = mean(favorite_count),\n            mean_retweets = mean(retweet_count)) %>%\n  ungroup()\n\nverified\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you call the ungroup() function when you are done with grouped functions. Failing to do this can cause all sorts of mysterious problems if you use that data table later assuming it isn’t grouped.\n\n\nWhilst the above code is functional, it adds an unnecessary object to the environment - tweets_grouped is taking up space and increases the risk we’ll use this grouped object by mistake. Enter… the pipe.\nRather than creating an intermediate object, we can use the pipe to string our code together.\n\nverified <- \n  tweets %>% # Start with the original dataset; and then\n  group_by(verified) %>% # group it; and then\n  summarise(count = n(), # summarise it by those groups\n            mean_favs = mean(favorite_count),\n            mean_retweets = mean(retweet_count)) %>%\n  ungroup()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWhat would you change to calculate the mean favourites and retweets by screen_name instead of by verified?\n\nsummarise(screen_name)count(screen_name)mean(screen_name)group_by(screen_name)\n\n\n\n\n\n\n5.5.1 Multiple groupings\nYou can add multiple variables to group_by() to further break down your data. For example, the below gives us the number of likes and retweets broken down by verified status and the device the person was tweeting from (source).\n\nReverse the order of verified and source in group_by() to see how it changed the output.\n\n\nverified_source <- tweets %>%\n  group_by(verified, source) %>%\n  summarise(count = n(),\n            mean_favs = mean(favorite_count),\n            mean_retweets = mean(retweet_count)) %>%\n  ungroup() %>%\n  arrange(desc(count))\n\nhead(verified_source)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou may get the following message when using summarise() after group_by().\n\nsummarise() has grouped output by ‘verified’. You can override using the .groups argument.\n\nTidyverse recently added a message to remind you whether the summarise() function automatically ungroups grouped data or not (it may do different things depending on how it’s used). You can set the argument .groups to “drop”, “drop_last”, “keep”, or “rowwise” (see the help for ?summarise), but it’s good practice to explicitly use ungroup() when you’re done working by groups, regardless.\n\n\n\n5.5.2 Filter and mutate\nYou can also use additional functions like filter() or mutate() after group_by. You’ll learn more about these in Chapter 9 but briefly:\n\n\nfilter() keeps observations (rows) according to specified criteria, e.g., all values above 5, or all verified users.\n\nmutate() creates new variables (columns), or overwrites existing ones.\n\nYou can combine functions like this to get detailed insights into your data. For example, what were the most favourited original and quoted tweets?\n\nThe variable is_quote tells us whether the tweet in question was an original tweet or a quote tweet. Because we want our output to treat these separately, we pass this variable to group_by().\nWe want the most favourited tweets, i.e., the maximum value of favourite_count, so we can use filter() to only return rows where favourite_count is equal to the maximum value in the variable favourite_count. Note the use of == rather than a single =.\nJust in case there was a tie, choose a random one with sample_n(size = 1).\n\n\nmost_fav <- tweets %>%\n  group_by(is_quote) %>%\n  filter(favorite_count == max(favorite_count)) %>%\n  sample_n(size = 1) %>%\n  ungroup()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nHow would you limit the results to sources with 10 or more rows?\n\ntweets %>% group_by(source) %>% filter(n() >= 10)tweets %>% group_by(source) %>% filter(count() >= 10)tweets %>% group_by(source) %>% select(count() >= 10)tweets %>% group_by(source) %>% select(n() >= 10)\n\n\n\n\n\n\n5.5.3 Inline coding 3\nThere’s a huge amount of data reported for each tweet, including things like the URLs of the tweets and any media attached to them. This means we can produce output like the below reproducibly and using inline coding.\n\n\n\nThe most favourited 22935 original tweet was by jackrooke:\n\n\nit’s #nationalcomingoutday 🎉 here’s a pic of how I came out back in 2003 xx https://t.co/spBmHhF6p4\n\n\n\nTo produce this, first we split most_fav, so that we have one object that contains the data from the original tweet and one object that contains the data from the quote tweet.\n\norig <- filter(most_fav,is_quote == FALSE)\nquote <- filter(most_fav,is_quote == TRUE)\n\nThe inline code is then as follows:\nThe most favourited `r orig$favorite_count` original tweet was by [`r orig$screen_name`](`r orig$status_url`):\n\n--------------------------------------------------\n  \n> `r orig$text`\n\n![](`r orig$ext_media_url`)\n\n--------------------------------------------------\nThis is quite complicated so let’s break it down.\n\nThe first bit of inline coding is fairly standard and is what you have used before.\nThe second bit of inline coding inserts a URL. The content of the [] is the text that will be displayed. The content of () is the underlying URL. In both cases, the content is being pulled from the dataset. In this case, the text is screen_name and status_url links to the tweet.\nThe line of dashes creates the solid line in the knitted output.\nThe > symbol changes the format to a block quote.\nThe image is then included using the format ![](url), which is an alternative method of including images in Markdown."
  },
  {
    "objectID": "05-summary.html#exercises",
    "href": "05-summary.html#exercises",
    "title": "5  Data Summaries",
    "section": "\n5.6 Exercises",
    "text": "5.6 Exercises\nThat was an intensive chapter! Take a break and then try one (or more) of the following and post your knitted HTML files on Teams so that other learners on the course can see what you did.\n\nIf you have your own Twitter account, conduct a similar analysis of a different hashtag.\nLook through the rest of the variables in tweets; what other insights can you generate about this data?\nRead through the kableExtra vignettes and apply your own preferred table style.\nWork through the first few chapters of Tidy Text to see how you can work with and analyse text. In particular, see if you can conduct a sentiment analysis on the tweet data."
  },
  {
    "objectID": "05-summary.html#sec-glossary-summary",
    "href": "05-summary.html#sec-glossary-summary",
    "title": "5  Data Summaries",
    "section": "\n5.7 Glossary",
    "text": "5.7 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nmean\n\n\nA descriptive statistic that measures the average value of a set of numbers.\n\n\n\n\nmedian\n\n\nThe middle number in a distribution where half of the values are larger and half are smaller.\n\n\n\n\npipe\n\n\nA way to order your code in a more readable format using the symbol %>%\n\n\n\n\nquantile\n\n\nCutoffs dividing the range of a distribution into continuous intervals with equal probabilities.\n\n\n\n\nvector\n\n\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings."
  },
  {
    "objectID": "05-summary.html#sec-resources-summary",
    "href": "05-summary.html#sec-resources-summary",
    "title": "5  Data Summaries",
    "section": "\n5.8 Further resources",
    "text": "5.8 Further resources\n\nData transformation cheat sheet\n\nChapter 5: Data Transformation in R for Data Science\n\nIntro to rtweet\nTidy Text\nkableExtra vignettes"
  },
  {
    "objectID": "06-formative.html#assessment-information",
    "href": "06-formative.html#assessment-information",
    "title": "6  Practice Report",
    "section": "6.1 Assessment information",
    "text": "6.1 Assessment information\n\nYou will be given a dataset and a finished report on that data. Your task is to write the code that produces the report.\nPlease check Moodle and the assessment information sheet for the deadline.\nThe solution will be released immediately after the deadline on Moodle.\nThis is a formative activity and does not count towards your course grade.\nYou should submit two files to Moodle to complete the exercise - your .Rmd file and the knitted html document.\nPlease read all the assessment information before attempting the exercise. Additionally, you should not expect to be able to complete it until you have worked through Chapters 1-5 (although particularly Chapters 1-3).\nThe practice report will not be individually graded. Instead, we will release the solution file so that you can compare your code to our solution as generic feedback. If you would like to discuss any aspect of the report, you can attend office hours with the course leads and/or teaching assistants."
  },
  {
    "objectID": "06-formative.html#intended-learning-outcomes-ilos",
    "href": "06-formative.html#intended-learning-outcomes-ilos",
    "title": "6  Practice Report",
    "section": "6.2 Intended Learning Outcomes (ILOs)",
    "text": "6.2 Intended Learning Outcomes (ILOs)\n\nKnowledge and skills\n\nDemonstrate the ability to use R for data wrangling and visualisation\nDemonstrate the ability to use R Markdown for reproducible reports\n\n\n\nEvaluation\n\nDemonstrate the ability to evaluate an output to understand the underlying data wrangling and analysis\n\n\n\nCommunication\n\nDemonstrate the ability to produce a clear and coherent report with appropriate spelling, grammar, and layout."
  },
  {
    "objectID": "06-formative.html#details-and-files",
    "href": "06-formative.html#details-and-files",
    "title": "6  Practice Report",
    "section": "6.3 Details and files",
    "text": "6.3 Details and files\n\nThere are two files you need to download. The first is the dataset: review_data.csv. The second is the finished report that you need to backwards engineer from the data: formative_report_output.html.\nThere are multiple ways to achieve the same outcome when coding. You do not have to use the same solutions that we did; what matters is that the output is the same.\nAll plots, and tables in the report should be fully reproducible, in addition, all numbers in bold should also be reproducible, i.e., you should use inline coding.\nFor any visuals, you should get as close to the example report as possible, although we recognise that it may be difficult to know the exact theme or colours used without seeing the code.\nIn order to replicate the report you will need the following skills:\n\nMarkdown text formatting (e.g., bold, italics, headings)\nAdding images to Markdown from online sources\nData visualisation\nSummarising data in tables Inline coding\n\n\nAlmost everything you need to do generate this report will have been covered at least once in the workbook, but you should also expect to need to Google some things – being able to search for alternative solutions is such a key skill for programming."
  },
  {
    "objectID": "06-formative.html#support",
    "href": "06-formative.html#support",
    "title": "6  Practice Report",
    "section": "6.4 Support",
    "text": "6.4 Support\nThis assessment will require you to use the skills you have learned from Chapter 1 (Intro to R and R Studio) up to and including Chapter 5 (Data Summaries) of the Applied Data Skills workbook. You can also ask for help on Teams, although because this is a formative activity, the help we give will be constrained.\nGoogle and Stack Overflow will help you a lot. Remember to include the names of packages and functions to help ensure the results are closer to what you’re trying to do."
  },
  {
    "objectID": "06-formative.html#helpful-hints",
    "href": "06-formative.html#helpful-hints",
    "title": "6  Practice Report",
    "section": "6.5 Helpful hints",
    "text": "6.5 Helpful hints\nA few hints to help you on your way:\n\nThis dataset is bigger than the ones you’ve used before, which means you might find it takes R a little longer to complete certain tasks.\nGoogling “ggplot rotate x axis labels” will help you out.\ntheme_fivethirtyeight()\nThe verified review definition is formatted as a block quote.\nCreate Awesome HTML Table with knitr::kable and kableExtra\nHow to Turn off scientific notation like 1e+09 in R?\nThere are a couple of ways you can create the final plot, one method might be to calculate the numbers you need first, and then use this new dataset for the plot, another method would be to use stat_summary()."
  },
  {
    "objectID": "06-formative.html#why-am-i-being-assessed-like-this",
    "href": "06-formative.html#why-am-i-being-assessed-like-this",
    "title": "6  Practice Report",
    "section": "6.6 Why am I being assessed like this?",
    "text": "6.6 Why am I being assessed like this?\nThis report serves as a practice for the full report, which will be due at the end of the semester. The practice report gives you the intended output so that you can focus on your coding skills, rather than deciding how to summarise and present the data. You will need to make these decisions for the full report.\nWhilst most of the skills required for the practice report have been explicitly taught, there are minor bits of the coding that will require you to look up solutions independently. This will be good practice for the final summative assessment and for applying your coding skills to your own work."
  },
  {
    "objectID": "07-joins.html#sec-ilo-joins",
    "href": "07-joins.html#sec-ilo-joins",
    "title": "7  Data Relations",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to match related data across multiple tables\nBe able to combine data from multiple files"
  },
  {
    "objectID": "07-joins.html#sec-walkthrough-joins",
    "href": "07-joins.html#sec-walkthrough-joins",
    "title": "7  Data Relations",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "07-joins.html#sec-setup-joins",
    "href": "07-joins.html#sec-setup-joins",
    "title": "7  Data Relations",
    "section": "\n7.1 Set-up",
    "text": "7.1 Set-up\nFirst, create a new project for the work we’ll do in this chapter named 07-relations. Second, open and save a new R Markdown document named relations.Rmd, delete the welcome text, and load the required packages for this chapter.\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)     # includes readr & tibble\n```\n\n\nDownload the Data transformation cheatsheet."
  },
  {
    "objectID": "07-joins.html#sec-joins-data",
    "href": "07-joins.html#sec-joins-data",
    "title": "7  Data Relations",
    "section": "\n7.2 Loading data",
    "text": "7.2 Loading data\nThe data you want to report on or visualise are often in more than one file (or more than one tab of an excel file or googlesheet). You might need to join up a table of customer information with a table of orders, or combine the monthly social media reports across several months.\nFor this demo, rather than loading in data, we’ll create two small data tables from scratch using the tibble() function.\ncustomers has id, city and postcode for five customers 1-5.\n\n\n1:5 will fill the variable id with all integers between 1 and 5.\n\ncity and code both use the c() function to enter multiple strings. Note that each entry is contained within its own quotation marks, apart from missing data, which is recorded as NA.\nWhen entering data like this, it’s important that the order of each variable matches up. So number 1 will correspond to “Port Ellen” and “PA42 7DU”.\n\n\ncustomers <- tibble(\n  id = 1:5,\n  city = c(\"Port Ellen\", \"Dufftown\", NA, \"Aberlour\", \"Tobermory\"),\n  postcode = c(\"PA42 7DU\", \"AB55 4DH\", NA, \"AB38 7RY\", \"PA75 6NR\")\n)\n\n\n\n\nDemo customers table.\n\n\n\nid\n\n\ncity\n\n\npostcode\n\n\n\n\n\n1\n\n\nPort Ellen\n\n\nPA42 7DU\n\n\n\n\n2\n\n\nDufftown\n\n\nAB55 4DH\n\n\n\n\n3\n\n\nNA\n\n\nNA\n\n\n\n\n4\n\n\nAberlour\n\n\nAB38 7RY\n\n\n\n\n5\n\n\nTobermory\n\n\nPA75 6NR\n\n\n\n\norders has customer id and the number of items ordered. Some customers from the previous table have no orders, some have more than one order, and some are not in the customer table.\n\norders <- tibble(\n  id = c(2, 3, 4, 4, 5, 5, 6, 6, 7),\n  items = c(10, 18, 21, 23, 9, 11, 11, 12, 3)\n)\n\n\n\n\nDemo orders table.\n\n\n\nid\n\n\nitems\n\n\n\n\n\n2\n\n\n10\n\n\n\n\n3\n\n\n18\n\n\n\n\n4\n\n\n21\n\n\n\n\n4\n\n\n23\n\n\n\n\n5\n\n\n9\n\n\n\n\n5\n\n\n11\n\n\n\n\n6\n\n\n11\n\n\n\n\n6\n\n\n12\n\n\n\n\n7\n\n\n3"
  },
  {
    "objectID": "07-joins.html#mutating-joins",
    "href": "07-joins.html#mutating-joins",
    "title": "7  Data Relations",
    "section": "\n7.3 Mutating Joins",
    "text": "7.3 Mutating Joins\nMutating joins act like the dplyr::mutate() function in that they add new columns to one table based on values in another table. (We’ll learn more about the mutate() function in Chapter 8).)\nAll the mutating joins have this basic syntax:\n****_join(x, y, by = NULL, suffix = c(\".x\", \".y\"))\n\n\nx = the first (left) table\n\ny = the second (right) table\n\nby = what columns to match on. If you leave this blank, it will match on all columns with the same names in the two tables.\n\nsuffix = if columns have the same name in the two tables, but you aren’t joining by them, they get a suffix to make them unambiguous. This defaults to “.x” and “.y”, but you can change it to something more meaningful.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can leave out the by argument if you’re matching on all of the columns with the same name, but it’s good practice to always specify it so your code is robust to changes in the loaded data.\n\n\n\n7.3.1 left_join()\n\n\n\nA left_join keeps all the data from the first (left) table and adds anything that matches from the second (right) table. If the right table has more than one match for a row in the left table, there will be more than one row in the joined table (see ids 4 and 5).\n\nleft_data <- left_join(customers, orders, by = \"id\")\nleft_data\n\n\n\n  \n\n\n\n\n\n\nThe order you specify the tables matters, in the below code we have reversed the order and so the result is all rows from the orders table joined to any matching rows from the customers table.\n\nleft2_data <- left_join(orders, customers, by = \"id\")\nleft2_data\n\n\n\n  \n\n\n\n\n7.3.2 right_join()\n\n\n\nA right_join keeps all the data from the second (right) table and joins anything that matches from the first (left) table.\n\nright_data <- right_join(customers, orders, by = \"id\")\nright_data\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis table has the same information as left_join(orders, customers, by = \"id\"), but the columns are in a different order (left table, then right table).\n\n\n\n7.3.3 inner_join()\n\n\n\nAn inner_join returns all the rows that have a match in both tables. Changing the order of the tables will change the order of the columns, but not which rows are kept.\n\ninner_data <- inner_join(customers, orders, by = \"id\")\ninner_data\n\n\n\n  \n\n\n\n\n7.3.4 full_join()\n\n\n\nA full_join lets you join up rows in two tables while keeping all of the information from both tables. If a row doesn’t have a match in the other table, the other table’s column values are set to NA.\n\nfull_data <- full_join(customers, orders, by = \"id\")\nfull_data"
  },
  {
    "objectID": "07-joins.html#filtering-joins",
    "href": "07-joins.html#filtering-joins",
    "title": "7  Data Relations",
    "section": "\n7.4 Filtering Joins",
    "text": "7.4 Filtering Joins\nFiltering joins act like the dplyr::filter() function in that they keep and remove rows from the data in one table based on the values in another table. The result of a filtering join will only contain rows from the left table and have the same number or fewer rows than the left table. We’ll learn more about the filter() function in Chapter 9.\n\n7.4.1 semi_join()\n\n\n\nA semi_join returns all rows from the left table where there are matching values in the right table, keeping just columns from the left table.\n\nsemi_data <- semi_join(customers, orders, by = \"id\")\nsemi_data\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike an inner join, a semi join will never duplicate the rows in the left table if there is more than one matching row in the right table.\n\n\n\n\n\nOrder matters in a semi join.\n\nsemi2_data <- semi_join(orders, customers, by = \"id\")\nsemi2_data\n\n\n\n  \n\n\n\n\n7.4.2 anti_join()\n\n\n\nAn anti_join return all rows from the left table where there are not matching values in the right table, keeping just columns from the left table.\n\nanti_data <- anti_join(customers, orders, by = \"id\")\nanti_data\n\n\n\n  \n\n\n\n\n\n\nOrder matters in an anti join.\n\nanti2_data <- anti_join(orders, customers, by = \"id\")\nanti2_data"
  },
  {
    "objectID": "07-joins.html#multiple-joins",
    "href": "07-joins.html#multiple-joins",
    "title": "7  Data Relations",
    "section": "\n7.5 Multiple joins",
    "text": "7.5 Multiple joins\nThe ****_join() functions are all two-table verbs, that is, you can only join together two tables at a time. However, you may often need to join together multiple tables. To do so, you simply need to add on additional joins. You can do this by creating an intermediate object or more efficiently by using a pipe.\n\n# create a table of overall customer satisfaction scores\nsatisfaction <- tibble(\n  id = 1:5,\n  satisfaction = c(4, 3, 2, 3, 1)\n)\n\n# perform the initial join\njoin_1 <- left_join(customers, orders, by = \"id\")\n\n# perform the second join on the new object\njoin_2 <- left_join(join_1, satisfaction, \n                    by = \"id\")\n\n\n# more efficient method using the pipe\npipe_join <- customers %>%\n  left_join(orders, by = \"id\") %>%\n  left_join(satisfaction, by = \"id\")\n\n\n\n\n\n\n\nWarning\n\n\n\nAt every stage of any analysis you should check your output to ensure that what you created is what you intended to create, but this is particularly true of joins. You should be familiar enough with your data through routine checks using functions like glimpse(), str(), and summary() to have a rough idea of what the join should result in. At the very least, you should know whether the joined object should result in more or fewer variables and observations.\nIf you have a multi-line join like in the above piped example, build up the code and check the output at each stage."
  },
  {
    "objectID": "07-joins.html#binding-joins",
    "href": "07-joins.html#binding-joins",
    "title": "7  Data Relations",
    "section": "\n7.6 Binding Joins",
    "text": "7.6 Binding Joins\nBinding joins bind one table to another by adding their rows or columns together.\n\n7.6.1 bind_rows()\nYou can combine the rows of two tables with bind_rows.\nHere we’ll add customer data for customers 6-9 and bind that to the original customer table.\n\nnew_customers <- tibble(\n  id = 6:9,\n  city = c(\"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\"),\n  postcode = c(\"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\")\n)\n\nbindr_data <- bind_rows(customers, new_customers)\nbindr_data\n\n\n\n  \n\n\n\nThe columns just have to have the same names, they don’t have to be in the same order. Any columns that differ between the two tables will just have NA values for entries from the other table.\nIf a row is duplicated between the two tables (like id 5 below), the row will also be duplicated in the resulting table. If your tables have the exact same columns, you can use union() (see Section 7.7.2) to avoid duplicates.\n\nnew_customers <- tibble(\n  id = 5:9,\n  postcode = c(\"PA75 6NR\", \"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\"),\n  city = c(\"Tobermory\", \"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\"),\n  new = c(1,2,3,4,5)\n)\n\nbindr2_data <- bind_rows(customers, new_customers)\nbindr2_data\n\n\n\n  \n\n\n\n\n7.6.2 bind_cols()\nYou can merge two tables with the same number of rows using bind_cols. This is only useful if the two tables have the same number of rows in the exact same order.\n\nnew_info <- tibble(\n  colour = c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\")\n)\n\nbindc_data <- bind_cols(customers, new_info)\nbindc_data \n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe only advantage of bind_cols() over a mutating join is when the tables don’t have any IDs to join by and you have to rely solely on their order. Otherwise, you should use a mutating join (all four mutating joins result in the same output when all rows in each table have exactly one match in the other table).\n\n\n\n7.6.3 Importing multiple files\nIf you need to import and bind a whole folder full of files that have the same structure, get a list of all the files you want to combine. It’s easiest if they’re all in the same directory, although you can use a pattern to select the files you want if they have a systematic naming structure.\nFirst, save the two customer tables to CSV files. The dir.create() function makes a folder called “data”. The showWarnings = FALSE argument means that you won’t get a warning if the folder already exists, it just won’t do anything.\n\n# write our data to a new folder for the demo\ndir.create(\"data\", showWarnings = FALSE)\nwrite_csv(x = customers, file = \"data/customers1.csv\")\nwrite_csv(x = new_customers, file = \"data/customers2.csv\")\n\nNext, retrieve a list of all file names in the data folder that contain the string “customers”\n\nfiles <- list.files(\n  path = \"data\", \n  pattern = \"customers\", \n  full.names = TRUE\n)\n\nfiles\n\n[1] \"data/customers1.csv\" \"data/customers2.csv\"\n\n\nNext, we’ll iterate over this list to read in the data from each file. Whilst this won’t be something we cover in detail in the core resources of this course, iteration is an important concept to know about. Iteration is where you perform the same task on multiple different inputs. As a general rule of thumb, if you find yourself copying and pasting the same thing more than twice, there’s a more efficient and less error-prone way to do it, although these functions do typically require a stronger grasp of programming.\nThe purrr package contains functions to help with iteration. purrr::map_df() maps a function to a list and returns a data frame (table) of the results.\n\n\n.x is the list of file paths\n\n.f specifies the function to map to each of those file paths.\nThe resulting object all_files will be a data frame that combines all the files together, similar to if you had imported them separately and then used bind_rows(). Note that map_df() will only work in this way if the structure of all files is identical.\n\n\nall_files <- purrr::map_df(.x = files, .f = read_csv)"
  },
  {
    "objectID": "07-joins.html#set-operations",
    "href": "07-joins.html#set-operations",
    "title": "7  Data Relations",
    "section": "\n7.7 Set Operations",
    "text": "7.7 Set Operations\nSet operations compare two tables and return rows that match (intersect), are in either table (union), or are in one table but not the other (setdiff).\n\n7.7.1 intersect()\ndplyr::intersect() returns all rows in two tables that match exactly. The columns don’t have to be in the same order, but they have to have the same names.\n\nnew_customers <- tibble(\n  id = 5:9,\n  postcode = c(\"PA75 6NR\", \"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\"),\n  city = c(\"Tobermory\", \"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\")\n)\n\nintersect_data <- intersect(customers, new_customers)\nintersect_data\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you’ve forgotten to load dplyr or the tidyverse, base R also has a base::intersect() function that doesn’t work like dplyr::intersect(). The error message can be confusing and looks something like this:\n\nbase::intersect(customers, new_customers)\n\nlist()\n\n\n\n\n\n7.7.2 union()\ndplyr::union() returns all the rows from both tables, removing duplicate rows, unlike bind_rows().\n\nunion_data <- union(customers, new_customers)\nunion_data\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you’ve forgotten to load dplyr or the tidyverse, base R also has a base::union() function. You usually won’t get an error message, but the output won’t be what you expect.\n\nbase::union(customers, new_customers)\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"Port Ellen\" \"Dufftown\"   NA           \"Aberlour\"   \"Tobermory\" \n\n[[3]]\n[1] \"PA42 7DU\" \"AB55 4DH\" NA         \"AB38 7RY\" \"PA75 6NR\"\n\n[[4]]\n[1] 5 6 7 8 9\n\n[[5]]\n[1] \"PA75 6NR\" \"FK1 4RS\"  \"PA42 7EA\" \"G81 4SJ\"  \"KW15 1SE\"\n\n[[6]]\n[1] \"Tobermory\" \"Falkirk\"   \"Ardbeg\"    \"Doogal\"    \"Kirkwall\" \n\n\n\n\n\n7.7.3 setdiff()\ndplyr::setdiff returns rows that are in the first table, but not in the second table.\n\nsetdiff_data <- setdiff(customers, new_customers)\nsetdiff_data\n\n\n\n  \n\n\n\nOrder matters for setdiff.\n\nsetdiff2_data <- setdiff(new_customers, customers)\nsetdiff2_data\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you’ve forgotten to load dplyr or the tidyverse, base R also has a base::setdiff() function. You usually won’t get an error message, but the output might not be what you expect because base::setdiff() expects columns to be in the same order, so id 5 here registers as different between the two tables.\n\nbase::setdiff(customers, new_customers)\n\n$id\n[1] 1 2 3 4 5\n\n$city\n[1] \"Port Ellen\" \"Dufftown\"   NA           \"Aberlour\"   \"Tobermory\" \n\n$postcode\n[1] \"PA42 7DU\" \"AB55 4DH\" NA         \"AB38 7RY\" \"PA75 6NR\""
  },
  {
    "objectID": "07-joins.html#conflicting-variable-types",
    "href": "07-joins.html#conflicting-variable-types",
    "title": "7  Data Relations",
    "section": "\n7.8 Conflicting variable types",
    "text": "7.8 Conflicting variable types\nAs we covered in Chapter 4, when you import or create data, R will do its best to set each column to an appropriate data type. However, sometimes it gets it wrong or sometimes there’s something in the way the data has been encoded in the original spreadsheet that causes the data type to be different than expected. When joining datasets by common columns, it’s important that not only are the variable names identical, but the data type of those variables is identical.\nLet’s recreate our new_customers dataset but this time, we’ll specify that id is a character variable.\n\nnew_customers2 <- tibble(\n  id = as.character(5:9),\n  postcode = c(\"PA75 6NR\", \"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\"),\n  city = c(\"Tobermory\", \"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\")\n)\nstr(new_customers2)\n\ntibble [5 × 3] (S3: tbl_df/tbl/data.frame)\n $ id      : chr [1:5] \"5\" \"6\" \"7\" \"8\" ...\n $ postcode: chr [1:5] \"PA75 6NR\" \"FK1 4RS\" \"PA42 7EA\" \"G81 4SJ\" ...\n $ city    : chr [1:5] \"Tobermory\" \"Falkirk\" \"Ardbeg\" \"Doogal\" ...\n\n\nIf you try to join this dataset to any of the other datasets where id is stored as a numeric variable, it will produce an error.\n\ninner_join(customers, new_customers2)\n\nJoining, by = c(\"id\", \"city\", \"postcode\")\n\n\nError in `inner_join()`:\n! Can't join on `x$id` x `y$id` because of incompatible types.\nℹ `x$id` is of type <integer>>.\nℹ `y$id` is of type <character>>.\n\n\nThe same goes for bind_rows():\n\nbind_rows(customers, new_customers2)\n\nError in `bind_rows()`:\n! Can't combine `..1$id` <integer> and `..2$id` <character>.\n\n\nAs alternative method to change variable types from what we showed you in Chapter 4 is to use the as.*** functions. If you type as. into a code chunk, you will see that there are a huge number of these functions for transforming variables and datasets to different types. Exactly which one you need will depend on the data you have, but a few commonly used ones are:\n\n\nas.numeric() - convert a variable to numeric. Useful for when you have a variable of real numbers that have been encoded as character. Any values that can’t be turned into numbers (e.g., if you have the word “missing” in cells that you have no data for), will be returned as NA.\n\nas.factor() - convert a variable to a factor. You can set the factor levels and labels manually, or use the default order (alphabetical).\n\nas.character() - convert a variable to character data.\n\nas.tibble() and as.data.frame() - convert a list object (not a variable) to a tibble or a data frame (two different table formats). This isn’t actually relevant to what we’re discussing here, but it’s a useful one to be aware of because sometimes you’ll run into issues where you get an error that specifically requests your data is a tibble or data frame type and you can use this function to overwrite your object.\n\nTo use these functions on a variable we can use mutate() to overwrite the variable with that variable as the new data type:\n\nnew_customers2 <- new_customers2 %>%\n  mutate(id = as.numeric(id))\n\nOnce you’ve done this, the joins will now work:\n\ninner_join(orders, new_customers2)\n\nJoining, by = \"id\""
  },
  {
    "objectID": "07-joins.html#exercises",
    "href": "07-joins.html#exercises",
    "title": "7  Data Relations",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\nThere’s lots of different use cases for the ****_join() functions. These exercises will allow you to practice different joins. If you have any examples of where joins might be helpful in your own work, please post them on Teams in the week 6 channel, as having many concrete examples can help distinguish between the different joins.\n\n7.9.1 Grade data\nThe University of Glasgow’s Schedule A grading scheme uses a 22-point alphanumeric scale (there’s more information in your summative report assessment information sheet). Each alphanumeric grade (e.g., B2) has an underlying numeric Grade Point (e.g., 16).\nOften when we’re working with student grades they are provided to us in only one of these forms, but we need to be able to go between the two. For example, we need the numeric form in order to be able to calculate descriptive statistics about the mean grade, but we need the alphanumeric form to release to student records.\n\nDownload grade_data.csv, grade_data2.csv and scheduleA.csv into your data folder.\nRead in scheduleA.csv and save it to an object named schedule.\nRead in grade_data1.csv and save it to an object named grades1.\nRead in grade_data2.csv and save it to an object named grades2.\n\n\n\n\nSolution\n\nschedule <- read_csv(\"data/scheduleA.csv\")\ngrades1 <- read_csv(\"data/grade_data1.csv\") \ngrades2 <- read_csv(\"data/grade_data2.csv\")\n\n\n\n7.9.2 Matching the variable types\nAt UofG, all students are given a GUID, a numeric ID number. However, that ID number is also then combined with the first letter from your surname to create your username that is used with your email address. For example, if your ID is 1234567 and your surname is Nordmann, your username would be 1234567n. From a data wrangling perspective this is very annoying because the numeric ID will be stored as numeric data, but the username will be stored as character because of the letter at the end. grades1 has a numeric id whilst grades2 has the additional letter. In order to join these datasets, we need to standardise the variables.\nFirst, remove the letter character from id using the function stringr::str_replace_all(), which replaces text that matches a pattern. Here, we’re using the pattern \"[a-z]\", which matches all lowercase letters a through z, and replacing them with \"\". See the help for ?about_search_regex for more info about how to set patterns (these can get really complex).\n\ngrades1 <- grades1 %>%\n  mutate(id = str_replace_all(\n    id, # the variable you want to search\n    pattern = \"[a-z]\", # find all letters a-z\n    replacement = \"\" # replace with nothing\n  ))  \n\nNow, transform the data type of id so that it matches the data type in grades2.\n\n\n\nSolution\n\n# check variable types\nglimpse(grades1)\nglimpse(grades2) \n\ngrades1 <- grades1 %>%\n  mutate(id = as.numeric(id))\n\n\n\n7.9.3 Complete records\nIn this example, we want to join the grade data to schedule A so that each student with a grade has both the grade and the grade point. But we also want a complete record of all students on the course, so students with missing grades should still be included in the data.\n\nJoin grades1 and scheduleA and store this table in an object named exam_all.\nDo the same for grades2 and save it in essay_all.\nBoth exam_all and essay_all should have 100 observations of 4 variables.\n\n\n\nHint\n\nYou want to keep all of the data from grade_data1 and grade_data2, but you only want the alphanumeric grades from schedule for the Grade Point values that exist in grades. E.g., if no-one was awarded an F1, your final dataset shouldn’t have that in it.\n\n\n\n\nSolution\n\nexam_all <- left_join(grades1, schedule, by = \"Points\")\nessay_all <- left_join(grades2, schedule, by = \"Points\")\n\n\n\n7.9.4 Missing data\nAlternatively, you may wish to have a dataset that only contains data for students who submitted each assessment and have a grade. First, run summary() on both exam_all and essay_all.\n\nHow many exam grades are missing? \n\nHow many essay grades are missing? \n\n\nNow, create an object exam_grades that joins together grades1 and schedule, but this time the resulting object should only contain data from students who have a grade. Do the same but for grades2 and store it in essay_grades.\nBefore you do this, given what you know about how many data points are missing in each data set:\n\nHow many observations should exam_grades have? \n\nHow many observations should essay_grades have? \n\n\n\n\n\nSolution\n\nexam_grades <- inner_join(grades1, schedule, by = \"Points\")\nessay_grades <- inner_join(grades2, schedule, by = \"Points\")\n\n\n\n\nAlternative solution\n\nIt’s worth noting that in reality you wouldn’t actually go back to the raw data and do another join to get this dataset, you could just remove all the missing response by adding %>% drop_na() to exam_all and essay_all. However, for the purposes of teaching joins, we’ll do it this slightly artificial way.\n\nNow, create a dataset completes that joins the grades for students who have a grade for both the essay and the exam.\n\nBecause both exam_grades and essay_grades have the variables Assessment, Points and Grades that are named the same, but have different data, you should amend the suffix so that the resulting variables are named Points_exam and Points_essay etc. You may need to consult the help documentation to see an example to figure this out.\nClean up the file with select() and only keep the variables id, Grade_exam, and Grade_essay\n\n\n\n\n\nSolution\n\ncompletes <- inner_join(exam_grades, essay_grades, \n                        by = \"id\", \n                        suffix = c(\"_exam\", \"_essay\")) %>%\n  select(id, Grade_exam, Grade_essay)\n\n\n\nHow many students have a grade for both the exam and the essay? \n\n\nNow create a dataset no_essay that contains students that have a grade for the exam, but not the essay.\n\n\n\nSolution\n\nno_essay <- anti_join(exam_grades, essay_grades, by = \"id\")\n\n\n\nHow many students have a grade for the exam but not the essay? \n\n\nFinally, now make a dataset no_exam that contains students have have a grade for the essay but not the exam\n\n\n\nSolution\n\nno_exam <- anti_join(essay_grades, exam_grades, by = \"id\")\n\n\n\nHow many students have a grade for the exam but not the essay? \n\n\n7.9.5 Report\nFor the final exercise in this chapter, create a report in R Markdown titled Assessment Report. Assume that you need to present this report at an exam board and you’re likely to be asked for the following information:\n\nHow many students submitted each assessment (essay and exam)?\nWhat was the average performance in each assessment?\nWhat was the distribution of grades for each assessment?\n\nIn preparation for the summative assessment, how you do this and what information you present is up to you. You can use plots or tables to present data, as well as inline code to make sure values reported in the text match the data, even after the underlying data files get updated with late grades. When you’re done, post your code and knitted html document in the week 6 Teams channel."
  },
  {
    "objectID": "07-joins.html#sec-glossary-joins",
    "href": "07-joins.html#sec-glossary-joins",
    "title": "7  Data Relations",
    "section": "\n7.10 Glossary",
    "text": "7.10 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nbase r\n\n\nThe set of R functions that come with a basic installation of R, before you add external packages.\n\n\n\n\nbinding joins\n\n\nJoins that bind one table to another by adding their rows or columns together.\n\n\n\n\ncharacter\n\n\nA data type representing strings of text.\n\n\n\n\nfactor\n\n\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\n\n\nfiltering joins\n\n\nJoins that act like the dplyr::filter() function in that they remove rows from the data in one table based on the values in another table.\n\n\n\n\niteration\n\n\nRepeating a process or function\n\n\n\n\nmutating joins\n\n\nJoins that act like the dplyr::mutate() function in that they add new columns to one table based on values in another table.\n\n\n\n\nnumeric\n\n\nA data type representing a real decimal number or integer.\n\n\n\n\nset operations\n\n\nFunctions that compare two tables and return rows that match (intersect), are in either table (union), or are in one table but not the other (setdiff)."
  },
  {
    "objectID": "07-joins.html#sec-resources-joins",
    "href": "07-joins.html#sec-resources-joins",
    "title": "7  Data Relations",
    "section": "\n7.11 Further resources",
    "text": "7.11 Further resources\n\nData transformation cheatsheet\n\nChapter 13: Relational Data in R for Data Science\n\n\nChapter 21: Iteration in R for Data Science.\npurrr cheatsheet"
  },
  {
    "objectID": "08-tidy.html#sec-ilo-tidy",
    "href": "08-tidy.html#sec-ilo-tidy",
    "title": "8  Data Tidying",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to reshape data between long and wide formats\nSeparate, change, reorder, and rename columns\nUse pipes to chain together functions"
  },
  {
    "objectID": "08-tidy.html#sec-walkthrough-tidy",
    "href": "08-tidy.html#sec-walkthrough-tidy",
    "title": "8  Data Tidying",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "08-tidy.html#sec-setup-tidy",
    "href": "08-tidy.html#sec-setup-tidy",
    "title": "8  Data Tidying",
    "section": "\n8.1 Set-up",
    "text": "8.1 Set-up\nFirst, create a new project for the work we’ll do in this chapter named 08-tidy. Second, open and save and new R Markdown document named tidy.Rmd, delete the welcome text and load the required packages for this chapter.\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse) # for data wrangling\n```\n\n\nYou’ll need to make a folder called “data” and download two data files into it: tidy_data.csv and untidy_data.csv.\nDownload the Data tidying cheat sheet."
  },
  {
    "objectID": "08-tidy.html#data-structures",
    "href": "08-tidy.html#data-structures",
    "title": "8  Data Tidying",
    "section": "\n8.2 Data Structures",
    "text": "8.2 Data Structures\nThe data you work with will likely come in many different formats and structures. Some of these structures may be driven by how the software you use outputs the data, but data structures may also differ because of human intervention or attempts at organisation, some of which may not be particularly helpful.\nData cleaning and tidying will likely be the most time consuming and difficult task you perform. Whilst you can create code recipes for analyses and visualisations, as Hadley Whickham puts it, “every messy dataset is messy in its own way”, which means that you will often have to solve new problems that are specific to the dataset. Additionally, moving between data structures that are intuitive to read by humans and those that are useful for a computer requires a conceptual shift that only comes with practice.\nThis is all a verbose way of saying that what lies ahead in this chapter is unlikely to sink in on the first attempt and you will need to practice with different examples (preferably with data you know well) before you truly feel comfortable with it.\nFirst, some terminology.\nAn observation is all the information about a single “thing” in a single condition, such as at one point in time. These things can be customers, sales, orders, feedback questionnaires, tweets, or really anything. Observations should have a way to identify them, such as a unique ID or a unique combination of values like country and year.\nA variable is one type of information about the observation. For example, if the observation is a sale, the variables you might have about the sale are the sale ID, the customer’s ID, the date of the sale, the price paid, and method of payment.\nA value is the data for one variable for one observation. For example, the value of the date variable from the observation of a sale might be 2021-08-20.\n\n\n\n\n\n\nNote\n\n\n\nThe following table is data that shows the number of items each customer bought each year.\n\n\n\n\n  \n\n\n\n\nWhat is items? \nObservation\nVariable\nValue\n\nHow many observations are there in this dataset? \n\nWhat is 8? \nObservation\nVariable\nValue\n\n\n\n\nExplain these answers\n\n\nThere are three variables, customer_id, year, and items.\nThere are six observations, one for each of two customers for each of three years.\n\n8 is a value because it is a single data point for one variable for one observation.\n\n\n\n\n\n8.2.1 Untidy data\nFirst, let’s have a look at an example of a messy, or untidy, dataset. Each row has all of the data relating to one customer.\n\nuntidy_data <- read_csv(\"data/untidy_data.csv\", show_col_types = FALSE)\n\n\n\n\nUntidy table\n\n\n\ncustomer_id\n\n\nitemsprice_2018\n\n\nitemsprice_2019\n\n\nitemsprice_2020\n\n\ntotalprice_2018\n\n\ntotalprice_2019\n\n\ntotalprice_2020\n\n\n\n\n\n1\n\n\n2 (3.91)\n\n\n8 (4.72)\n\n\n10 (5.59)\n\n\n7.82\n\n\n37.76\n\n\n55.90\n\n\n\n\n2\n\n\n1 (3.91)\n\n\n6 (4.72)\n\n\n1 (5.59)\n\n\n3.91\n\n\n28.32\n\n\n5.59\n\n\n\n\n3\n\n\n4 (3.91)\n\n\n5 (4.72)\n\n\n5 (5.59)\n\n\n15.64\n\n\n23.60\n\n\n27.95\n\n\n\n\n4\n\n\n10 (3.91)\n\n\n1 (4.72)\n\n\n3 (5.59)\n\n\n39.10\n\n\n4.72\n\n\n16.77\n\n\n\n\n5\n\n\n3 (3.91)\n\n\n9 (4.72)\n\n\n8 (5.59)\n\n\n11.73\n\n\n42.48\n\n\n44.72\n\n\n\n\n\nThe itemsprice_{year} columns contain two values (number of items and price per item)\nThe totalprice_{year} columns contain the total amount spent by that customer that year, i.e., items * price.\nThere is data for three different years in the dataset.\n\nLet’s say you wanted to calculate the total price per customer over the three years and the total number of items bought per customer. You can’t perform mathematical operations on the itemsprice_{year} columns because they are character data types.\nYou would probably normally use Excel to\n\nsplit itemsprice_2018 column into item_2018 and price_2018 columns\nsplit itemsprice_2019 column into item_2019 and price_2019 columns\nsplit itemsprice_2020 column into item_2018 and price_2020 columns\nadd item_2018 + item_2019 + item_2020 to get the total number of items bought per customer\nadd totalprice_2018 + totalprice_2019 + totalprice_2020 to get the total price per customer\n\n\n\n\n\n\n\nNote\n\n\n\nThink about how many steps in Excel this would be if there were 10 years in the table, or a different number of years each time you encountered data like this.\n\n\n\n8.2.2 Tidy data\nThere are three rules for “tidy data, which is data in a format that makes it easier to combine data from different tables, create summary tables, and visualise your data.\n\nEach observation must have its own row\nEach variable must have its own column\nEach value must have its own cell\n\nThis is the tidy version:\n\ntidy_data <- read_csv(\"data/tidy_data.csv\", show_col_types = FALSE)\n\n\n\n\nTidy table\n\n\n\ncustomer_id\n\n\nyear\n\n\nitems\n\n\nprice_per_item\n\n\ntotalprice\n\n\n\n\n\n1\n\n\n2018\n\n\n2\n\n\n3.91\n\n\n7.82\n\n\n\n\n1\n\n\n2019\n\n\n8\n\n\n4.72\n\n\n37.76\n\n\n\n\n1\n\n\n2020\n\n\n10\n\n\n5.59\n\n\n55.90\n\n\n\n\n2\n\n\n2018\n\n\n1\n\n\n3.91\n\n\n3.91\n\n\n\n\n2\n\n\n2019\n\n\n6\n\n\n4.72\n\n\n28.32\n\n\n\n\n2\n\n\n2020\n\n\n1\n\n\n5.59\n\n\n5.59\n\n\n\n\n3\n\n\n2018\n\n\n4\n\n\n3.91\n\n\n15.64\n\n\n\n\n3\n\n\n2019\n\n\n5\n\n\n4.72\n\n\n23.60\n\n\n\n\n3\n\n\n2020\n\n\n5\n\n\n5.59\n\n\n27.95\n\n\n\n\n4\n\n\n2018\n\n\n10\n\n\n3.91\n\n\n39.10\n\n\n\n\n4\n\n\n2019\n\n\n1\n\n\n4.72\n\n\n4.72\n\n\n\n\n4\n\n\n2020\n\n\n3\n\n\n5.59\n\n\n16.77\n\n\n\n\n5\n\n\n2018\n\n\n3\n\n\n3.91\n\n\n11.73\n\n\n\n\n5\n\n\n2019\n\n\n9\n\n\n4.72\n\n\n42.48\n\n\n\n\n5\n\n\n2020\n\n\n8\n\n\n5.59\n\n\n44.72\n\n\n\n\n\nThere are now five variables (columns) because there are five different types of information we have for each observation: the customer id, the year, number of items bought, price per item, and total price.\nEach row is a customer’s orders in a particular year.\nThe number of items (items) and price per item (price_per_item) are in separate columns, so now you can perform mathematical operations on them.\n\nTo calculate the total price per customer over the three years and the total number of items bought per customer in R, you could then:\n\ngroup the table by customer_id\nsum the items column to get the total number of items bought per customer\nsum the totalprice column to get the total price per customer\n\n\ntidy_data %>%\n  group_by(customer_id) %>%\n  summarise(\n    total_items = sum(items),\n    total_price = sum(totalprice)\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf there were 10 years in the table, or a different number of years each time you encountered data like this, the code for producing the tables and plots above never changes.\n\n\nIf you have control over how the data are recorded, it will make your life easier to record it in a tidy format from the start. However, we don’t always have control, so this class will also teach you how to convert untidy tables into tidy tables."
  },
  {
    "objectID": "08-tidy.html#reshaping-data",
    "href": "08-tidy.html#reshaping-data",
    "title": "8  Data Tidying",
    "section": "\n8.3 Reshaping Data",
    "text": "8.3 Reshaping Data\nData tables can be in wide format or long format (or a mix of the two). Wide data are where all of the observations about one thing are in the same row, while long data are where each observation is on a separate row. You often need to convert between these formats to do different types of summaries or visualisation. You may have done something similar using pivot tables in Excel.\n\n\n\n\nConverting between wide and long formats using pivot tables in Excel.\n\n\n\n\nIt can be easier to just consider one type of measurement at a time. untidy_data has two types of measurements, total price and price per item. Let’s look at just the totalprice data first.\nWe can select just the columns we want using the dplyr::select() function. This function’s first argument is the data table you want to select from, then each argument after that is either the name of a column in that table, or new_name = old_name. This is a useful function for changing the column names and order of columns, as well as selecting a subset of columns.\n\n\n\n\n\n\nWarning\n\n\n\nNote that because the names of the columns are numbers, they need to be wrapped in backticks otherwise you’ll get an error like:\nError: unexpected '=' in:\n\"  customer_id, \n  2018 =\"\n\n\n\n# select just the customer ID and 3 total price columns\nwide_totalprice <- select(\n  .data = untidy_data,\n  customer_id, \n  `2018` = totalprice_2018,\n  `2019` = totalprice_2019,\n  `2020` = totalprice_2020\n)\n\n\n\n\n\nWide data\n \n customer_id \n    2018 \n    2019 \n    2020 \n  \n\n\n 1 \n    7.82 \n    37.76 \n    55.90 \n  \n\n 2 \n    3.91 \n    28.32 \n    5.59 \n  \n\n 3 \n    15.64 \n    23.60 \n    27.95 \n  \n\n 4 \n    39.10 \n    4.72 \n    16.77 \n  \n\n 5 \n    11.73 \n    42.48 \n    44.72 \n  \n\n\n\n\nThis is in wide format, where each row is a customer, and represents the data from several years. This is a really intuitive way for humans to read a table, but it’s not as easy to process with code.\nThe same data can be represented in a long format by creating a new column that specifies what year the observation is from and a new column that specifies the totalprice of that observation. This is easier to use to make summaries and plots.\n\nlong_totalprice <- pivot_longer(\n  data = wide_totalprice,\n  cols = `2018`:`2020`,\n  names_to = \"year\",\n  values_to = \"totalprice\")\n\n\n\n\n\nLong data\n \n customer_id \n    year \n    totalprice \n  \n\n\n 1 \n    2018 \n    7.82 \n  \n\n 1 \n    2019 \n    37.76 \n  \n\n 1 \n    2020 \n    55.90 \n  \n\n 2 \n    2018 \n    3.91 \n  \n\n 2 \n    2019 \n    28.32 \n  \n\n 2 \n    2020 \n    5.59 \n  \n\n 3 \n    2018 \n    15.64 \n  \n\n 3 \n    2019 \n    23.60 \n  \n\n 3 \n    2020 \n    27.95 \n  \n\n 4 \n    2018 \n    39.10 \n  \n\n 4 \n    2019 \n    4.72 \n  \n\n 4 \n    2020 \n    16.77 \n  \n\n 5 \n    2018 \n    11.73 \n  \n\n 5 \n    2019 \n    42.48 \n  \n\n 5 \n    2020 \n    44.72 \n  \n\n\n\n\nIt also makes it very easy to use with ggplot(). Run the following plot, and consider how you’d make it with the wide version.\n\nggplot(long_totalprice, aes(x = totalprice, fill = year)) +\n  geom_histogram(binwidth = 10, color = \"black\")\n\n\n\nMost plots are easier to make with data in a long format.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCreate a long version of the following table of how many million followers each band has on different social media platforms. You don’t need to use code, just sketch it in a notebook or make a table in a spreadsheet.\n\n\nband\ntwitter\ninstagram\n\n\n\nThe Beatles\n3.8\n3.8\n\n\nThe Rolling Stones\n3.4\n3.1\n\n\nOne Direction\n31.3\n22.8\n\n\n\n\n\nAnswer\n\nYour answer doesn’t need to have the same column headers or be in the same order.\n\n\naccount\nsocial_media\nfollowers\n\n\n\nThe Beatles\ntwitter\n3.8\n\n\nThe Beatles\ninstagram\n3.8\n\n\nThe Rolling Stones\ntwitter\n3.4\n\n\nThe Rolling Stones\ninstagram\n3.1\n\n\nOne Direction\ntwitter\n31.3\n\n\nOne Direction\ninstagram\n322.8\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re a researcher and you’re used to thinking about IVs and DVs, you may find it easier to remember that each IV and DV should have its own column, rather than a column for each level of the IV.\n\n\nThe pivot functions allow you to transform a data table from wide to long or long to wide.\n\n8.3.1 Wide to long\nThe function pivot_longer() converts a wide data table to a longer format by converting the headers from specified columns into the values of new columns, and combining the values of those columns into a new condensed column.\nThis function has several arguments:\n\n\ncols: the columns you want to make long; you can refer to them by their names, like c(`2018`, `2019`, `2020`) or `2018`:`2020` or by their numbers, like c(2, 3, 4) or 2:4\n\n\nnames_to: what you want to call the new columns that the cols column header names will go into\n\nvalues_to: what you want to call the new column that contains the values in the cols\n\n\nWith the pivot functions, it can be easier to show than tell - run the below code and then compare wide_totalprice with long_totalprice and the pivot code and try to map each argument to what has changed.\n\nlong_totalprice <- pivot_longer(\n  data = wide_totalprice, \n  cols = `2018`:`2020`, # columns to make long \n  names_to = \"year\", # new column name for headers\n  values_to = \"totalprice\" # new column name for values\n)\n\n\n8.3.2 Long to wide\nWe can also go from long to wide format using the pivot_wider() function. Instead of returning to the original table with a row for each customer and a column for each year, this new wide table will have a row for each year and a column for each customer. It can be awkward to have numbers for column names, so we use names_prefix to add “C_” before each new column name.\n\n\nid_cols: the column(s) that uniquely identify each new row\n\nnames_from: the column(s) that contain your new column headers\n\nnames_prefix: A prefix to add to the values in the names column\n\nvalues_from: the column that contains the values for the new columns\n\n\nwide_by_customer <- pivot_wider(\n  data = long_totalprice,\n  id_cols = year, # identifying column(s)\n  names_from = customer_id, # the new column names\n  names_prefix = \"C_\", # prefix for new column names\n  values_from = totalprice # the new column values\n)\n\n\n\n\nData made wider with pivot_wider()\n\n\n\nyear\n\n\nC_1\n\n\nC_2\n\n\nC_3\n\n\nC_4\n\n\nC_5\n\n\n\n\n\n2018\n\n\n7.82\n\n\n3.91\n\n\n15.64\n\n\n39.10\n\n\n11.73\n\n\n\n\n2019\n\n\n37.76\n\n\n28.32\n\n\n23.60\n\n\n4.72\n\n\n42.48\n\n\n\n\n2020\n\n\n55.90\n\n\n5.59\n\n\n27.95\n\n\n16.77\n\n\n44.72"
  },
  {
    "objectID": "08-tidy.html#sec-multistep",
    "href": "08-tidy.html#sec-multistep",
    "title": "8  Data Tidying",
    "section": "\n8.4 Multi-step tidying",
    "text": "8.4 Multi-step tidying\nYou often need to go from wide, to long, to an intermediate shape in order to get your data into a format that is useful for plotting, where there is a column for each variable that you want to represent with an aesthetic.\nOur full untidy_data table has seven columns: a customer ID, three columns for itemsprice and 3 columns for totalprice.\n\n\n\n\nWe want to get it into the tidy format below where each row is an observation of one customer per year, with the columns of customer_id, year, item, price_per_item and totalprice. Before trying to reshape any dataset, you should be able to visualise what it will look like. Sketching out your tables on a piece of paper can really help make these transformations make sense.\n\n\n\n\n\n8.4.1 One observation per row\nThe original table has observations from each customer over three years. This is too many observations per row, so first we’ll start by making the table long. We need to make 6 rows for each customer, one for each category (item price/total price) and year combination, with columns for the customer ID, year, category, and value.\nBecause we’ll be combining columns with numeric (totalprice) and character (itemsprice) data, we need to make the new value column a character data type using values_transform, since numbers can be represented as characters (like \"3.5\"), but character strings can’t be represented as numbers.\nThe argument names_sep is set to the character string used to join names if names_from is more than one column. Alternatively, you can use the argument names_pattern, which can be more powerful but also a little harder to understand how to set up.\n\nlonger_data <- pivot_longer(\n  data = untidy_data, \n  cols = itemsprice_2018:totalprice_2020, # columns to make long \n  names_to = c(\"category\", \"year\"), # new column names for cols\n  names_sep = \"_\", # how to split cols into new columns\n  # names_pattern = \"(.*)_(.*)\", # alternative to names_sep\n  values_to = \"value\", # new column name for values\n  \n  # make sure new columns are the right data type\n  names_transform = list(year = as.integer),\n  values_transform = list(value = as.character) \n)\n\n\n\n\nUntidy data converted from wide to long.\n\n\n\ncustomer_id\n\n\ncategory\n\n\nyear\n\n\nvalue\n\n\n\n\n\n1\n\n\nitemsprice\n\n\n2018\n\n\n2 (3.91)\n\n\n\n\n1\n\n\nitemsprice\n\n\n2019\n\n\n8 (4.72)\n\n\n\n\n1\n\n\nitemsprice\n\n\n2020\n\n\n10 (5.59)\n\n\n\n\n1\n\n\ntotalprice\n\n\n2018\n\n\n7.82\n\n\n\n\n1\n\n\ntotalprice\n\n\n2019\n\n\n37.76\n\n\n\n\n1\n\n\ntotalprice\n\n\n2020\n\n\n55.9\n\n\n\n\n2\n\n\nitemsprice\n\n\n2018\n\n\n1 (3.91)\n\n\n\n\n2\n\n\nitemsprice\n\n\n2019\n\n\n6 (4.72)\n\n\n\n\n2\n\n\nitemsprice\n\n\n2020\n\n\n1 (5.59)\n\n\n\n\n2\n\n\ntotalprice\n\n\n2018\n\n\n3.91\n\n\n\n\n2\n\n\ntotalprice\n\n\n2019\n\n\n28.32\n\n\n\n\n2\n\n\ntotalprice\n\n\n2020\n\n\n5.59\n\n\n\n\n3\n\n\nitemsprice\n\n\n2018\n\n\n4 (3.91)\n\n\n\n\n3\n\n\nitemsprice\n\n\n2019\n\n\n5 (4.72)\n\n\n\n\n3\n\n\nitemsprice\n\n\n2020\n\n\n5 (5.59)\n\n\n\n\n3\n\n\ntotalprice\n\n\n2018\n\n\n15.64\n\n\n\n\n3\n\n\ntotalprice\n\n\n2019\n\n\n23.6\n\n\n\n\n3\n\n\ntotalprice\n\n\n2020\n\n\n27.95\n\n\n\n\n4\n\n\nitemsprice\n\n\n2018\n\n\n10 (3.91)\n\n\n\n\n4\n\n\nitemsprice\n\n\n2019\n\n\n1 (4.72)\n\n\n\n\n4\n\n\nitemsprice\n\n\n2020\n\n\n3 (5.59)\n\n\n\n\n4\n\n\ntotalprice\n\n\n2018\n\n\n39.1\n\n\n\n\n4\n\n\ntotalprice\n\n\n2019\n\n\n4.72\n\n\n\n\n4\n\n\ntotalprice\n\n\n2020\n\n\n16.77\n\n\n\n\n5\n\n\nitemsprice\n\n\n2018\n\n\n3 (3.91)\n\n\n\n\n5\n\n\nitemsprice\n\n\n2019\n\n\n9 (4.72)\n\n\n\n\n5\n\n\nitemsprice\n\n\n2020\n\n\n8 (5.59)\n\n\n\n\n5\n\n\ntotalprice\n\n\n2018\n\n\n11.73\n\n\n\n\n5\n\n\ntotalprice\n\n\n2019\n\n\n42.48\n\n\n\n\n5\n\n\ntotalprice\n\n\n2020\n\n\n44.72\n\n\n\n\n8.4.2 One variable per column\nNow this table is long, but not tidy. The value column contains data from two different variables. We need to make the table wider, but not as wide as before. We want to keep the year column and make new columns called itemsprice and totalprice with the relevant customer’s value for that variable and year.\n\nwider_data <- pivot_wider(\n  data = longer_data,\n  id_cols = c(customer_id, year),\n  names_from = category,\n  values_from = value\n)\n\n\n\n\nData converted from long to an intermediate shape.\n\n\n\ncustomer_id\n\n\nyear\n\n\nitemsprice\n\n\ntotalprice\n\n\n\n\n\n1\n\n\n2018\n\n\n2 (3.91)\n\n\n7.82\n\n\n\n\n1\n\n\n2019\n\n\n8 (4.72)\n\n\n37.76\n\n\n\n\n1\n\n\n2020\n\n\n10 (5.59)\n\n\n55.9\n\n\n\n\n2\n\n\n2018\n\n\n1 (3.91)\n\n\n3.91\n\n\n\n\n2\n\n\n2019\n\n\n6 (4.72)\n\n\n28.32\n\n\n\n\n2\n\n\n2020\n\n\n1 (5.59)\n\n\n5.59\n\n\n\n\n3\n\n\n2018\n\n\n4 (3.91)\n\n\n15.64\n\n\n\n\n3\n\n\n2019\n\n\n5 (4.72)\n\n\n23.6\n\n\n\n\n3\n\n\n2020\n\n\n5 (5.59)\n\n\n27.95\n\n\n\n\n4\n\n\n2018\n\n\n10 (3.91)\n\n\n39.1\n\n\n\n\n4\n\n\n2019\n\n\n1 (4.72)\n\n\n4.72\n\n\n\n\n4\n\n\n2020\n\n\n3 (5.59)\n\n\n16.77\n\n\n\n\n5\n\n\n2018\n\n\n3 (3.91)\n\n\n11.73\n\n\n\n\n5\n\n\n2019\n\n\n9 (4.72)\n\n\n42.48\n\n\n\n\n5\n\n\n2020\n\n\n8 (5.59)\n\n\n44.72\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTechinically, you can skip setting the id_cols argument, because all of the columns apart from the names_from column and the values_from column identify the observation (e.g., each observation is identified by the unique combination of customer_id and year). You only have to set the id_cols argument when this is not the case.\n\n\n\n8.4.3 One value per cell\nThe cells in the itemsprice column actually contain two different values. We need to split it into two columns for the variables items, and price_per_item. You can split a column into parts with the function tidyr::separate(). There is a space between the number of items and the brackets, so we can split it along this space – if you are in charge of how data is stored, ensuring data is entered consistently makes this much easier.\n\nsplit_data <- separate(\n  data = wider_data, \n  col = itemsprice, # the column to split\n  into = c(\"items\", \"price_per_item\"), # the new columns to create\n  sep = \" \", # split col by space\n  remove = TRUE, # whether to remove to old col\n  convert = TRUE # whether to fix the data type of the new columns\n)\n\n\n\n\nThe itemsprice column split into items and price_per_item using separate()\n\n\n\ncustomer_id\n\n\nyear\n\n\nitems\n\n\nprice_per_item\n\n\ntotalprice\n\n\n\n\n\n1\n\n\n2018\n\n\n2\n\n\n(3.91)\n\n\n7.82\n\n\n\n\n1\n\n\n2019\n\n\n8\n\n\n(4.72)\n\n\n37.76\n\n\n\n\n1\n\n\n2020\n\n\n10\n\n\n(5.59)\n\n\n55.9\n\n\n\n\n2\n\n\n2018\n\n\n1\n\n\n(3.91)\n\n\n3.91\n\n\n\n\n2\n\n\n2019\n\n\n6\n\n\n(4.72)\n\n\n28.32\n\n\n\n\n2\n\n\n2020\n\n\n1\n\n\n(5.59)\n\n\n5.59\n\n\n\n\n3\n\n\n2018\n\n\n4\n\n\n(3.91)\n\n\n15.64\n\n\n\n\n3\n\n\n2019\n\n\n5\n\n\n(4.72)\n\n\n23.6\n\n\n\n\n3\n\n\n2020\n\n\n5\n\n\n(5.59)\n\n\n27.95\n\n\n\n\n4\n\n\n2018\n\n\n10\n\n\n(3.91)\n\n\n39.1\n\n\n\n\n4\n\n\n2019\n\n\n1\n\n\n(4.72)\n\n\n4.72\n\n\n\n\n4\n\n\n2020\n\n\n3\n\n\n(5.59)\n\n\n16.77\n\n\n\n\n5\n\n\n2018\n\n\n3\n\n\n(3.91)\n\n\n11.73\n\n\n\n\n5\n\n\n2019\n\n\n9\n\n\n(4.72)\n\n\n42.48\n\n\n\n\n5\n\n\n2020\n\n\n8\n\n\n(5.59)\n\n\n44.72\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the new columns should have a different data type from the old column, set convert = TRUE to automatically fix them. This is common when you have columns that contain multiple numbers, separated by commas or semicolons. These are character types before they are separated, but should be numeric types after so that you can do mathematical operations like sum them.\n\n\n\n8.4.4 Altering data\nThe column price_per_item is still a character column because it has parentheses. There are a few ways to fix this. You can use the dplyr::mutate() function to change a column or add a new one.\nHere, we’ll use stringr::str_replace_all() to replace all of the “(” and “)” with ““.\n\nmutated_data <- mutate(\n  .data = split_data,\n  price_per_item = stringr::str_replace_all(\n    string = price_per_item, \n    pattern = \"[()]\", \n    replacement = \"\"\n  )\n)\n\n\n\n\nMutating data to remove the parentheses from price_per_item.\n\n\n\ncustomer_id\n\n\nyear\n\n\nitems\n\n\nprice_per_item\n\n\ntotalprice\n\n\n\n\n\n1\n\n\n2018\n\n\n2\n\n\n3.91\n\n\n7.82\n\n\n\n\n1\n\n\n2019\n\n\n8\n\n\n4.72\n\n\n37.76\n\n\n\n\n1\n\n\n2020\n\n\n10\n\n\n5.59\n\n\n55.9\n\n\n\n\n2\n\n\n2018\n\n\n1\n\n\n3.91\n\n\n3.91\n\n\n\n\n2\n\n\n2019\n\n\n6\n\n\n4.72\n\n\n28.32\n\n\n\n\n2\n\n\n2020\n\n\n1\n\n\n5.59\n\n\n5.59\n\n\n\n\n3\n\n\n2018\n\n\n4\n\n\n3.91\n\n\n15.64\n\n\n\n\n3\n\n\n2019\n\n\n5\n\n\n4.72\n\n\n23.6\n\n\n\n\n3\n\n\n2020\n\n\n5\n\n\n5.59\n\n\n27.95\n\n\n\n\n4\n\n\n2018\n\n\n10\n\n\n3.91\n\n\n39.1\n\n\n\n\n4\n\n\n2019\n\n\n1\n\n\n4.72\n\n\n4.72\n\n\n\n\n4\n\n\n2020\n\n\n3\n\n\n5.59\n\n\n16.77\n\n\n\n\n5\n\n\n2018\n\n\n3\n\n\n3.91\n\n\n11.73\n\n\n\n\n5\n\n\n2019\n\n\n9\n\n\n4.72\n\n\n42.48\n\n\n\n\n5\n\n\n2020\n\n\n8\n\n\n5.59\n\n\n44.72\n\n\n\n\n8.4.5 Fixing data types\nThe price_per_item and totalprice columns are still characters, so you can’t do things like calculate the sum of totalprice.\n\n# check the data types\nglimpse(mutated_data)\n\nRows: 15\nColumns: 5\n$ customer_id    <dbl> 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5\n$ year           <int> 2018, 2019, 2020, 2018, 2019, 2020, 2018, 2019, 2020, 2…\n$ items          <int> 2, 8, 10, 1, 6, 1, 4, 5, 5, 10, 1, 3, 3, 9, 8\n$ price_per_item <chr> \"3.91\", \"4.72\", \"5.59\", \"3.91\", \"4.72\", \"5.59\", \"3.91\",…\n$ totalprice     <chr> \"7.82\", \"37.76\", \"55.9\", \"3.91\", \"28.32\", \"5.59\", \"15.6…\n\n\nOnce the data are clean and tidy, you can fix all of your column data types in one step using readr::type_convert(). This is good practice when you’ve finished cleaning a data set. If the automatic type detection doesn’t work as expected, this usually means that you still have non-numeric characters in a column where there were only supposed to be numbers. You can also manually set the column types in the same way as for readr::read_csv() (see Chapter 4)).\n\ntidy_data <- type_convert(\n  df = mutated_data,\n  trim_ws = TRUE # removes spaces before and after values\n)\n\n# check the data types\nglimpse(tidy_data)\n\nRows: 15\nColumns: 5\n$ customer_id    <dbl> 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5\n$ year           <int> 2018, 2019, 2020, 2018, 2019, 2020, 2018, 2019, 2020, 2…\n$ items          <int> 2, 8, 10, 1, 6, 1, 4, 5, 5, 10, 1, 3, 3, 9, 8\n$ price_per_item <dbl> 3.91, 4.72, 5.59, 3.91, 4.72, 5.59, 3.91, 4.72, 5.59, 3…\n$ totalprice     <dbl> 7.82, 37.76, 55.90, 3.91, 28.32, 5.59, 15.64, 23.60, 27…"
  },
  {
    "objectID": "08-tidy.html#sec-pipes",
    "href": "08-tidy.html#sec-pipes",
    "title": "8  Data Tidying",
    "section": "\n8.5 Pipes",
    "text": "8.5 Pipes\n\n\n\nWe’ve already introduced pipes in Section 5.3.2 but this type of data processing is where they really start to shine, as they can significantly reduce the amount of code you write.\nAs a recap, a pipe takes the result of the previous function and sends it to the next function as its first argument, which means that you do not need to create intermediate objects. Below is all the code we’ve used in this chapter, and in the process we created five objects. This can get very confusing in longer scripts.\n\nuntidy_data <- read_csv(\"data/untidy_data.csv\", \n                        show_col_types = FALSE)\n\nlonger_data <- pivot_longer(\n  data = untidy_data,\n  cols = itemsprice_2018:totalprice_2020,\n  names_to = c(\"category\", \"year\"),\n  names_sep = \"_\", \n  values_to = \"value\", \n  names_transform = list(year = as.integer),\n  values_transform = list(value = as.character) \n) \n\nwider_data <- pivot_wider(\n  data = longer_data,\n  id_cols = c(customer_id, year),\n  names_from = category,\n  values_from = value\n)\n\nsplit_data <- separate(\n  data = wider_data,\n  col = itemsprice,\n  into = c(\"items\", \"price_per_item\"),\n  sep = \" \", \n  remove = TRUE, \n  convert = TRUE\n) \n\nmutated_data <- mutate(\n  .data = split_data,\n  price_per_item = stringr::str_replace_all(\n    string = price_per_item, \n    pattern = \"[()]\", \n    replacement = \"\"\n  )\n) \n\ntidy_data <- type_convert(\n  df = mutated_data,\n  trim_ws = TRUE\n)\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can give each object the same name and keep replacing the old data object with the new one at each step. This will keep your environment clean, but it makes debugging code much harder.\n\n\nFor longer series of steps like the one above, using pipes can eliminate many intermediate objects. This also makes it easier to add an intermediate step to your process without having to think of a new table name and edit the table input to the next step (which is really easy to accidentally miss).\n\ntidy_data <- read_csv(file = \"data/untidy_data.csv\",\n                      show_col_types = FALSE) %>%\n  pivot_longer(\n    cols = itemsprice_2018:totalprice_2020,\n    names_to = c(\"category\", \"year\"),\n    names_sep = \"_\", \n    values_to = \"value\", \n    names_transform = list(year = as.integer),\n    values_transform = list(value = as.character) \n  ) %>%\n  pivot_wider(\n    id_cols = c(customer_id, year),\n    names_from = category,\n    values_from = value\n  ) %>%\n  separate(\n    col = itemsprice,\n    into = c(\"items\", \"price_per_item\"),\n    sep = \" \", \n    remove = TRUE, \n    convert = TRUE\n  ) %>%\n  mutate(\n    price_per_item = stringr::str_replace_all(\n      string = price_per_item, \n      pattern = \"[()]\", \n      replacement = \"\"\n    )\n  ) %>%\n  type_convert(\n    trim_ws = TRUE\n  )\n\nYou can read the code above like this:\n\n\nRead the data with read_csv()\n\n\nfile: from the file at r path(“data/untidy_data.csv”)`,\n\nshow_col_types: do not show the colukmn types message; and then\n\n\n\n\nReshape the data longer with pivot_longer()\n\n\ncols: take the columns from itemsprice_2018 to totalprice_2020,\n\nnames_to: create new columns called “category” and “year” from the cols header names,\n\nnames_sep: separate the column names using “_”\n\nvalues_to: create a new column called “value” from the cols values,\n\nnames_transform: transform the year column to integers,\n\nvalues_transform: transform the value column to characters; and then\n\n\n\n\nReshape the data wider with pivot_wider()\n\n\nid_cols: each row should be an observation of a unique customer_id and year,\n\nnames_from: get the new column names from the values in the category column,\n\nvalues_from: get the new column values from the values in the value column; and then\n\n\n\n\nSplit multiple values in the same column with separate()\n\n\ncol: separate the column itemsprice,\n\ninto: into new columns called “items” and “price_per_item”,\n\nsep: separate the values at each ” “,\n\nremove: do remove the old column,\n\nconvert: do convert the new columns into the right data types; and then\n\n\n\n\nChange a column with mutate()\n\n\nprice_per_item: replace the existing column price_per_item with the result of a search and replace with str_replace_all():\n\n\nstring: the strings to modify come from the price_per_item columns,\n\npattern: search for left or right parentheses,\n\nreplacement: replace them with ““; and then,\n\n\n\n\n\nFix data types with type_convert()\n\n\ntrim_ws: remove spaces, tabs, and line breaks from the start and end of each value\n\n\n\nDon’t feel like you always need to get all of your data wrangling code into a single pipeline. You should make intermediate objects whenever you need to break up your code because it’s getting too complicated or if you need to debug something.\n\n\n\n\n\n\nNote\n\n\n\nYou can debug a pipe by highlighting from the beginning to just before the pipe you want to stop at. Try this by highlighting from data <- to the end of the separate function and typing command-enter (mac) or control-enter (PC). What does data look like now?"
  },
  {
    "objectID": "08-tidy.html#exercises",
    "href": "08-tidy.html#exercises",
    "title": "8  Data Tidying",
    "section": "\n8.6 Exercises",
    "text": "8.6 Exercises\nLet’s try a couple of examples.\n\nSave your current Markdown, close it, and open a new Rmd named “Patient_survey”.\nDownload a copy of wide_exercise-1.csv and wide_exercise-2.csv into your data folder.\nIn the set-up code chunk, load the tidyverse then load the two data files in using read_csv() and name the objects wide1 and wide2\n\n\n\n\n\nSolution\n\nlibrary(tidyverse)\nwide1 <- read_csv(\"data/wide_exercise-1.csv\")\nwide2 <- read_csv(\"data/wide_exercise-2.csv\")\n\n\nThe two datasets represent simulated data from a patient satisfaction survey. We’ll do them one at a time, as they differ in complexity.\n\n8.6.1 Survey 1\nwide1 has data from 50 patients who were asked five questions about their most recent experience at a health centre. The results from this questionnaire are typically reported as a single overall satisfaction score, which is calculated by taking the mean of the five responses. Additionally, the survey also records whether the patient was attending the clinic for the first time, or as a repeat patient.\n\nUse your method of choice to look at the dataset and familiarise yourself with its structure and data.\n\nAs noted, it’s important to think through what your tidied data should look like. Often, the problem with data wrangling in R isn’t actually the code, it’s a lack of understanding of the data that’s being worked on.\n\nHow many variables should the long-form version of wide have? \n\nHow many observations should the long-form version of wide1 have? \n\n\n\n\nExplain these answers\n\n\nThere should be four variables, as there are 4 types of data: patient id, whether they are a repeat patient, the question they were asked, and their response.\nThere will be 250 observations or rows of data because each patient will have 5 rows of data (one per question) and there are 50 patients (50 * 5 = 250).\n\n\n\n8.6.2 Tidy 1\nTransform wide1 to long-form using pivot_longer() and store it in an object named tidy1\n\n\n\nSolution\n\ntidy1 <- wide1 %>%\n  pivot_longer(cols = q1:q5,\n               names_to = \"question\", \n               values_to = \"response\")\n\n\n\n8.6.3 Survey 2\nwide2 also has data from 50 patients, however, there are now two measures included in the questionnaire. There are still five questions that relate to satisfaction, but there are also five questions that relate to whether the patient would recommend the medical practice to a friend. Both measures are typically reported by calculating an overall mean for each of the five items.\n\nUse your method of choice to look at the dataset and familiarise yourself with its structure and data.\n\nThis is not as simple as the first exercise because there’s actually two potential ways you might tidy this data, depending on what you want to do with it and how you conceptualise the two different measurements. It’s important to recognise that many of your coding problems will not have just one solution.\nTidy 2a\nFor the first option, we’re going to treat the “satisfaction” and “recommendation” measurements as two categories of the same variable. This will be a fully long-form data set with five variables id, repeat_patient, question (the question number), category (whether it’s sat or rec), and response (the numerical rating).\n\nHow many observations should the fully long-form version of wide2 have? \n\n\n\n\nExplain this answer\n\nThere will be 500 rows of data because each participant will have 10 rows: 5 for the satisfaction questions and five for the recommendation questions.\n\nTransform wide2 to full long-form using pivot_longer() and store it in an object named tidy2a.\nThis exercise requires multiple steps and you may need to look at the help documentation.\n\n\nHint 1\n\ndata %>% pivot_longer() %>% separate()\n\n\n\nHint 2\n\ninto  = c(\"col1\", \"col2\")\n\n\n\n\nSolution\n\ntidy2a <- wide2 %>%\n  pivot_longer(cols = q1_sat:q5_rec,\n               names_to = \"question\", \n               values_to = \"response\") %>%\n  separate(col = \"question\", into = c(\"question\", \"category\"))\n\n\n\n\n\nAlternative solution\n\n# combine pivot_longer and separate by setting two values for names_to\n# must include names_sep to determine how to separate the column names\ntidy2a <- wide2 %>%\n  pivot_longer(cols = q1_sat:q5_rec,\n               names_to = c(\"question\", \"category\"), \n               names_sep = \"_\",\n               values_to = \"response\")\n\n\nTidy 2b\nThe second option is to treat the satisfaction and recommendation scores as two distinct variables. This only makes sense if the satisfaction and recommendation scores for each question number are related to each other (e.g., q1 is about the same thing for both questions), making them part of the same observation.\nThis version should also have five variables, but it won’t be fully long-form, it’ll be a slight mix of the two that we’re going to call “semi-long”. The variables in the semi-long version will be id, repeat, question (the question number), sat (the response for the satisfaction question), and rec (the response for the recommendation question).\n\nHow many observations should the semi-long version of wide2 have? \n\n\n\n\nExplain this answer\n\nThere will be 250 rows of data because, just like tidy1, each participant will have 5 rows, one for each of the five questions. The different responses to the satisfaction and recommendation questions are in different variables.\n\nThis also takes multiple steps.\n\n\nHint 1\n\nYou can reuse the code from tidy2a, you just need to add on an extra line that makes the data slightly wider.\n\n\n\nHint 2\n\ndata %>% pivot_longer() %>% separate() %>% pivot_wider()\n\n\n\n\nSolution\n\ntidy2b <- wide2 %>%\n  pivot_longer(cols = q1_sat:q5_rec,\n               names_to = \"question\", \n               values_to = \"response\") %>%\n  separate(col = \"question\", into = c(\"question\", \"category\")) %>%\n  pivot_wider(names_from = \"category\", values_from = \"response\")\n\n\n\n8.6.4 Analysis and visualisation\nUsing group_by() and summarise(), calculate the mean score for each participant for both satisfaction and recommendation. Do this for both versions of the dataset so that you can see how the structure of the dataset changes the approach you need to take.\n\n\n\nSolution\n\ntidy2a %>%\n  group_by(id, category) %>%\n  summarise(mean = mean(response),\n            .groups = \"drop\")\n\ntidy2b %>%\n  group_by(id) %>%\n  summarise(mean_satisfaction = mean(sat),\n            mean_rec = mean(rec))\n\n\nReplicate the following:\nPlot 1\nScatterplot showing the relationship between satisfaction and recommendation scores, by whether the patient is a repeat patient.\n\n\nHint\n\ngeom_jitter()\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nggplot(tidy2b, aes(x = sat, y = rec, colour = repeat_patient)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Satisfaction score\", y = \"Recommendation score\", title = \"Satisfaction and recommendation scores\") +\n  theme_classic()\n\n\nPlot 2\nBoxplots showing satisfaction and recommends scores for new and repeat patients separately.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nggplot(tidy2a, aes(x = repeat_patient, y = response, fill = repeat_patient)) +\n  geom_boxplot(show.legend = FALSE) +\n  facet_wrap(~category)+\n  theme_bw() +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\nPlot 3\nHistogram showing the distribution of all responses, across questions and categories.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nggplot(tidy2a, aes(x = response)) +\n  geom_histogram(binwidth = 1, colour = \"black\", fill = \"Grey\") +\n  labs(x = \"Responses across all questions and categories\") +\n  theme_bw()\n\n\n\n8.6.5 Your data\nFinally, find a wide-form dataset of your own and try and tidy it into long-form. If you get stuck or you just want to check your solution, post it on Teams - just remember to be careful about sharing confidential data if it’s your own dataset.\nIf your head hurts a bit at this point, rest assured it’s absolutely normal. As we said at the start, reshaping and tidying data is a conceptual leap and there’s no shortcut to the fact it just takes a bit of time and practice with different datasets - you will get there eventually!"
  },
  {
    "objectID": "08-tidy.html#sec-glossary-tidy",
    "href": "08-tidy.html#sec-glossary-tidy",
    "title": "8  Data Tidying",
    "section": "\n8.7 Glossary",
    "text": "8.7 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\ncharacter\n\n\nA data type representing strings of text.\n\n\n\n\ndata type\n\n\nThe kind of data represented by an object.\n\n\n\n\nlong\n\n\nA data format where each observation is on a separate row\n\n\n\n\nobservation\n\n\nAll of the data about a single trial or question.\n\n\n\n\ntidy data\n\n\nA format for data that maps the meaning onto the structure.\n\n\n\n\nvalue\n\n\nA single number or piece of data.\n\n\n\n\nvariable\n\n\n(coding): A word that identifies and stores the value of some data for later use; (stats): An attribute or characteristic of an observation that you can measure, count, or describe\n\n\n\n\nwide\n\n\nA data format where all of the observations about one subject are in the same row"
  },
  {
    "objectID": "08-tidy.html#sec-resources-tidy",
    "href": "08-tidy.html#sec-resources-tidy",
    "title": "8  Data Tidying",
    "section": "\n8.8 Further resources",
    "text": "8.8 Further resources\n\nData tidying cheat sheet\nTidy Data\n\nChapter 12: Tidy Data in R for Data Science\n\n\nChapter 18: Pipes in R for Data Science"
  },
  {
    "objectID": "09-wrangle.html#sec-ilo-wrangle",
    "href": "09-wrangle.html#sec-ilo-wrangle",
    "title": "9  Data Wrangling",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\n\nBe able to select and filter data for relevance\nBe able to create new columns and edit existing ones\nBe able to handle missing data"
  },
  {
    "objectID": "09-wrangle.html#sec-walkthrough-wrangle",
    "href": "09-wrangle.html#sec-walkthrough-wrangle",
    "title": "9  Data Wrangling",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360. Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "09-wrangle.html#sec-setup-wrangle",
    "href": "09-wrangle.html#sec-setup-wrangle",
    "title": "9  Data Wrangling",
    "section": "\n9.1 Set-up",
    "text": "9.1 Set-up\nFirst, create a new project for the work we’ll do in this chapter named 09-wrangle. Second, open and save and new R Markdown document named wrangle.Rmd, delete the welcome text and load the required packages for this chapter.\n\n\n\nChapter packages\n\nlibrary(tidyverse)   # data wrangling functions\n\n\nYou’ll need to make a folder called “data” and download a data file into it: budget.csv.\nDownload the Data transformation cheat sheet."
  },
  {
    "objectID": "09-wrangle.html#wrangling-functions",
    "href": "09-wrangle.html#wrangling-functions",
    "title": "9  Data Wrangling",
    "section": "\n9.2 Wrangling functions",
    "text": "9.2 Wrangling functions\nData wrangling refers to the process of cleaning, transforming, and restructuring your data to get it into the format you need for analysis and it’s something you will spend an awful lot of time doing. Most data wrangling involves the reshaping functions you learned in Chapter 8 and six functions from the dplyr package that is loaded as part of the tidyverse: select, filter, arrange, mutate, summarise, and group_by. You’ll remember the last two from Chapter 5, so we’ll only cover them briefly.\nIt’s worth highlighting that in this chapter we’re going to cover these common functions and common uses of said functions. However, dplyr (and packages beyond it) has a huge number of additional wrangling functions and each function has many different arguments. Essentially, if you think you should be able to wrangle your data in a particular way that we haven’t explicitly shown you, you almost certainly can, it might just take a bit of Googling to find out how.\n\n\n\nWe’ll use a small example table with the sales, expenses, and satisfaction for two years from four regions over two products. After you load the data, use glimpse(budget) or View(budget) to get familiar with the data.\n\nbudget <- read_csv(\"data/budget.csv\", show_col_types = FALSE)\n\n\n\n\n\n  \n\n\n\n\n9.2.1 Select\nYou can select a subset of the columns (variables) in a table to make it easier to view or to prepare a table for display. You can also select columns in a new order.\nBy name or index\nYou can select columns by name or number (which is sometimes referred to as the column index). Selecting by number can be useful when the column names are long or complicated.\n\n# select single column by name\nproduct_dat <- budget %>% select(product) \n\n# select single column by number\nproduct_dat <- budget %>% select(2) \n\nYou can select each column individually, separated by commas (e.g., region, sales_2019) but you can also select all columns from one to another by separating them with a colon (e.g., sales_2019:expenses_2020).\nThe colon notation can be much faster because you don’t need to type out each individual variable name, but make sure that you know what order your columns are in and always check the output to make sure you have selected what you intended.\n\n# select columns individually\nsales2019 <- budget %>% select(region, product, sales_2019)\n\n# select columns with colon\nsales2019 <- budget %>% select(region:sales_2019)\n\nYou can rename columns at the same time as selecting them by setting new_name = old_col.\n\nregions <- budget %>% select(`Sales Region` = 1, 3:6)\n\nhead(regions, 2)\n\n\n\n  \n\n\n\nUn-selecting columns\nYou can select columns either by telling R which ones you want to keep as in the previous examples, or by specifying which ones you want to exclude by using a minus symbol to un-select columns. You can also use the colon notation to de-select columns, but to do so you need to put parentheses around the span first, e.g., -(sales_2019:expenses_2020), not -sales_2019:expenses_2020.\n\n# de-select individual columns\nsales <- budget %>% select(-expenses_2019, -expenses_2020)\n\n# de-select a range of columns\nsales <- budget %>% select(-(expenses_2019:expenses_2020))\n\nSelect helpers\nFinally, you can select columns based on criteria about the column names.\n\n\n\n\n\n\nfunction\ndefinition\n\n\n\nstarts_with()\nselect columns that start with a character string\n\n\nends_with()\nselect columns that end with a character string\n\n\ncontains()\nselect columns that contain a character string\n\n\nnum_range()\nselect columns with a name that matches the pattern prefix\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat are the resulting columns for these four examples?\n\n\nbudget %>% select(contains(\"_\"))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\nbudget %>% select(num_range(\"expenses_\", 2019:2020))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\nbudget %>% select(starts_with(\"sales\"))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\nbudget %>% select(ends_with(\"2020\"))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\n\n\n\n9.2.2 Filter\nWhilst select() chooses the columns you want to retain, filter() chooses the rows to retain by matching row or column criteria.\nYou can filter by a single criterion. This criterion can be rows where a certain column’s value matches a character value (e.g., “North”) or a number (e.g., 9003). It can also be the result of a logical equation (e.g., keep all rows with a specific column value larger than a certain value). The criterion is checked for each row, and if the result is FALSE, the row is removed. You can reverse equations by specifying != where ! means “not”.\n\n# select all rows where region equals North\nbudget %>% filter(region == \"North\")\n\n# select all rows where expenses_2020 were exactly equal to 200\nbudget %>% filter(expenses_2020 == 200)\n\n# select all rows where sales_2019 was more than 100\nbudget %>% filter(sales_2019 > 100)\n\n# everything but the North\nbudget %>% filter(region != \"North\")\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember to use == and not = to check if two things are equivalent. A single = assigns the right-hand value to the left-hand variable (much like the <- operator).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhich IDs are kept from the table below?\n\n\n\n\n  \n\n\n\n\n\ndemo %>% filter(score < 80) \n1, 2\n2\n3\n3, 4\n\n\ndemo %>% filter(grade == \"A\") \n1, 2\n2\n3\n3, 4\n\n\ndemo %>% filter(grade != \"A\") \n1, 2\n2\n3\n3, 4\n\n\ndemo %>% filter(score == 91) \n1, 2\n2\n3\n3, 4\n\n\n\n\nYou can also select on multiple criteria by separating them by commas (rows will be kept if they match all criteria). Additionally, you can use & (“and”) and | (“or”) to create complex criteria.\n\n# regions and products with profit in both 2019 and 2020\nprofit_both <- budget %>% \n  filter(\n    sales_2019 > expenses_2019,\n    sales_2020 > expenses_2020\n  )\n\n# the same as above, using & instead of a comma\nprofit_both <- budget %>% \n  filter(\n    sales_2019 > expenses_2019 &\n    sales_2020 > expenses_2020\n  )\n\n# regions and products with profit in 2019 or 2020\nprofit_either <- budget %>% \n  filter(\n    sales_2019 > expenses_2019 |\n    sales_2020 > expenses_2020\n  )\n\n# 2020 profit greater than 1000\nprofit_1000 <- budget %>%\n  filter(sales_2020 - expenses_2020 > 1000)\n\nIf you want the filter to retain multiple specific values in the same variable, the match operator (%in%) should be used rather than | (or). The ! can also be used in combination here, but it is placed before the variable name.\n\n# retain any rows where region is north or south, and where product equals widget\nbudget %>%\n  filter(region %in% c(\"North\", \"South\"),\n         product == \"widgets\")\n\n# retain any rows where the region is not east or west, and where the product does not equal gadgets\nbudget %>%\n  filter(!region %in% c(\"East\", \"West\"),\n         product != \"gadgets\")\n\n\n\n\n\n\n\n\nOperator\nName\nis TRUE if and only if\n\n\n\nA < B\nless than\nA is less than B\n\n\nA <= B\nless than or equal\nA is less than or equal to B\n\n\nA > B\ngreater than\nA is greater than B\n\n\nA >= B\ngreater than or equal\nA is greater than or equal to B\n\n\nA == B\nequivalence\nA exactly equals B\n\n\nA != B\nnot equal\nA does not exactly equal B\n\n\nA %in% B\nin\nA is an element of vector B\n\n\n\nFinally, you can also pass many other functions to filter. For example, the package stringr that is loaded as part of the tidyverse contains many different functions for working with strings (character data). For example, you you use str_detect() to only retain rows where the customer satisfaction rating includes the word “high”\n\nbudget %>%\n  filter(str_detect(satisfaction_2019, \"high\"))\n\n\n\n  \n\n\n\nNote that str_detect() is case sensitive so it would not return values of “High” or “HIGH”. You can use the function tolower() or toupper() to convert a string to lowercase or uppercase before you search for substring if you need case-insensitive matching.\n\n\n\n\n\n\nWarning\n\n\n\nfilter() is incredibly powerful and can allow you to select very specific subsets of data. But, it is also quite dangerous because when you start combining multiple criteria and operators, it’s very easy to accidentally specify something slightly different than what you intended. Always check your output. If you have a small dataset, then you can eyeball it to see if it looks right. With a larger dataset, you may wish to compute summary statistics or count the number of groups/observations in each variable to verify your filter is correct. There is no level of expertise in coding that can substitute knowing and checking your data.\n\n\n\n9.2.3 Arrange\nYou can sort your dataset using arrange(). You will find yourself needing to sort data in R much less than you do in Excel, since you don’t need to have rows next to each other in order to, for example, calculate group means. But arrange() can be useful when preparing data for display in tables. arrange() works on character data where it will sort alphabetically, as well as numeric data where the default is ascending order (smallest to largest). Reverse the order using desc().\n\n# arranging the table \n# first by product in alphabetical order\n# then by \"region\" in reverse alphabetical order\nbudget %>%\n  arrange(product, desc(region))\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to sort character data/categories in a specific order, turn the column into a factor and set the levels in the desired order.\n\nbudget %>%\n  mutate(region = factor(region, levels = c(\"North\", \"South\", \"East\", \"West\"))) %>%\n  filter(product == \"gadgets\") %>%\n  arrange(region)\n\n\n\n  \n\n\n\n\n\n\n9.2.4 Mutate\nThe function mutate() allows you to add new columns or change existing ones by overwriting them by using the syntax new_column = operation. You can add more than one column in the same mutate function by separating the columns with a comma. Once you make a new column, you can use it in further column definitions. For example, the creation of profit below uses the column expenses, which is created above it.\n\nbudget2 <- budget %>%\n  mutate(\n    sales = sales_2019 + sales_2020,\n    expenses = expenses_2019 + expenses_2020,\n    profit = sales - expenses,\n    region = paste(region, \"Office\")\n  )\n\nmutate() can also be used in conjunction with other functions and Boolean operators. For example, we can add another column to budget2 that states whether a profit was returned that year or overwrite our product variable as a factor. Just like when we used Boolean expressions with filter, it will evaluate the equation and return TRUE or FALSE depending on whether the observation meets the criteria.\n\nbudget2 <- budget2 %>%\n  mutate(profit_category = profit > 0,\n         product = as.factor(product))\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can overwrite a column by giving a new column the same name as the old column (see region or product) above. Make sure that you mean to do this and that you aren’t trying to use the old column value after you redefine it.\n\n\nYou can also use case_when() to specify what values to return, rather than defaulting to TRUE or FALSE:\n\nbudget3 <- budget2 %>%\n  mutate(profit_category = case_when(profit > 0 ~ \"PROFIT\",\n                                     profit < 0 ~ \"NO PROFIT\"))\n\nUse it to recode values:\n\n# create a column where people get a bonus if customer satisfaction was overall high or very high\n\nbonus <- budget3 %>%\n  mutate(bonus_2019 = case_when(satisfaction_2019 %in% c(\"very low\", \"low\", \"neutral\") ~ \"no bonus\",\n                                satisfaction_2019 %in% c(\"high\", \"very high\") ~ \"bonus\"))\n\nAnd combine different criteria:\n\n# new management takes over - people only get a bonus if customer satisfaction was overall high or very high AND if a profit was returned\n\nbonus2 <- budget3 %>%\n  mutate(bonus_2020 = case_when(satisfaction_2020 == \"high\" & \n                                  profit_category == \"PROFIT\" ~ \"bonus\",\n                                satisfaction_2020 == \"very high\" & \n                                  profit_category == \"PROFIT\" ~ \"bonus\",\n                                TRUE ~ \"No bonus\")) # set all other values to \"no bonus\"\n\nJust like filter(), mutate() is incredibly powerful and the scope of what you can create is far beyond what we can cover in this book.\n\n9.2.5 Summarise\nYou were introduced to the summarise() function in Section 5.3. This applies summary functions to an entire table (or groups, as you’ll see in the next section).\nLet’s say we want to determine the mean sales and expenses, plus the minimum and maximum profit, for any region, product and year. First, we need to reshape the data like we learned in Chapter 8, so that there is a column for year and one column each for sales and expenses, instead of separate columns for each year. We’ll also drop the satisfaction data as we don’t need it for this analysis.\n\nbudget4 <- budget %>%\n  select(-satisfaction_2019, -satisfaction_2020) %>%\n  pivot_longer(cols = sales_2019:expenses_2020,\n               names_to = c(\"type\", \"year\"),\n               names_sep = \"_\",\n               values_to = \"value\") %>%\n  pivot_wider(names_from = type,\n              values_from = value)\n\nhead(budget4) # check the format\n\n\n\n  \n\n\n\nNow we can create summary statistics for the table.\n\nbudget4 %>%\n  summarise(\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales)\n  )\n\n\n\n  \n\n\n\n\n9.2.6 Group By\nYou were introduced to the group_by() function in Section 5.5. For example, you can break down the summary statistics above by year and product.\n\nyear_prod <- budget4 %>%\n  group_by(year, product) %>%\n  summarise(\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales)\n  ) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nyear_prod\n\n\n\n  \n\n\n\nNote that you can use the other wrangling functions on the summary table, for example:\n\n# arrange by maximum profit\nyear_prod %>%\n  arrange(desc(max_profit))\n\n# filter out gadgets\nyear_prod %>%\n  filter(product != \"gadgets\")\n\n\n\n\n\n\n\nNote\n\n\n\nHow would you find out the maximum sales for each region?\n\n\n\nbudget3 %>%\n  group_by(region) %>%\n  summarise(max_sales = max(region)\n\nbudget3 %>%\n  group_by(sales) %>%\n  summarise(max_sales = max(sales)\n\nbudget3 %>%\n  group_by(region) %>%\n  summarise(max_sales = max(sales)\n\nbudget3 %>%\n  group_by(sales) %>%\n  summarise(max_sales = max(region)\n\n\n\n\n\nYou can also use group_by() in combination with other functions. For example, slice_max() returns the top N rows, ordered by a specific variable.\n\n# return top 3 sales\nbudget4 %>%\n  slice_max(n = 3, order_by = sales)\n\n\n\n  \n\n\n\nBut this can be combined with group_by() to return the top sales for each region:\n\n# return top sale for each region\nbudget4 %>%\n  group_by(region) %>%\n  slice_max(n = 1, order_by = sales)"
  },
  {
    "objectID": "09-wrangle.html#complications",
    "href": "09-wrangle.html#complications",
    "title": "9  Data Wrangling",
    "section": "\n9.3 Complications",
    "text": "9.3 Complications\n\n9.3.1 Rounding\nLet’s say we want to round all the values to the nearest pound. The pattern below uses the across() function to apply the round() function to the columns from mean_sales to max_profit.\n\nyear_prod %>%\n  mutate(across(.cols = mean_sales:max_profit, \n                .fns = round))\n\n\n\n  \n\n\n\nIf you compare this table to the one in Section 9.2.6, you’ll see that the 2019 gadgets mean sales rounded up from 881.5 to 882, while the mean expenses rounded down from 238.5 to 238. What’s going on!?\nThis may seem like a mistake, but R rounds .5 to the nearest even number, rather than always up, like you were probably taught in school. This prevents overestimation biases, since x.5 is exactly halfway between x and x+1, so there is no reason it should always round up.\n\nround(0.5)\nround(1.5)\n\n[1] 0\n[1] 2\n\n\nHowever, this might throw a monkey wrench into your own systems. For example, our school policy is to round up for course marks at x.5. The solution is to define your own version of round() (modified from Andrew Landgraf’s blog). Put it in a hidden code block at the top of your script, with a clear warning that this is changing the way round() normally works. You don’t need to understand how this function works, just how to use it.\nwhen you run this code, a new section will appear in the environment pane labelled “Functions”. In addition to using functions from packages, you can also make your own. It’s not something we are going to go into detail on in this course, but it’s useful to know the functionality exists.\n\n#!!!!!! redefining round so 5s round up !!!!!! \nround <- function(x, digits = 0) {\n  posneg = sign(x)\n  z = abs(x)*10^digits\n  z = z + 0.5 + sqrt(.Machine$double.eps)\n  z = trunc(z)\n  z = z/10^digits\n  z*posneg\n}\n\nNow round() should work as you’d expect.\n\nround(0.5)\nround(1.5)\n\n[1] 1\n[1] 2\n\n\nJust remove your version if you want R to go back to the original method. Remember that you have to define the new round method in any script that uses it, and run the definition code before you use it interactively. You can check your Environment pane to see whether round is listed under “Functions”.\n\n# remove new round() method\nrm(round)\n\n\n9.3.2 Missing values\nIf you have control over your data, it is always best to keep missing values as empty cells rather than denoting missingness with a word or implausible number. If you used “missing” rather than leaving the cell empty, the entire variable would be read as character data, which means you wouldn’t be able to perform mathematical operations like calculating the mean. If you use an implausible number (0 or 999 are common), then you risk these values being included in any calculations as real numbers.\nHowever, we often don’t have control over how the data come to us, so let’s run through how to fix this.\nBad missing values\nWhat if the South region hadn’t returned their expenses (entered as 0) and the North region hadn’t returned their sales data for 2020 yet, so someone entered it as “missing”? We’re going to show you two functions that you can use to recode or change values, ifelse() and case_when(). ifelse() is from Base R and can be slightly more intuitive to use initially. case_when() is from the dplyr and allows you to specify multiple criteria, although is slightly more difficult to use so it’s good to be aware of both.\nFirst, we’re going to recode the data to add in the missing values\nFor the South data, we can use ifelse() to set the value of expenses to 0 if the year is 2020 and region is “South”, otherwise use the value from the expenses column (i.e., don’t change).\n\nmissing_bad <- budget4 %>%\n  mutate(expenses = ifelse(\n    test = year == 2020 & region == \"South\", \n    yes = 0, # value if above conditions are met\n    no = expenses # value if above conditions are not met\n  ))\n\nAlternatively, we can use case_when() to convert the expenses for 2020 to 0. The last line of TRUE ~ expenses means that the default value is retrieved from the expenses column, if none of the previous criteria applied.\n\nmissing_bad <- budget4 %>%\n  mutate(expenses = case_when(\n    # set to 0 when year is 2020 and region is North\n    year == 2020 & region == \"South\" ~ 0, \n    # otherwise, set to the value in the expenses column\n    TRUE ~ expenses   \n  ))\n\n\n\nUsing case_when() for multiple criteria\n\nThe case_when() function allows allows you to set multiple criteria, although we’re only using one non-default criterion here. It can be very useful, but takes a little practice.\nThe example below creates a label for each row. Notice how the label for the first row is “x < 2”, even though this row also fits the second criterion “y < 4”. This is because case_when() applies the first match to each row, even if other criteria in the function also match that row.\n\ndata <- tibble(\n  x = 1:5,\n  y = 1:5\n)\n\ndata %>%\n  mutate(label = case_when(\n    x < 2           ~ \"x < 2\",\n    y < 4           ~ \"y < 4\",\n    x == 5 & y == 5 ~ \"both 5\",\n    TRUE            ~ \"default\"\n  ))\n\n\n\n  \n\n\n\n\nFor the North, we need to to recode these values as “missing”. Since this is character data, and sales are numeric data, the result will be coerced to a character.\n\n# set sales values to \"missing\" for North 2020 rows\nmissing_bad <- missing_bad %>%\n  mutate(sales = ifelse(year == 2020 & region == \"North\", \n                        \"missing\", \n                        sales))\n\n# check structure of data, sales now character\nstr(missing_bad)\n\ntibble [16 × 5] (S3: tbl_df/tbl/data.frame)\n $ region  : chr [1:16] \"North\" \"North\" \"North\" \"North\" ...\n $ product : chr [1:16] \"widgets\" \"widgets\" \"gadgets\" \"gadgets\" ...\n $ year    : chr [1:16] \"2019\" \"2020\" \"2019\" \"2020\" ...\n $ sales   : chr [1:16] \"2129\" \"missing\" \"723\" \"missing\" ...\n $ expenses: num [1:16] 822 -897 1037 1115 1004 ...\n\n\nIf we’re using case_when(), first we need to convert the sales column to a character, as this function is a little pickier and won’t let you combine data types, since this almost always means that you’re making a mistake.\n\n# set sales values to \"missing\" for North 2020 rows\nmissing_bad <- missing_bad %>%\n  mutate(sales = as.character(sales),\n         sales = case_when(year == 2020 & region == \"North\" ~ \"missing\", \n                           TRUE ~ sales))\n\nNow, if you try to compute the mean sales, you will get an error message and the result will be NA.\n\n# try to compute mean sales\nmissing_bad %>%\n  summarise(mean_sales = mean(sales))\n\nWarning in mean.default(sales): argument is not numeric or logical: returning NA\n\n\n\n\n  \n\n\n\nConvert missing values to NA\nTo set the missing values to NA, you could use either ifelse() or case_when(). Because the sales column was converted to character, we also need to transform this back to numeric.\n\nmissing_data <- missing_bad %>%\n  mutate(\n    # set \"0\" values to NA using ifelse\n    expenses = ifelse(expenses == 0, NA, expenses),\n    # set \"missing\" values to NA using case_when\n    sales = case_when(sales == \"missing\" ~ NA_character_,\n                      TRUE ~ sales),\n    # convert to numeric\n    sales = as.numeric(sales)\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nBecause case_when() is so picky about character types, you need to specify which type of NA you’re using (there is a specific NA for each data type). If you change NA_character_ to NA in the code above, you’ll get the following error message:\nError: Problem with `mutate()` column `sales`. ℹ `sales = case_when(sales == \"missing\" ~ NA, TRUE ~ sales)`. x must be a logical vector, not a character vector.\nTechnically, NA has a logical data type, so functions that aren’t as picky about combining data types will coerce this to the NA version for the most compatible data type.\n\n\nNow, if we try to calculate the mean sales and profits, we get missing values for any summary value that used one of the North 2020 sales values or the South 2020 expenses.\n\nmissing_data %>%\n  group_by(region) %>%\n  summarise(\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales),\n    .groups = \"drop\")\n\n\n\n  \n\n\n\nIgnore missing values\nThis is because NA basically means “I don’t know”, and the sum of 100 and “I don’t know” is “I don’t know”, not 100. However, when you’re calculating means, you often want to just ignore missing values. Set na.rm = TRUE in the summary function to remove missing values before calculating.\n\nmissing_data %>%\n  group_by(region) %>%\n  summarise(\n    mean_sales = mean(sales, na.rm = TRUE),\n    mean_expenses = mean(expenses, na.rm = TRUE),\n    min_profit = min(expenses - sales, na.rm = TRUE),\n    max_profit = max(expenses - sales, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n\n\n  \n\n\n\nCount missing values\nIf you want to find out how many missing or non-missing values there are in a column, use the is.na() function to get a logical vector of whether or not each value is missing, and use sum() to count how many values are TRUE or mean() to calculate the proportion of TRUE values.\n\nmissing_data %>%\n  group_by(year, product) %>%\n  summarise(\n    n_valid = sum(!is.na(sales)),\n    n_missing = sum(is.na(sales)),\n    prop_missing = mean(is.na(sales)),\n    .groups = \"drop\"\n  )\n\n\n\n  \n\n\n\nOmit missing values\nYou may also want to remove rows that have missing values and only work from complete datasets. drop_na() will remove any row that has a missing observation. You can use drop_na() on the entire dataset which will remove any row that has any missing value, or you can specify to only remove rows that are missing a specific value.\n\n# remove any rows with any missing values\ncomplete_data <- missing_data %>%\n  drop_na()\n\n# remove any rows that are missing a value for sales\ncomplete_sales <- missing_data %>%\n  drop_na(sales)\n\nMissing data can be quite difficult to deal with depending on how it is represented. As always, no amount of coding expertise can make up for not understanding the structure and idiosyncrasies of your data."
  },
  {
    "objectID": "09-wrangle.html#sec-together-wrangle",
    "href": "09-wrangle.html#sec-together-wrangle",
    "title": "9  Data Wrangling",
    "section": "\n9.4 Exercises",
    "text": "9.4 Exercises\nLet’s try some exercises using a dataset you already encountered in Chapter 3 so that you can see how much more you’re able to do with the data now.\n\nSave your current Markdown, close it, and open a new Rmd named “survey_data_mad_skillz”.\nIn the set-up code chunk, load the tidyverse, then load the dataset from https://psyteachr.github.io/ads-v1/data/survey_data.csv into an object named survey_data.\nUse your method of choice to review the dataset and familiarise yourself with its structure.\n\n\n\n\nSolution\n\n# from https://www.kaggle.com/kyanyoga/sample-sales-data\nlibrary(tidyverse)\nsurvey_data <- read_csv(\"https://psyteachr.github.io/ads-v1/data/survey_data.csv\")\n\nRows: 707 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): caller_id, employee_id, issue_category\ndbl  (3): wait_time, call_time, satisfaction\ndttm (1): call_start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n9.4.1 Creating new categories\nEmployees 1-5 were trained by Michael and employees 6-10 were trained by Dwight.\n\nCreate a new column named trainer that lists the trainer for each employee.\nThen, calculate the average satisfaction scores for employees trained by each trainer and visualise the satisfaction scores for each in whatever way you think best.\n\nr hide(\"Hint\") To add the trainer column you can either use case_when() and specify multiple criteria (e.g., if the employee is 1-5, Michael, if the employee is 6-10 Dwight), or you could use ifelse() and set the test to Michael’s employees and return Dwight for all others. r unhide()\n\n\n\nSolution\n\n# case_when() method\nsurvey_data <- survey_data %>%\n  mutate(trainer = case_when(employee_id %in% c(\"E01\", \"E02\", \"E03\", \"E04\", \"E05\") ~ \"Michael\",\n                             employee_id %in% c(\"E06\", \"E07\", \"E08\", \"E09\", \"E10\") ~ \"Dwight\"))\n\n# ifelse() method\n\nsurvey_data <- survey_data %>%\n  mutate(trainer = ifelse(test = employee_id %in% c(\"E01\", \"E02\", \"E03\", \"E04\", \"E05\"),\n                           yes = \"Michael\",\n                           no = \"Dwight\"))\n\n# mean satisfaction scores\nsurvey_data %>%\n  group_by(trainer) %>%\n  summarise(mean_satisfaction = mean(satisfaction))\n\n# possible visualisation \n\nggplot(survey_data, aes(x = satisfaction, fill = trainer)) +\n  geom_histogram(binwidth = 1, show.legend = FALSE, colour = \"black\") +\n  facet_wrap(~trainer) +\n  labs(title = \"Satisfaction scores by employee trainer\")\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n9.4.2 Filter by calculated score\nFirst, calculate the average wait time and store this in an object named mean_wait. This should be a single value rather than a table.\n\n\nHint\n\nThere are multiple ways to achieve this. You could create the table and then pull out the single value, or just calculate the single value.\n\n\n\n\nSolution\n\n# method 1 - tidyverse\nmean_wait <- survey_data %>%\n  summarise(mean_wait = mean(wait_time)) %>%\n  pull(mean_wait)\n\n# method 2 - base R\nmean_wait <- mean(survey_data$wait_time)\n\n\nNow create a dataset named long_wait that just contains data from customers who waited more than the average wait time.\n\n\n\nSolution\n\nlong_wait <- survey_data %>%\n  filter(wait_time > mean_wait)\n\n\nCreate a visualisation that shows how many customers waited more than the average wait time for each employee.\n\n\n\nSolution\n\nlong_wait %>%\n  ggplot(aes(x = employee_id)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n9.4.3 Multiple critera\nNow, add a column to survey_data named follow_up that flags whether a customer should be followed up with a courtesy phone call. Your company is short-staffed so only customers that meet all three of the following criteria should be followed-up:\n\nTheir wait time should be above the average for all calls\n\nTheir call time should be above the average for their category\n\nTheir satisfaction should be less than three 3.\n\nThis is quite complicated and there are multiple ways to achieve the desired outcome. Some approaches may need other functions that were covered in previous chapters and you may need to create intermediate objects.\nCall the final object follow_data and keep only the customer ID, employee ID, trainer, and follow up columns.\n\n\n\nSolution\n\n# this is one possible solution, there are many other valid approaches \n\n# calculate mean wait time across all calls\nmean_wait <- mean(survey_data$wait_time)\n\n# calculate mean call time for each category\nfollow_data <- survey_data %>%\n  group_by(issue_category) %>%\n  summarise(mean_call = mean(call_time)) %>%\n#then join it to the survey data  \n  left_join(survey_data, by = \"issue_category\") %>%\n# then add on the column\n  mutate(follow_up = case_when(wait_time > mean_wait & \n                               call_time > mean_call & \n                               satisfaction < 3 ~ \"yes\",\n                               TRUE ~ \"no\")) %>%\n  select(caller_id, employee_id, trainer, follow_up)\n\n\nFor all of the above, write code that stores the answer as a single value, so that you could easily use it in inline coding.\nHow many customers need to be followed up:\n\nIn total? \n\nFrom calls by employee 06? \n\nFrom calls by employees trained by Michael \n\nFrom calls by employees trained by Dwight \n\n\n\n\nHint\n\n`group_by %>% count() %>% filter() %>% pull()\n\nWhich employee needs to make the largest number of follow-up courtesy calls? \n\n\nHint\n\nAs above but add in an ungroup() and slice_max() along the way.\n\n\n\n\nSolution\n\n# in total\nfollow_data %>%\n  group_by(follow_up) %>%\n  count()%>%\n  filter(follow_up == \"yes\") %>%\n  pull(n)\n\n# by employee 6\nfollow_data %>%\n  group_by(follow_up, employee_id) %>%\n  count() %>%\n  filter(employee_id == \"E06\",\n         follow_up == \"yes\") %>%\n  pull(n)\n\n# by michael\nfollow_data %>%\n  group_by(follow_up, trainer) %>%\n  count() %>%\n  filter(trainer == \"Michael\",\n         follow_up == \"yes\") %>%\n  pull(n)\n\n# by dwight\nfollow_data %>%\n  group_by(follow_up, trainer) %>%\n  count() %>%\n  filter(trainer == \"Dwight\",\n         follow_up == \"yes\") %>%\n  pull(n)\n\n# most follow-ups needed\nfollow_data %>%\n  group_by(follow_up, employee_id) %>%\n  count() %>%\n  ungroup() %>%\n  filter(follow_up == \"yes\") %>%\n  slice_max(n = 1, order_by = n) %>%\n  pull(employee_id)\n\n[1] 120\n[1] 16\n[1] 65\n[1] 55\n[1] \"E02\"\n\n\n\n\n9.4.4 Original insight\nIn preparation for the final summative assessment, explore the data to provide one original insight of your own.\n\n9.4.5 Report\nCompile all the above into a visually appealing reproducible report that could be used to target employees and trainers for extra training (or depending on what mood you’re in, to fire them). Use inline coding to report any numbers in the text. Once you’re finished, post your Rmd and knitted html document on teams so that other learners can see your approach."
  },
  {
    "objectID": "09-wrangle.html#sec-glossary-wrangle",
    "href": "09-wrangle.html#sec-glossary-wrangle",
    "title": "9  Data Wrangling",
    "section": "\n9.5 Glossary",
    "text": "9.5 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nboolean expression\n\n\nAn expression that evaluates to TRUE or FALSE.\n\n\n\n\ncoercion\n\n\nChanging the data type of values in a vector to a single compatible type.\n\n\n\n\ndata type\n\n\nThe kind of data represented by an object.\n\n\n\n\ndata wrangling\n\n\nThe process of preparing data for visualisation and statistical analysis.\n\n\n\n\nfactor\n\n\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\n\n\nlogical\n\n\nA data type representing TRUE or FALSE values.\n\n\n\n\nmatch operator\n\n\nA binary operator (%in%) that returns a logical vector indicating if there is a match or not for its left operand.\n\n\n\n\noperator\n\n\nA symbol that performs some mathematical or comparative process.\n\n\n\n\nstring\n\n\nA piece of text inside of quotes."
  },
  {
    "objectID": "09-wrangle.html#sec-resources-wrangle",
    "href": "09-wrangle.html#sec-resources-wrangle",
    "title": "9  Data Wrangling",
    "section": "\n9.6 Further resources",
    "text": "9.6 Further resources\n\nData transformation cheat sheet\n\nChapter 5: Data Transformation in R for Data Science\n\n\nChapter 19: Functions in R for Data Science\n\nIntroduction to stringr"
  },
  {
    "objectID": "10-custom.html#sec-ilo-custom",
    "href": "10-custom.html#sec-ilo-custom",
    "title": "10  Customising Visualisations & Reports",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nThe final chapter of ADS moves beyond the core skills needed to complete the course. Our aim in this chapter is to give you a sense of what is now available to you with your new-found R skills. There’s a lot in this chapter, as we wanted to present an overview of the possibilities, although there are no exercises at the end. If you’re enrolled on ADS, you may wish to prioritise the summative assessment this week and come back to this chapter at a later date.\n\nCreate and customise advanced types of plots\nStructure data in report, presentation, and dashboard formats\nInclude linked figures, tables, and references"
  },
  {
    "objectID": "10-custom.html#sec-walkthrough-custom",
    "href": "10-custom.html#sec-walkthrough-custom",
    "title": "10  Customising Visualisations & Reports",
    "section": "Walkthrough video",
    "text": "Walkthrough video\nThere is a walkthrough video of this chapter available via Echo360 (coming soon). Please note that there may have been minor edits to the book since the video was recorded. Where there are differences, the book should always take precedence."
  },
  {
    "objectID": "10-custom.html#sec-custom-viz",
    "href": "10-custom.html#sec-custom-viz",
    "title": "10  Customising Visualisations & Reports",
    "section": "\n10.1 Visualisation",
    "text": "10.1 Visualisation\n\n10.1.1 Set-up\nFirst, create a new project for the work we’ll do in this chapter named 10-advanced. Second, open and save a new R Markdown document named visualisations.Rmd, delete the welcome text and load the required packages for this section. You will probably need to install several new packages.\n\n\n\nChapter packages\n\nlibrary(tidyverse)   # data wrangling functions\nlibrary(ggthemes)    # for themes\nlibrary(patchwork)   # for combining plots\nlibrary(plotly)      # for interactive plots\n# devtools::install_github(\"hrbrmstr/waffle\")\nlibrary(waffle)      # for waffle plots\nlibrary(ggbump)      # for bump plots\nlibrary(treemap)     # for treemap plots\nlibrary(ggwordcloud) # for word clouds\nlibrary(tidytext)    # for manipulating text for word clouds\nlibrary(sf)          # for mapping geoms\nlibrary(rnaturalearth) # for map data\nlibrary(rnaturalearthdata) # extra mapping data\nlibrary(gganimate)   # for animated plots\n\ntheme_set(theme_light())\n\n\nYou’ll need to make a folder called “data” and download data files into it: survey_data.csv and scottish_population.csv.\nDownload the ggplot2 cheat sheet.\n\n10.1.2 Defaults\nThe code below creates two familiar plots from Chapter 3), using the default (light) theme and palettes. First, load the data and set issue_category to a factor so you can control the order of the categories.\n\n# update column specification\nct <- cols(issue_category = col_factor(levels = c(\"tech\", \"returns\", \"sales\", \"other\")))\n\n# load data\nsurvey_data <- read_csv(file = \"data/survey_data.csv\",\n                        col_types = ct)\n\nNext, create a bar plot of number of calls by issue category.\n\n# create bar plot\nbar <- ggplot(data = survey_data, \n              mapping = aes(x = issue_category,\n                            fill = issue_category)) +\n  geom_bar(show.legend = FALSE) +\n  labs(x = \"Issue Category\", \n       y = \"Count\",\n       title = \"Calls by Issue Category\")\n\nAnd create a scatterplot of wait time by call time, distinguished by issue category.\n\n#create scatterplot\npoint <- ggplot(data = survey_data, \n                mapping = aes(x = wait_time, \n                              y = call_time,\n                              color = issue_category)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = lm, formula = y~x) +\n  labs(x = \"Wait Time\",\n       y = \"Call Time\",\n       color = \"Issue Category\",\n       title = \"Wait Time by Call Time\")\n\nFinally, combine the two plots using the + from patchwork to see the default styles for these plots.\n\nbar + point\n\n\n\nDefault plot styles.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry changing the theme using built-in themes or customising the colours or linetypes with scale_* functions. See Appendix I for details.\n\n\n\n10.1.3 Annotations\nIt’s often useful to add annotations to a plot, for example, to highlight an important part of the plot or add labels. The annotate() function creates a specific geom at x- and y-coordinates you specify.\nText annotations\nAdd a text annotation by setting the geom argument to “text” or “label” and adding a label. Labels have padding and a background, while text is just text.\n\nBackslash-n \\n in the label text controls where the line breaks are. Try removing or changing the position of these to see what happens.\n\nx and y control the coordinates of the label. You will likely have to play around with these values to get them right.\nThe argument hjust is the horizontal justification of text, and vjust is the vertical justification. The default values are 0.5, where the text is centred on the x and y coordinates. 0 will justify to the left and bottom, while 1 justifies to the right and top.\nYou can change the angle of text, but not labels.\n\n\nbar +\n  # add left-justified text to the second bar\n  annotate(geom = \"text\",\n           label = \"Our goal is to\\nreduce this\\ncategory\",\n           x = 1.65, y = 150,\n           hjust = 0, vjust = 1, \n           color = \"white\", fontface = \"bold\",\n           angle = 45) +\n  # add a centred label to the third bar\n  annotate(geom = \"label\",\n           label = \"Our goal is\\nto increase this\\ncategory\",\n           x = 3, y = 75,\n           hjust = 0.5, vjust = 1, \n           color = \" darkturquoise\", fontface = \"bold\")\n\n\n\nAn example of annotation text and label.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee if you can work out how to make the figure below, starting with the following:\n\ntibble(x = c(0, 0, 1, 1),\n       y = c(0, 1, 0, 1)) %>%\n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.25, size = 4, color = \"red\")\n\nHint: you will need to add 1 label annotation and 8 separate text annotations.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\ntibble(x = c(0, 0, 1, 1),\n       y = c(0, 1, 0, 1)) %>%\n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.25, size = 4, color = \"red\") +\n  annotate(\"label\", label = \"In the\\nmiddle\",\n           x = 0.5, y = 0.5,\n           fill = \"dodgerblue\", color = \"white\",\n           label.padding = unit(1, \"lines\"),\n           label.r = unit(1.5, \"lines\")) +\n  annotate(\"text\", label = \"Bottom\\nLeft\",\n           x = 0, y = 0, hjust = 0, vjust = 0) +\n  annotate(\"text\", label = \"Top\\nLeft\", \n           x = 0, y = 1, hjust = 0, vjust = 1) +\n  annotate(\"text\", label = \"Bottom\\nRight\",\n           x = 1, y = 0, hjust = 1, vjust = 0) +\n  annotate(\"text\", label = \"Top\\nRight\",\n           x = 1, y = 1, hjust = 1, vjust = 1) +\n  annotate(\"text\", label = \"45 degrees\",\n           x = 0, y = 0.5, hjust = 0, angle = 45) +\n  annotate(\"text\", label = \"90 degrees\",\n           x = 0.25, y = 0.5, angle = 90) +\n  annotate(\"text\", label = \"270 degrees\",\n           x = 0.75, y = 0.5, angle = 270)+\n  annotate(\"text\", label = \"-45 degrees\",\n           x = 1, y = 0.5, hjust = 1, angle = -45)\n\n\n\n\nOther annotations\nYou can add other geoms to highlight parts of a plot. The example below adds a rectangle around a group of points, a text label, a straight arrow from the label to the rectangle, and a curved arrow from the label to an individual point.\n\npoint +\n  # add a rectangle surrounding long call times\n  annotate(geom = \"rect\",\n           xmin = 100, xmax = 275,\n           ymin = 140, ymax = 180,\n           fill = \"transparent\", color = \"red\") +\n  # add a text label\n  annotate(\"text\",\n           x = 260, y = 120,\n           label = \"outliers\") +\n  # add an line with an arrow from the text to the box\n  annotate(geom = \"segment\", \n           x = 240, y = 120, \n           xend = 200, yend = 135,\n           arrow = arrow(length = unit(0.5, \"lines\"))) +\n  # add a curved line with an arrow \n  # from the text to a wait time outlier\n  annotate(geom = \"curve\", \n          x = 280, y = 120, \n          xend = 320, yend = 45,\n          curvature = -0.5,\n          arrow = arrow(length = unit(0.5, \"lines\")))\n\n\n\nExample of annotatins with the rect, text, segment, and curve geoms.\n\n\n\n\nSee the ggforce”, “https://ggforce.data-imaginist.com/”)` package for more sophisticated options, such as highlighting a group of points with an ellipse.\n\n10.1.4 Other Plots\nInteractive Plots\nThe plotly package can be used to make interactive graphs. Assign your ggplot to a variable and then use the function ggplotly() on the plot object. Note that interactive plots only work in HTML files, not PDFs or Word files.\n\nggplotly(point)\n\n\nInteractive graph using plotly\n\n\n\n\n\n\n\n\nNote\n\n\n\nHover over the data points above and click on the legend items.\n\n\nWaffle Plots\nIn Chapter 3, we mentioned that pie charts are such a poor way to visualise proportions that we refused to even show you how to make one. Waffle plots are a delicious alternative.\n\n\n\n\n\n\nWarning\n\n\n\nUse waffle by hrbrmstr on GitHub using the install_github() function below, rather than the one on CRAN you get from using install.packages().\n\ndevtools::install_github(\"hrbrmstr/waffle\")\n\n\n\nBy default, geom_waffle() represents each observation with a tile and splits these across 10 rows. You can play about with the n_rows argument to determine what works best for your data.\n\nsurvey_data %>% \n  count(issue_category) %>%\n  ggplot(aes(fill = issue_category, values = n)) +\n  geom_waffle(\n    n_rows = 23, # try setting this to 10 (the default)\n    size = 0.33, # line size\n    make_proportional = FALSE, # use raw values\n    colour = \"white\", # line colour\n    flip = FALSE, # bottom-top or left-right\n    radius = grid::unit(0.1, \"npc\") # set to 0.5 for circles\n  ) +\n  theme_enhance_waffle() + # gets rid of axes\n  scale_fill_colorblind(name = \"Issue Category\")\n\n\n\nWaffle plot.\n\n\n\n\nThe waffle plot can also be used to display the counts as proportions To achieve these, set n_rows = 10 and make_proportional = TRUE. Now, rather than each tile representing one observation, each tile represents 1% of the data.\n\nsurvey_data %>% \n  count(issue_category) %>%\n  ggplot(aes(fill = issue_category, values = n)) +\n  geom_waffle(\n    n_rows = 10, \n    size = 0.33, \n    make_proportional = TRUE, # compute proportions\n    colour = \"white\", \n    flip = FALSE, \n    radius = grid::unit(0.1, \"npc\") \n  ) +\n  theme_enhance_waffle() + \n  scale_fill_colorblind(name = \"Issue Category\")\n\n\n\nProportional waffle plot.\n\n\n\n\nTreemap\nTreemap plots are another way to visualise proportions. Like the waffle plots, you need to count the data by category first. You can use any brewer palette for the fill.\n\nsurvey_data %>% \n  count(issue_category) %>%\n  treemap(\n    index = \"issue_category\", # column with number of rectangles\n    vSize = \"n\", # column with size of rectangle\n    title = \"\",\n    palette = \"BuPu\",\n    inflate.labels = TRUE # expand labels to size of rectangle\n  )\n\n\n\nTreemap plot.\n\n\n\n\nYou can also represent multiple categories with treemaps\n\nsurvey_data %>% \n  count(issue_category, employee_id) %>%\n  arrange(employee_id) %>%\n  treemap(\n    # use c() to specify two variables\n    index = c(\"employee_id\", \"issue_category\"), \n    vSize = \"n\", \n    title = \"\",\n    palette = \"Dark2\",\n    # set different label sizes for each type of label\n    fontsize.labels = c(30, 10), \n    # set different alignments for two label types\n    align.labels = list(c(\"left\", \"top\"), c(\"center\", \"center\")) \n  )\n\n\n\nTreemap with two variables\n\n\n\n\nBump Plots\nBump plots are very useful for visualising how rankings change over time. So first, we need to get some ranking data. Let’s start with a more typical raw data table, containing an identifying column of person and three columns for their scores each week\n\n# make a small dataset of scores for 3 people over 3 weeks\nscore_data <- tribble(\n  ~person, ~week_1, ~week_2, ~week_3,\n  \"Abeni\",      80,     75,       90,\n  \"Beth\",       75,     85,       75,\n  \"Carmen\",     60,     70,       80\n)\n\nNow we make the table long, group by week, and use the rank() function to find the rank of each person’s score each week. Use n() - rank(score) + 1 to reverse the ranks so that the highest score gets rank 1. We also need to make the week variable a number.\n\n# calculate ranks\nrank_data <- score_data %>%\n  pivot_longer(cols = -person,\n               names_to = \"week\",\n               values_to = \"score\") %>%\n  group_by(week) %>%\n  mutate(rank = n() - rank(score) + 1) %>%\n  ungroup() %>%\n  arrange(week, rank) %>%\n  mutate(week = str_replace(week, \"week_\", \"\") %>% as.integer())\n\nrank_data\n\n\n\n  \n\n\n\nA typical mapping for a bump plot puts the time variable in the x-axis, the rank variable on the y-axis, and sets colour to the identifying variable.\n\nggplot(data = rank_data, \n       mapping = aes(x = week, \n                     y = rank, \n                     colour = person)) +\n  ggbump::geom_bump()\n\n\n\nBasic bump plot\n\n\n\n\nWe can make this more attractive by customising the axes and adding text labels. Try running each line of this code to see how it builds up.\n\nAdd label = person to the mapping so we can add in text labels.\nIncrease the size of the lines with the size argument to geom_bump()\n\nWe don’t need labels for weeks 1.5 and 2.5, so change the x-axis breaks\n\nThe expand argument for the two scale_ functions expands the plot area so we can fit text labels to the right.\nIt makes more sense to have first place at the top, so reverse the order of the y-axis with scale_y_reverse() and fix the breaks and expansion.\nAdd text labels with geom_text(), but just for week 3, so set data =  filter(rank_data, week == 3) for this geom.\nSet x = 3.05 to move the text labels just to the right of week 3, and set hjust = 0 to right-justify the text labels (the default is hjust = 0.5, which would center them on 3.05).\nRemove the legend and grid lines. Increase the x-axis text size.\n\n\nggplot(data = rank_data, \n       mapping = aes(x = week, \n                     y = rank, \n                     colour = person,\n                     label = person)) +\n  ggbump::geom_bump(size = 10) +\n  scale_x_continuous(name = \"\",\n                     breaks = 1:3, \n                     labels = c(\"Week 1\", \"Week 2\", \"Week 3\"),\n                     expand = expansion(c(.05, .2))) +\n  scale_y_reverse(name = \"Ranking\",\n                  breaks = 1:3, \n                  expand = expansion(.2)) +\n  geom_text(data = filter(rank_data, week == 3),\n            color = \"black\", x = 3.05, hjust = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_text(size = 12))\n\n\n\nBump plot with added features.\n\n\n\n\nWord Clouds\nWord clouds are a common way to summarise text data. First, download amazon_alexa.csv into your data folder and then load it into an object. This dataset contains text reviews as well as the 1-5 rating from customers who bought an Alexa device on Amazon.\n\n# https://www.kaggle.com/sid321axn/amazon-alexa-reviews\n# extracted from Amazon by Manu Siddhartha & Anurag Bhatt\nalexa <- rio::import(\"data/amazon_alexa.csv\")\n\nWe can use this data to look at how the words used differ depending on the rating given. To make the text data easy to work with, the function tidytext::unnest_tokens() splits the words in the input column into individual words in a new output column. unnnest_tokens() is particularly helpful because it also does things like removes punctuation and transforms all words to lower case to make it easier to work with. Compare words and alexa to see how they map on to each other.\n\nwords <- alexa %>%\n  unnest_tokens(output = \"word\", input = \"verified_reviews\")\n\nWe can then add another line of code using a pipe that counts how many instances of each word there is by rating to give us the most popular words.\n\nwords <- alexa %>%\n  unnest_tokens(output = \"word\", input = \"verified_reviews\") %>%\n  count(word, rating, sort = TRUE) \n\n\n\n\n\n  \n\n\n\nThe problem is that the most common words are all function words rather than content words, which makes sense because these words have the highest word frequency in natural language.\nHelpfully, tidytext contains a list of common “stop words”, i.e., words that you want to ignore, that are stored in an object named stop_words. It is also very useful to define a list of custom stop words based upon the unique properties of your data (it can sometimes take a few attempts to identify what’s appropriate for your dataset). This dataset contains a lot of numbers that aren’t informative, and it also contains “https” from website links, so we’ll get rid of both with a custom stop list.\nOnce you have defined your stop words, you can then use anti_join() to remove any word that is present in the stop word list.\nTo get the top 25 words, we then group by rating and use dplyr::slice_max(), ordered by the column n.\n\ncustom_stop <- tibble(word = c(0:9, \"https\", 34))\n\nwords <- alexa %>%\n  unnest_tokens(output = \"word\", input = \"verified_reviews\") %>%\n  count(word, rating) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  anti_join(custom_stop, by = \"word\") %>%\n  group_by(rating) %>%\n  slice_max(order_by = n, n = 25, with_ties = FALSE) %>%\n  ungroup()\n\nFirst, let’s make a word cloud for customers who gave a 1-star rating:\n\nFilter retains only the data for 1-star ratings.\n\nlabel comes from the word column and is the data to plot (i.e., the words).\n\ncolour makes the words red (you could also set this to word to give each word a different colour or n to vary colour continuously by frequency).\n\nsize makes the size of the word proportional to n, the number of times the word appeared.\n\nggwordcloud::geom_text_wordcloud_area() is the word cloud geom.\n\nggwordcloud::scale_size_area() controls how big the word cloud is (this usually takes some trial-and-error).\n\n\nrating1 <- filter(words, rating == 1) %>%\n  ggplot(aes(label = word, colour = \"red\", size = n)) +\n  geom_text_wordcloud_area() +\n  scale_size_area(max_size = 10) +\n  ggtitle(\"Rating = 1\") +\n  theme_minimal(base_size = 14)\n\nrating1\n\n\n\n\n\n\n\nWe can now do the same but for 5-star ratings and paste the plots together with patchwork (word clouds don’t play well with facets).\n\nrating5 <- filter(words, rating == 5) %>%\n  ggplot(aes(label = word, size = n)) +\n  geom_text_wordcloud_area(colour = \"darkolivegreen3\") +\n  scale_size_area(max_size = 12) +\n  ggtitle(\"Rating = 5\") +\n  theme_minimal(base_size = 14)\n\nrating1 + rating5\n\n\n\nWord cloud.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt’s worth highlighting that whilst word clouds are very common, they’re really the equivalent of pie charts for text data because we’re not very good at making accurate comparisons based on size. You might be able to see what’s the most popular word, but can you accurately determine the 2nd, 3rd, 4th or 5th most popular word based on the clouds alone? There’s also the issue that just because it’s text data doesn’t make it a qualitative analysis and just because something is said a lot doesn’t mean it’s useful or important. But, this argument is outwith the scope of this book, even if it is a recurring part of Emily’s life thanks to her qualitative wife.\n\n\nMaps\nWorking with maps can be tricky. The sf package provides functions that work with ggplot2, such as geom_sf(). The rnaturalearth package (and associated data packages that you may be prompted to download) provide high-quality mapping coordinates.\n\n\nne_countries() returns world country polygons (i.e., a world map). We specify the object should be returned as a “simple feature” class sf so that it will work with geom_sf(). If you would like a deep dive on simple feature objects, check out a vignette from the sf package.\nIt’s worth checking out what the object ne_countries() returns to see just how much information is available.\nTry changing the values and colours below to get a sense of how the code works.\n\n\n# get the world map coordinates\nworld_sf <- ne_countries(returnclass = \"sf\", scale = \"medium\")\n\n# plot them on a light blue background\nggplot() + \n  geom_sf(data = world_sf, size = 0.3) +\n  theme(panel.background = element_rect(fill = \"lightskyblue2\"))\n\n\n\n\n\n\n\nYou can combine multiple countries using bind_rows() and visualise them with different colours for each country.\n\n# get and bind country data\nuk_sf <- ne_states(country = \"united kingdom\", returnclass = \"sf\")\nireland_sf <- ne_states(country = \"ireland\", returnclass = \"sf\")\nislands <- bind_rows(uk_sf, ireland_sf) %>%\n  filter(!is.na(geonunit))\n\n# set colours\ncountry_colours <- c(\"Scotland\" = \"#0962BA\",\n                     \"Wales\" = \"#00AC48\",\n                     \"England\" = \"#FF0000\",\n                     \"Northern Ireland\" = \"#FFCD2C\",\n                     \"Ireland\" = \"#F77613\")\n\nggplot() + \n  geom_sf(data = islands,\n          mapping = aes(fill = geonunit),\n          colour = NA,\n          alpha = 0.75) +\n  coord_sf(crs = sf::st_crs(4326),\n           xlim = c(-10.7, 2.1), \n           ylim = c(49.7, 61)) +\n  scale_fill_manual(name = \"Country\", \n                    values = country_colours)\n\n\n\nMap coloured by country.\n\n\n\n\nYou can join Scottish population data to the map table to visualise data on the map using colours or labels.\n\n# load map data\nscotland_sf <- ne_states(geounit = \"Scotland\", \n                         returnclass = \"sf\")\n\n# load population data from\n# https://www.indexmundi.com/facts/united-kingdom/quick-facts/scotland/population\nscotpop <- read_csv(\"data/scottish_population.csv\", \n                    show_col_types = FALSE)\n\n# join data and fix typo in the map\nscotmap_pop <- scotland_sf %>%\n  mutate(name = ifelse(name == \"North Ayshire\", \n                       yes = \"North Ayrshire\", \n                       no = name)) %>%\n  left_join(scotpop, by = \"name\") %>%\n  select(name, population, geometry)\n\n\n\n\n\n\n\nWarning\n\n\n\nThere is a typo in the data from rnaturalearth, so you need to change “North Ayshire” to “North Ayrshire” before you join the population data.\n\n\n\nSetting the fill to population in geom_sf() gives each region a colour based on its population.\nThe colours are customised with scale_fill_viridis_c(). The breaks of the fill scale are set to increments of 100K (1e5 in scientific notation) and the scale is set to span 0 to 600K.\n\npaste0() creates the labels by taking the numbers 0 through 6 and adding “00 k” to them.\nFinally, the position of the legend is moved into the sea using legend.position().\n\n\n# plot\nggplot() + \n  geom_sf(data = scotmap_pop,\n          mapping = aes(fill = population),\n          color = \"white\", \n          size = .1) +\n  coord_sf(xlim = c(-8, 0), ylim = c(54, 61)) +\n  scale_fill_viridis_c(name = \"Population\",\n                       breaks = seq(from = 0, to = 6e5, by = 1e5), \n                       limits = c(0, 6e5),\n                       labels = paste0(0:6, \"00 K\")) +\n  theme(legend.position = c(0.16, 0.84))\n\n\n\nMap coloured by population.\n\n\n\n\nAnimated Plots\nAnimated plots are a great way to add a wow factor to your reports, but they can be complex to make, distracting, and not very accessible, so use them sparingly and only for data visualisation where the animation really adds something. The package gganimate”, “https://gganimate.com/”)` has many functions for animating ggplots.\nHere, we’ll load some population data from the United Nations. Download the file into your data folder and open it in Excel first to see what it looks like. The code below gets the data from the first tab, filters it to just the 6 world regions, makes the data long, and makes sure the year column is numeric and the pop column shows population in whole numbers (the original data is in 1000s).\n\n# load and process data\nworldpop <- readxl::read_excel(\"data/WPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx\", skip = 16) %>%\n  filter(Type == \"Region\") %>%\n  select(region = 3, `1950`:`1992`) %>%\n  pivot_longer(cols = -region, \n               names_to = \"year\",\n               values_to = \"pop\") %>%\n  mutate(year = as.integer(year),\n         pop = round(1000 * as.numeric(pop)))\n\nLet’s make an animated plot showing how the population in each region changes with year. First, make a static plot. Filter the data to the most recent year so you can see what a single frame of the animation will look like.\n\nworldpop %>%\n  filter(year == 1992) %>%\n  ggplot(aes(x = region, y = pop, fill = region)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d() +\n  scale_x_discrete(name = \"\", \n                   guide = guide_axis(n.dodge=2))+\n  scale_y_continuous(name = \"Population\",\n                     breaks = seq(0, 3e9, 1e9),\n                     labels = paste0(0:3, \"B\")) +\n  ggtitle('Year: 1992')\n\n\n\n\n\n\n\nTo convert this to an animated plot that shows the data from multiple years:\n\nRemove the filter and add transition_time(year).\nUse the {} syntax to include the frame_time in the title.\nUse anim_save() to save the animation to a GIF file and set this code chunk to eval = FALSE because creating an animation takes a long time and you don’t want to have to run it every time you knit your report.\n\n\nanim <- worldpop %>%\n  ggplot(aes(x = region, y = pop, fill = region)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d() +\n  scale_x_discrete(name = \"\",\n                   guide = guide_axis(n.dodge=2))+\n  scale_y_continuous(name = \"Population\",\n                     breaks = seq(0, 3e9, 1e9),\n                     labels = paste0(0:3, \"B\")) +\n  ggtitle('Year: {frame_time}') +\n  transition_time(year)\n  \ndir.create(\"images\", FALSE) # creates an images directory if needed\n\nanim_save(filename = \"images/gganim-demo.gif\",\n          animation = anim,\n          width = 8, height = 5, units = \"in\", res = 150)\n\nYou can show your animated gif in an html report (animations don’t work in Word or a PDF) using include_graphics(), or include the GIF in a dynamic document like PowerPoint.\n\nknitr::include_graphics(\"images/gganim-demo.gif\")\n\n\n\nAnimated gif.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThere are actually not many plots that are really improved by animating them. The plot below gives the same information at a single glance.\n\n\n\n\n\n\n\n\n\n\n\n10.1.5 Resources\nThere are so many more options for data visualisation in R than we have time to cover here. The following resources will get you started on your journey to informative, intuitive visualisations.\n\n\nThe R Graph Gallery (this is really useful)\n\nLook at Data from Data Vizualization for Social Science\n\n\nGraphs in Cookbook for R\n\nTop 50 ggplot2 Visualizations\n\nR Graphics Cookbook by Winston Chang\nggplot extensions\n\nplotly for creating interactive graphs\nDrawing Beautiful Maps Programmatically\ngganimate"
  },
  {
    "objectID": "10-custom.html#sec-custom-reports",
    "href": "10-custom.html#sec-custom-reports",
    "title": "10  Customising Visualisations & Reports",
    "section": "\n10.2 Reports",
    "text": "10.2 Reports\n\n10.2.1 Set-up\nClose your visualisation Markdown and open and save an new R Markdown document named reports.Rmd, delete the welcome text and load the required packages for this section.\n\nlibrary(tidyverse)     # data wrangling functions\nlibrary(bookdown)      # for chaptered reports\nlibrary(flexdashboard) # for dashboards\nlibrary(DT)            # for interactive tables\n\nYou’ll need to make a folder called “data” and download data files into it: amazon_alexa.csv and scottish_population.csv.\n\n10.2.2 Linked documents\nIf you need to create longer reports with links between sections, you can edit the YAML to use a bookdown format. bookdown::html_document2 is a useful one that adds figure and table numbers automatically to any figures or tables with a caption and allows you to link to these by reference.\nTo create links to tables and figures, you need to name the code chunk that created your figures or tables, and then call those names in your inline coding:\n\n\n\n```{r tab-my-table}\n# table code here\n```\n\n\n\n\n```{r fig-my-figure}\n# figure code here\n```\n\n\nSee @tab-my-table or @fig-my-figure.\n\n\n\n\n\n\nWarning\n\n\n\nThe code chunk names can only contain letters, numbers and dashes. If they contain other characters like spaces or underscores, the referencing will not work.\n\n\nYou can also link to different sections of your report by naming your headings with {#sec-}:\n# My first heading {#sec-heading-1}\n\n## My second heading {#sec-heading-2}\n\nSee @sec-heading-1 and @sec-heading-2.\nThe code below shows how to link text to figures or tables in a full report using the built-in diamonds dataset - use your reports.Rmd to create this document now. You can see the HTML output here.\n\n\nLinked document code\n\n\n---\ntitle: \"Linked Document Demo\"\noutput: \n  bookdown::html_document2:\n    number_sections: true\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE,\n                      message = FALSE,\n                      warning = FALSE)\nlibrary(tidyverse)\nlibrary(kableExtra)\ntheme_set(theme_minimal())\n```\n\nDiamond price depends on many features, such as:\n\n- cut (See @tab-tab:by-cut))\n- colour (See @tab-tab:by-colour))\n- clarity (See @fig-by-clarity))\n- carats (See @fig-by-carat))\n- See @sec-conclusion) for concluding remarks\n\n## Tables\n\n### Cut\n\n```{r by-cut}\ndiamonds %>%\n  group_by(cut) %>%\n  summarise(avg = mean(price),\n            .groups = \"drop\") %>%\n  kable(digits = 0, \n        col.names = c(\"Cut\", \"Average Price\"),\n        caption = \"Mean diamond price by cut.\") %>%\n  kable_material()\n```\n\n### Colour\n\n```{r by-colour}\ndiamonds %>%\n  group_by(color) %>%\n  summarise(avg = mean(price),\n            .groups = \"drop\") %>%\n  kable(digits = 0, \n        col.names = c(\"Cut\", \"Average Price\"),\n        caption = \"Mean diamond price by colour.\") %>%\n  kable_material()\n```\n\n## Plots\n\n### Clarity\n\n```{r by-clarity, fig.cap = \"Diamond price by clarity\"}\nggplot(diamonds, aes(x = clarity, y = price)) +\n  geom_boxplot() \n```\n\n### Carats\n\n```{r by-carat, fig.cap = \"Diamond price by carat\"}\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_smooth()\n```\n\n### Concluding remarks {#sec-conclusion}\n\nI am not rich enough to worry about any of this.\n\n\nThis format defaults to numbered sections, so set number_sections: false in the YAML header if you don’t want this. If you remove numbered sections, links like @sec-conclusion) will show “??”, so you need to use URL link syntax instead, like this:\nSee the [last section](#conclusion) for concluding remarks.\n\n10.2.3 References\nThere are several ways to do in-text references and automatically generate a bibliography in R Markdown. Markdown files need to link to a BibTex file (a plain text file with references in a specific format) that contains the references you need to cite. You specify the name of this file in the YAML header, like bibliography: filename.bib and cite references in text using an at symbol and a shortname, like [@tidyverse].\nCreating a BibTeX file\nMost reference software like EndNote, Zotero or Mendeley have exporting options that can export to BibTeX format. You just need to check the shortnames in the resulting file.\nYou can also make a BibTeX file and add references manually. In RStudio, go to File > New File… > Text File and save the file as “bibliography.bib”.\nNext, add the line bibliography: bibliography.bib to your YAML header.\nAdding references\nYou can add references to a journal article in the following format:\n@article{shortname,\n  author = {Author One and Author Two and Author Three},\n  title = {Paper Title},\n  journal = {Journal Title},\n  volume = {vol},\n  number = {issue},\n  pages = {startpage--endpage},\n  year = {year},\n  doi = {doi}\n}\nSee A complete guide to the BibTeX format for instructions on citing books, technical reports, and more.\nYou can get the reference for an R package using the functions citation() and toBibtex(). You can paste the bibtex entry into your bibliography.bib file. Make sure to add a short name (e.g., “ggplot2”) before the first comma to refer to the reference.\n\ncitation(package=\"ggplot2\") %>% toBibtex()\n\n@Book{,\n  author = {Hadley Wickham},\n  title = {ggplot2: Elegant Graphics for Data Analysis},\n  publisher = {Springer-Verlag New York},\n  year = {2016},\n  isbn = {978-3-319-24277-4},\n  url = {https://ggplot2.tidyverse.org},\n}\n\n\nGoogle Scholar entries have a BibTeX citation option. This is usually the easiest way to get the relevant values, although you have to add the DOI yourself. You can keep the suggested shortname or change it to something that makes more sense to you.\n\n\n\n\n\n\n\n\nCiting references\nYou can cite references in text like this:\nThis tutorial uses several R packages [@tidyverse;@rmarkdown].\nThis tutorial uses several R packages (Allaire et al., 2018; Wickham, 2017).\nPut a minus in front of the @ if you just want the year:\nFranconeri and colleagues [-@franconeri2021science] review research-backed guidelines for creating effective and intuitive visualizations.\nFranconeri and colleagues (2021) review research-backed guidelines for creating effective and intuitive visualizations.\nCitation styles\nYou can search a list of style files (e.g., APA, MLA, Harvard) and download a file that will format your bibliography. You’ll need to add the line csl: filename.csl to your YAML header.\nReference section\nBy default, the reference section is added to the end of the document. If you want to change the position (e.g., to add figures and tables after the references), include <div id=\"refs\"></div> where you want the references.\n\n\n\n\n\n\nNote\n\n\n\nAdd in-text citations and a reference list to your report.\n\n\n\n10.2.4 Interactive tables\nOne way to make your reports more exciting is to use interactive tables. The DT::datatable() function displays a table with some extra interactive elements to allow readers to search and reorder the data, as well as controlling the number of rows shown at once. This can be especially helpful. This only works with HTML output types. The DT website has extensive tutorials, but we’ll cover the basics here.\n\nlibrary(DT)\n\nscotpop <- read_csv(\"data/scottish_population.csv\", \n                    show_col_types = FALSE)\n\ndatatable(data = scotpop)\n\n\n\n\n\n\nYou can customise the display, such as changing column names, adding a caption, moving the location of the filter boxes, removing row names, applying classes to change table appearance, and applying advanced options.\n\n# https://datatables.net/reference/option/\nmy_options <- list(\n  pageLength = 5, # how many rows are displayed\n  lengthChange = FALSE, # whether pageLength can change\n  info = TRUE, # text with the total number of rows\n  paging = TRUE, # if FALSE, the whole table displays\n  ordering = FALSE, # whether you can reorder columns\n  searching = FALSE # whether you can search the table\n)\n\ndatatable(\n  data = scotpop,\n  colnames = c(\"County\", \"Population\"),\n  caption = \"The population of Scottish counties.\",\n  filter = \"none\", # \"none\", \"bottom\" or \"top\"\n  rownames = FALSE, # removes the number at the left\n  class = \"cell-border hover stripe\", # default is \"display\"\n  options = my_options\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCreate an interactive table like the one below from the diamonds dataset of diamonds where the table value is greater than 65 (the whole table is much too large to display with an interactive table). Show 20 items by default and remove the search box, but leave in the filter and other default options.\n\n\n\n\n\n\n\n\n\n\nSolution\n\nmy_options <- list(\n  pageLength = 20, # how many rows are displayed\n  searching = FALSE # whether you can search the table\n)\n\ndiamonds %>% \n  filter(table > 65) %>%\n  select(-table, -(x:z)) %>%\n  DT::datatable(\n    caption = \"All diamonds with table > 65.\",\n    options = my_options\n  )\n\n\n\n\n\n10.2.5 Other formats\nYou can create more than just reports with R Markdown. You can also create presentations, interactive dashboards, books, websites, and web applications.\nPresentations\nYou can choose a presentation template when you create a new R Markdown document. We’ll use ioslides for this example, but the other formats work similarly.\n\n\n\n\nIoslides RMarkdown template.\n\n\n\n\nThe main differences between this and the Rmd files you’ve been working with until now are that the output type in the YAML header is ioslides_presentation instead of html_document and this format requires a specific title structure. Each slide starts with a level-2 header.\nThe template provides you with examples of text, bullet point, code, and plot slides. You can knit this template to create an HTML document with your presentation. It often looks odd in the RStudio built-in browser, so click the button to open it in a web browser. You can use the space bar or arrow keys to advance slides.\nThe code below shows how to load some packages and display text, a table, and a plot. You can see the HTML output here.\n\n\nSolution\n\n\n---\ntitle: \"Presentation Demo\"\nauthor: \"Lisa DeBruine\"\noutput: ioslides_presentation\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(tidyverse)\nlibrary(kableExtra)\n```\n\n## Slide with Markdown\n\nThe following slides will present some data from the `diamonds` dataset from **ggplot2**.\n\nDiamond price depends on many features, such as:\n\n- cut\n- colour\n- clarity\n- carats\n\n## Slide with a Table\n\n```{r}\ndiamonds %>%\n  group_by(cut, color) %>%\n  summarise(avg_price = mean(price),\n            .groups = \"drop\") %>%\n  pivot_wider(names_from = cut, values_from = avg_price) %>%\n  kable(digits = 0, caption = \"Mean diamond price by cut and colour.\") %>%\n  kable_material()\n```\n\n## Slide with a Plot\n\n```{r pressure}\nggplot(diamonds, aes(x = cut, y = price, color = color)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(aes(x = as.integer(cut)), \n               fun = mean, geom = \"line\") +\n  scale_x_discrete(position = \"top\") +\n  scale_color_viridis_d(guide = guide_legend(reverse = TRUE)) +\n  theme_minimal() \n```\n\n\nDashboards\nDashboards are a way to display text, tables, and plots with dynamic formatting. After you install flexdashboard, you can choose a flexdashboard template when you create a new R Markdown document.\n\n\n\n\nFlexdashboard RMarkdown template.\n\n\n\n\nThe code below shows how to load some packages, display two tables in a tabset, and display two plots in a column. You can see the HTML output here.\n\n\nSolution\n\n\n---\ntitle: \"Flexdashboard Demo\"\noutput: \n  flexdashboard::flex_dashboard:\n    social: [ \"twitter\", \"facebook\", \"linkedin\", \"pinterest\" ]\n    source_code: embed\n    orientation: columns\n    vertical_layout: fill\n---\n\n```{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(DT) # for interactive tables\ntheme_set(theme_minimal())\n```\n\nColumn {data-width=350, .tabset}\n--------------------------------\n\n### By Cut\n\nThis table uses `kableExtra` to render the table with a specific theme.\n\n```{r}\ndiamonds %>%\n  group_by(cut) %>%\n  summarise(avg = mean(price),\n            .groups = \"drop\") %>%\n  kable(digits = 0, \n        col.names = c(\"Cut\", \"Average Price\"),\n        caption = \"Mean diamond price by cut.\") %>%\n  kable_classic()\n```\n\n### By Colour\n\nThis table uses `DT::datatable()` to render the table with a searchable interface.\n\n```{r}\ndiamonds %>%\n  group_by(color) %>%\n  summarise(avg = mean(price),\n            .groups = \"drop\") %>%\n  DT::datatable(colnames = c(\"Colour\", \"Average Price\"), \n                caption = \"Mean diamond price by colour\",\n                options = list(pageLength = 5),\n                rownames = FALSE) %>%\n  DT::formatRound(columns=2, digits=0)\n```\n\nColumn {data-width=350}\n-----------------------\n\n### By Clarity\n\n```{r by-clarity, fig.cap = \"Diamond price by clarity\"}\nggplot(diamonds, aes(x = clarity, y = price)) +\n  geom_boxplot() \n```\n\n\n### By Carats\n\n```{r by-carat, fig.cap = \"Diamond price by carat\"}\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_smooth()\n```\n\n\nChange the size of your web browser to see how the boxes, tables and figures change.\nThe best way to figure out how to format a dashboard is trial and error, but you can also look at some sample layouts.\nBooks\nYou can create online books with bookdown. In fact, the book you’re reading was created using bookdown. After you download the package, start a new project and choose “Book project using bookdown” from the list of project templates.\n\n\n\n\nBookdown project template.\n\n\n\n\nEach chapter is written in a separate .Rmd file and the general book settings can be changed in the _bookdown.yml and _output.yml files.\nWebsites\nYou can create a simple website the same way you create any R Markdown document. Choose “Simple R Markdown Website” from the project templates to get started. See Appendix K for a step-by-step tutorial.\nFor more complex, blog-style websites, you can investigate blogdown. After you install this package, you will also be able to create template blogdown projects to get you started.\nShiny\nTo get truly interactive, you can take your R coding to the next level and learn Shiny. Shiny apps let your R code react to user input. You can do things like make a word cloud, search a google spreadsheet, or conduct a survey.\nThis is well outside the scope of this class, but the skills you’ve learned here provide a good start. The free book Building Web Apps with R Shiny by one of the authors of this book can get you started creating shiny apps.\n\n10.2.6 Resources\n\nRStudio Formats\nR Markdown Cookbook\nDT\nFlexdashboard\nBookdown\nBlogdown\nShiny\nBuilding Web Apps with R Shiny"
  },
  {
    "objectID": "10-custom.html#course-complete",
    "href": "10-custom.html#course-complete",
    "title": "10  Customising Visualisations & Reports",
    "section": "\n10.3 Course Complete",
    "text": "10.3 Course Complete\nAnd so, we are done. We’ve covered a huge amount over the course of Applied Data Skills, and whilst you’re likely more comfortable with some bits than others, the skills you have developed are truly impressive. Even if you go no further than what you’ve learned in this book, you can now work reproducibly to produce informative summaries and visualisations that provide new insights into your data and reduce human error.\nBut it’s also important to recognise that your knowledge of R will never be complete. In the course of writing this book, the entire ADS team have learned new functions, new arguments, new approaches, and new reasons to love or loathe certain data visualisations. The flexibility and possibility of R is what makes it frustrating and empowering in equal measure. What we hope more than anything is that Applied Data Skills is the start of your journey with R, not the end. Please keep in touch, we’d love to see where it takes you.\nEmily Nordmann,\nLisa DeBruine,\nGaby Mahrholz,\nJaimie Torrance\n\n\n\n\nAllaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., & Chang, W. (2018). Rmarkdown: Dynamic documents for r. https://CRAN.R-project.org/package=rmarkdown\n\n\nFranconeri, S. L., Padilla, L. M., Shah, P., Zacks, J. M., & Hullman, J. (2021). The science of visual data communication: What works. Psychological Science in the Public Interest, 22(3), 110–161. https://doi.org/10.1177/15291006211051956\n\n\nWickham, H. (2017). Tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse"
  },
  {
    "objectID": "11-refs.html",
    "href": "11-refs.html",
    "title": "References",
    "section": "",
    "text": "Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins,\nA., Wickham, H., Cheng, J., & Chang, W. (2018). Rmarkdown:\nDynamic documents for r. https://CRAN.R-project.org/package=rmarkdown\n\n\nDeBruine, L., & Barr, D. (2020). Data skills for reproducible\nresearch. Zenodo. https://doi.org/10.5281/zenodo.3564348\n\n\nFranconeri, S. L., Padilla, L. M., Shah, P., Zacks, J. M., &\nHullman, J. (2021). The science of visual data communication: What\nworks. Psychological Science in the Public Interest,\n22(3), 110–161. https://doi.org/10.1177/15291006211051956\n\n\nNordmann, E., McAleer, P., Toivo, W., Paterson, H., & DeBruine, L.\nM. (2021). Data visualisation using R, for researchers\nwho don’t use R. PsyArXiv. https://doi.org/10.31234/osf.io/4huvw\n\n\nR Core Team. (2022). R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nWickham, H. (2017). Tidyverse: Easily install and load the\n’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H. (2021). Tidyverse: Easily install and load the\ntidyverse. https://CRAN.R-project.org/package=tidyverse\n\n\nXie, Y. (2021). Tinytex: Helper functions to install and maintain\nTeX live, and compile LaTeX documents. https://github.com/yihui/tinytex"
  },
  {
    "objectID": "12-license.html#citation",
    "href": "12-license.html#citation",
    "title": "License/Citation",
    "section": "Citation",
    "text": "Citation\n\n\n\nDOI\n\n\nNordmann, E. & DeBruine, L. (2023) Applied Data Skills. v2.0. Retrieved from https://psyteachr.github.io/ads-v2/ doi: 10.5281/zenodo.6365077\n\n\n\n\nDeBruine, L., & Barr, D. (2020). Data skills for reproducible research. Zenodo. https://doi.org/10.5281/zenodo.3564348\n\n\nNordmann, E., McAleer, P., Toivo, W., Paterson, H., & DeBruine, L. M. (2021). Data visualisation using R, for researchers who don’t use R. PsyArXiv. https://doi.org/10.31234/osf.io/4huvw"
  },
  {
    "objectID": "app-installing-r.html#installing-base-r",
    "href": "app-installing-r.html#installing-base-r",
    "title": "Appendix A — Installing R",
    "section": "\nA.1 Installing Base R",
    "text": "A.1 Installing Base R\nInstall base R. Choose the download link for your operating system (Linux, Mac OS X, or Windows).\nIf you have a Mac, install the latest release from the newest R-x.x.x.pkg link (or a legacy version if you have an older operating system). You may also need to install XQuartz to be able to use some visualisation packages.\nIf you are installing the Windows version, choose the “base” subdirectory and click on the download link at the top of the page.\nIf you are using Linux, choose your specific operating system and follow the installation instructions.\n\n\n\n\n\n\nInstallation Location\n\n\n\nIt can often cause problems to install R on a network or cloud drive, such as OneDrive or DropBox. It’s better to install these programs on your computer’s drive. Depending on your computer’s settings, you may have to get IT support to give you access to installing programs.\nIt can also cause rare, but hard-to-debug problems if any of the folders in the path where you install R have non-Latin characters, including Chinese characters or Latin characters with accents (e.g., C:\\\\Daniël\\Programs\\)."
  },
  {
    "objectID": "app-installing-r.html#installing-rstudio",
    "href": "app-installing-r.html#installing-rstudio",
    "title": "Appendix A — Installing R",
    "section": "\nA.2 Installing RStudio",
    "text": "A.2 Installing RStudio\nGo to rstudio.com and download the RStudio Desktop (Open Source License) version for your operating system under the list titled Installers for Supported Platforms."
  },
  {
    "objectID": "app-installing-r.html#installing-rtools",
    "href": "app-installing-r.html#installing-rtools",
    "title": "Appendix A — Installing R",
    "section": "\nA.3 Installing RTools",
    "text": "A.3 Installing RTools\nIf you are using Windows, after you install R, you should also install RTools; use the “recommended” version highlighted near the top of the list. RTools is used for installing and loading some packages. You can get started without installing RTools, but if you’re having problems with installing and loading some packages, this should be the first thing you try.\nRTools will require you to put it “on the PATH”. The instructions for this can seem a bit vague - the easiest way to do it is to open RStudio, run the below code in the console:\n\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\n\nOnce you’ve done that, restart R by clicking Session - Restart R and then run the below code in the console which should give you the path to your RTools installation:\n\nSys.which(\"make\")\n\n           make \n\"/usr/bin/make\""
  },
  {
    "objectID": "app-installing-r.html#sec-rstudio-settings",
    "href": "app-installing-r.html#sec-rstudio-settings",
    "title": "Appendix A — Installing R",
    "section": "\nA.4 RStudio Settings",
    "text": "A.4 RStudio Settings\nThere are a few settings you should fix immediately after updating RStudio. Go to Tools > Global Options… (⌘,), and in the General tab, uncheck the box that says Restore .RData into workspace at startup. If you keep things around in your workspace, things will get messy, and unexpected things will happen. You should always start with a clear workspace. This also means that you never want to save your workspace when you exit, so set this to Never. The only thing you want to save are your scripts.\nYou may also want to change the appearance of your code. Different fonts and themes can sometimes help with visual difficulties or dyslexia.\n\n\n\n\nRStudio General and Appearance settings\n\n\n\n\nYou may also want to change the settings in the Code tab. For example, Lisa prefers two spaces instead of tabs for my code and likes to be able to see the whitespace characters. But these are all a matter of personal preference.\n\n\n\n\nRStudio Code settings"
  },
  {
    "objectID": "app-installing-r.html#installing-latex",
    "href": "app-installing-r.html#installing-latex",
    "title": "Appendix A — Installing R",
    "section": "\nA.5 Installing LaTeX",
    "text": "A.5 Installing LaTeX\nYou can install the LaTeX typesetting system to produce PDF reports from RStudio. Without this additional installation, you will be able to produce reports in HTML but not PDF. To generate PDF reports, you will additionally need to install tinytex(Xie, 2021) and run the following code:\n\n# run this in the console\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\n\n\n\n\nXie, Y. (2021). Tinytex: Helper functions to install and maintain TeX live, and compile LaTeX documents. https://github.com/yihui/tinytex"
  },
  {
    "objectID": "app-updating-r.html#updating-rstudio",
    "href": "app-updating-r.html#updating-rstudio",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.1 Updating RStudio",
    "text": "B.1 Updating RStudio\nRStudio is the easiest component to update. Typically, updates to RStudio won’t affect your code, instead they add in new features, like spell-check or upgrades to what RStudio can do. There’s usually very little downside to updating RStudio and it’s easy to do.\nClick Help - Check for updates\n\n\n\n\nUpdating RStudio\n\n\n\n\nIf an update is available, it will prompt you to download it and you can install it as usual."
  },
  {
    "objectID": "app-updating-r.html#updating-r",
    "href": "app-updating-r.html#updating-r",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.2 Updating R",
    "text": "B.2 Updating R\nFinally, you may also wish to update R itself. The key thing to be aware of is that when you update R, if you just download the latest version from the website, you will lose all your packages.\n\nB.2.1 Windows\nThe easiest way to update R on Windows and not cause yourself a huge headache is to use the installr package. When you use the updateR() function, a series of dialogue boxes will appear. These should be fairly self-explanatory but there is a full step-by-step guide available for how to use installr, the important bit is to select “Yes” when it asked if you would like to copy your packages from the older version of R.\n\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()\n\n\nB.2.2 Mac\nFor a Mac, you can use the updateR”, “https://github.com/AndreaCirilloAC/updateR”)` package. You’ll need to install this from GitHub. You will be asked to type your system password (that you use to log into your computer) in the console pane. If relevant, it will ask you if you want to restore your packages for a new major version.\n\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()"
  },
  {
    "objectID": "app-updating-r.html#updating-packages",
    "href": "app-updating-r.html#updating-packages",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.3 Updating packages",
    "text": "B.3 Updating packages\nPackage developers will occasionally release updates to their packages. This is typically to add in new functions to the package, or to fix or amend existing functions. Be aware that some package updates may cause your previous code to stop working. This does not tend to happen with minor updates to packages, but occasionally with major updates, you can have serious issues if the developer has made fundamental changes to how the code works. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) - don’t do it before an assessment or deadline just in case!\nTo update an individual package, the easiest way is to use the install.packages() function, as this always installs the most recent version of the package.\n\ninstall.packages(\"tidyverse\")\n\nTo update multiple packages, or indeed all packages, RStudio provides helpful tools. Click Tools - Check for Package Updates. A dialogue box will appear and you can select the packages you wish to update. Be aware that if you select all packages, this may take some time and you will be unable to use R whilst the process completes.\n\n\n\n\nUpdating packages with RStudio"
  },
  {
    "objectID": "app-updating-r.html#sec-package-install-troubleshooting",
    "href": "app-updating-r.html#sec-package-install-troubleshooting",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.4 Troubleshooting",
    "text": "B.4 Troubleshooting\nOccasionally, you might have a few problem packages that seemingly refuse to update. For me, rlang and vctrs cause me no end of trouble. These aren’t packages that you will likely every explicitly load, but they’re required beneath the surface for R to do things like knit your Markdown files etc.\n\nB.4.1 Non-zero exit status\nIf you try to update a package and get an error message that says something like Warning in install.packages : installation of package ‘vctrs’ had non-zero exit status or perhaps Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :  namespace 'rlang' 0.4.9 is being loaded, but >= 0.4.10 is required one solution I have found is to manually uninstall the package, restart R, and then install the package new, rather than trying to update an existing version. The installr package also has a useful function for uninstalling packages.\n\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")\n\n\nB.4.2 Cannot open file\nYou may get the following error after trying to install any packages at all:\n\nError in install packages : Cannot open file ‘C:/…..’: Permission denied\n\nThis usually indicates a permissions problem with writing to the default library (the folder that packages are kept in). Sometimes this means that you need to install R and RStudio as administrator or run it as administrator.\nOne other fix may be to change the library location using the following code (check in “C:/Program Files/R” for what version you should have instead of “R-3.5.2”):\n\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nIf that works and you can install packages, set this library path permanently:\n\nInstall the usethis package\nRun usethis::edit_r_profile() in the console; it will open up a blank file\nPaste into the file (your version of): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nSave and close the file\nRestart R for changes to take effect\n\nThe code in your .Rprofile will now run every time you start up R.\nAs always, if you’re having issues, please ask on Teams or come to office hours."
  },
  {
    "objectID": "app-symbols.html",
    "href": "app-symbols.html",
    "title": "Appendix C — Symbols",
    "section": "",
    "text": "Symbol\npsyTeachR Term\nAlso Known As\n\n\n\n()\n(round) brackets\nparentheses\n\n\n[]\nsquare brackets\nbrackets\n\n\n{}\ncurly brackets\nsquiggly brackets\n\n\n<>\nchevrons\nangled brackets / guillemets\n\n\n<\nless than\n\n\n\n>\ngreater than\n\n\n\n&\nampersand\n“and” symbol\n\n\n#\nhash\npound / octothorpe\n\n\n/\nslash\nforward slash\n\n\n\\\nbackslash\n\n\n\n-\ndash\nhyphen / minus\n\n\n_\nunderscore\n\n\n\n*\nasterisk\nstar\n\n\n^\ncaret\npower symbol\n\n\n~\ntilde\ntwiddle / squiggle\n\n\n=\nequal sign\n\n\n\n==\ndouble equal sign\n\n\n\n.\nfull stop\nperiod / point\n\n\n!\nexclamation mark\nbang / not\n\n\n?\nquestion mark\n\n\n\n’\nsingle quote\nquote / apostrophe\n\n\n”\ndouble quote\nquote\n\n\n%>%\npipe\nmagrittr pipe\n\n\n|\nvertical bar\npipe\n\n\n,\ncomma\n\n\n\n;\nsemi-colon\n\n\n\n:\ncolon\n\n\n\n@\n“at” symbol\nvarious hilarious regional terms\n\n\n…\nglossary(\"ellipsis\")\ndots\n\n\n\n\n\n\n\nFigure C.1: Image by James Chapman/Soundimals"
  },
  {
    "objectID": "app-conventions.html#test-yourself",
    "href": "app-conventions.html#test-yourself",
    "title": "Appendix D — Conventions",
    "section": "\nD.1 Test Yourself",
    "text": "D.1 Test Yourself\nI am going to learn a lot: \nTRUE\nFALSE\n\n\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\n\n\n\n\n\n\nHidden Solutions\n\nYou found it!"
  },
  {
    "objectID": "app-conventions.html#callout-boxes",
    "href": "app-conventions.html#callout-boxes",
    "title": "Appendix D — Conventions",
    "section": "\nD.2 Callout boxes",
    "text": "D.2 Callout boxes\nSee the quarto reference for more options.\n\n\n\n\n\n\nNote\n\n\n\nInformational asides.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTips\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotes to warn you about something.\n\n\n\n\n\n\n\n\nDanger\n\n\n\nNotes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotes about things that are important.\n\n\n\n\n\n\n\n\nTry (click to expand)\n\n\n\n\n\nTry it yourself."
  },
  {
    "objectID": "app-conventions.html#code-and-output",
    "href": "app-conventions.html#code-and-output",
    "title": "Appendix D — Conventions",
    "section": "\nD.3 Code and Output",
    "text": "D.3 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n\n\n\n\nWickham, H. (2021). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse"
  },
  {
    "objectID": "app-teams.html#what-is-the-best-way-to-share-r-code-on-microsoft-teams",
    "href": "app-teams.html#what-is-the-best-way-to-share-r-code-on-microsoft-teams",
    "title": "Appendix E — Using Teams",
    "section": "\nE.1 What is the best way to share R code on Microsoft Teams?",
    "text": "E.1 What is the best way to share R code on Microsoft Teams?\nYou have run into a problem and need to get help on MS Teams. What is the right way to share your code?\nPlease do not share a screenshot unless you are asked, or if it is not the code that is giving you problems, but something weird is happening with the RStudio IDE.\nIf it’s your code that is not working, it is almost always better to copy and paste the code, because then people who are trying to help you can copy and paste the code exactly to try it out, rather than having to re-type everything from the image. Let’s look at an example. Below is a screenshot of how the RStudio IDE might look when your code throws an error. Here the code block labelled cars is causing the error.\n\n\na screenshot of RStudio IDE showing an error indicated by the red arrow\n\n\nThe particular error that our code threw was\nError in mtcars %>% filter(mpg > 20) : could not find function \"%>%\"\nAnd the code that threw it was\n\nmtcars %>%\n  filter(mpg > 20)\n\nNote that you can select and copy the code above if you wanted to run it yourself, but you could not do that if all you had to rely on was the screenshot.\nCopying the code and/or error in RStudio is easy; just highlight the code using the mouse and press Ctrl-C.\nIf you just paste the code into a Teams channel, the formatting is not so nice; you lose the formatting that allows you to read the code easily.\n\n\nA screenshot of MS Teams with the code pasted directly in. Not pretty!\n\n\nHere are two ways to get your code into Teams, one that is quick and easy but not very flexible, and another that is far more flexible but requires more steps.\n\nE.1.1 Quick and easy method\nFirst, if it is just a short function call, a single line, or an error, you can signal that text is meant to appear as code by surrounding it by single backticks—i.e., putting a backtick (`) right before and right after the text that you want to be formatted as code. Teams will automatically format it for you.\nFor multi-line code, the easiest and fastest way is just to type three backticks inside your message at the beginning of a line. Any subsequent text you enter will be treated as code. To get to the beginning of a line without submitting your post, press Ctrl-Enter while typing your message. Then type the three backticks, and paste your code right into the gray box that automatically appears. Press Enter twice in a row to get back out of the code entry box. So your message might look like this. \nAbove, I surrounded the error message Error in mtcars %>% filter(mpg > 20) : could not find function \"%>%\" with single backticks to indicate code, and we typed triple backticks at the start of the line to create a code chunk. (The next method might be easier for making multi-line posts.)\n\nE.1.2 More flexible method\nThere is a more flexible (and possibly easier) way. Before pasting any text, click on the icon that looks like the letter “A”, highlighted below.\n\n\nScreenshot of Teams showing the icon that looks like an “A”\n\n\nThis will open up options for text formatting and will allow you to easily create a multi-line post. From those options, select the icon that looks like </>, which stands for code.\n\n\nScreenshot of Teams formatting icons, with code icon highlighted\n\n\nThe code icon will open a window where you can paste your code. In the dropdown menu on the top right, select ‘R’ as the type of code. This will give you syntax highlighting.\n\n\nScreenshot of Teams formatting icons, with code icon highlighted\n\n\nHere is how you might begin your post.\n\n\nScreenshot of Teams with unsubmitted post\n\n\n\nE.1.3 Reprex\nYou might see people in coding forums like StackOverflow asking for a “reprex”, or a reproducible example. This is the smallest, completely self-contained example of your problem or question.\nFor example, you may have a question about how to figure out how to select rows that contain the value “test” in a certain column, but it isn’t working. It’s clearer if you can provide a concrete example, but you don’t want to have to type out the whole table you’re using or all the code that got you to this point in your script.\nYou can include a very small table with just the basics or a smaller version of your problem. Make comments at each step about what you expect and what you actually got.\nWhich version is easier for you to figure out the solution?\n\n# this doesn't work\nno_test_data <- data %>%\n  filter(!str_detect(type, \"test\"))\n\n\n# with a minimal example table\ndata <- tribble(\n  ~id, ~type, ~x,\n  1, \"test\", 12,\n  2, \"testosterone\", 15,\n  3, \"estrogen\", 10\n)\n\n# this should keep IDs 2 and 3, but removes ID 2\nno_test_data <- data %>%\n  filter(!str_detect(type, \"test\"))\n\nOne of the big benefits to creating a reprex is that you often solve your own problem while you’re trying to break it down to explain to someone else.\nIf you really want to go down the rabbit hole, you can create a reproducible example using the reprex package from tidyverse."
  },
  {
    "objectID": "app-teams.html#screenshots",
    "href": "app-teams.html#screenshots",
    "title": "Appendix E — Using Teams",
    "section": "\nE.2 Screenshots",
    "text": "E.2 Screenshots\nIf you do need to take a screenshot, for example, if something goes wrong during installation, please use the screenshot functions built-in to your computer rather than taking a photo of your screen using your phone.\n\nE.2.1 Taking a screenshot on Windows\n\nUse the Windows search function to search for “Snip & Sketch”\nClick “New” then “Snip now”\nUse the tool to select the area on the screen you want to take a screenshot of. This photo will automatically be copied to your clipboard, so you can paste it into e.g., a Teams chat or a document using Ctrl + V but you can also click the Save icon in the top right to save the screenshot as an image file.\nThe shortcut for the snipping tool is Win + Shift + S.\n\nE.2.2 Taking a screenshot on Mac\n\nPress Shift + Command (⌘) + 4 to bring up the Screenshot app.\nUse the tool to select the area on the screen you want to take a screenshot of.\nIf you see a thumbnail in the corner of your screen, click it to edit the screenshot or drag it into e.g., a Teams chat.\nThis photo will also automatically save to your desktop."
  },
  {
    "objectID": "app-debugging.html#report-setup",
    "href": "app-debugging.html#report-setup",
    "title": "Appendix F — Debugging",
    "section": "\nF.1 Report Setup",
    "text": "F.1 Report Setup\nCreate a new R Markdown file and delete everything below the setup chunk. Edit the YAML header to use a floating table of contents and add the outline of your report.\n---\ntitle: \"Report\"\ndate: \"2022-12-21\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\n\n## Introduction\n\n## Data\n\n### Term 1\n\n### Term 2\n\n## Analysis\n\n## References\nSave this file and knit it. Ideally, this will generate some output in a new tab in the console pane called “Render” that starts with processing file: report-demo.Rmd and ends with Output created: report-demo.html. There will be a lot of output in between, but you don’t need to worry about it until something goes wrong."
  },
  {
    "objectID": "app-debugging.html#yaml-errors",
    "href": "app-debugging.html#yaml-errors",
    "title": "Appendix F — Debugging",
    "section": "\nF.2 YAML Errors",
    "text": "F.2 YAML Errors\nOne of the more frequent problems is errors in the YAML header. Let’s create a few to see how to deal with them.\n\nF.2.1 YAML borders\nDelete the last dash below the header and knit.\n---\ntitle: \"Report\"\ndate: \"2022-12-21\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n--\nThis will actually knit without error (and look odd), but you’ll get a warning about the empty title. This is because R Markdown doesn’t recognise that there even is a YAML header if the three dashes to start and end it aren’t right.\n\nF.2.2 Spaces\nUnlike R and markdown, YAML is extremely picky about spaces. Try removing the space after the colon after “toc”.\n---\ntitle: \"Report\"\ndate: \"2022-12-21\"\noutput: \n  html_document:\n    toc:true\n    toc_float: true\n---\nYou should get an error that looks like this:\nError in yaml::yaml.load(..., eval.expr = TRUE) : \n  Scanner error: mapping values are not allowed in this context at line 6, column 14\nCalls: <Anonymous> ... parse_yaml_front_matter -> yaml_load -> <Anonymous>\nExecution halted\nIf you see Error in yaml and it gives you a line and column number, this refers to the YAML line, so start counting with 1 at the title line. Sometimes the actual problem is in the line above or below the reference. Here, the problem is a missing space in the toc line, but that doesn’t cause an error in the YAML parsing until it gets to the next line.\n\nF.2.3 Indenting\nYAML is also extremely picky about indenting. A common error is not putting html_document: on a separate line when adding options like a table of contents.\n---\ntitle: \"Report\"\ndate: \"2022-12-21\"\noutput: html_document:\n    toc: true\n    toc_float: true\n---\nYou should get an error that looks like this:\nError in yaml::yaml.load(..., eval.expr = TRUE) : \n  Scanner error: mapping values are not allowed in this context at line 3, column 22\nSome indenting problems don’t cause an error, but result in an output that isn’t doing what you expect. Try removing the indent for the table of contents lines and knitting.\n---\ntitle: \"Report\"\ndate: \"2022-12-21\"\noutput: \n  html_document:\n  toc: true\n  toc_float: true\n---"
  },
  {
    "objectID": "app-debugging.html#common-errors",
    "href": "app-debugging.html#common-errors",
    "title": "Appendix F — Debugging",
    "section": "\nF.3 Common Errors",
    "text": "F.3 Common Errors\nThe best way to learn to deal with errors is to make a lot of them. That way, the next time you encounter a similar error, you’ll have some experience solving it.\nRun the following code in the console; don’t add it to the report script.\n\n\n\nRun in the console\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ forcats 0.5.2\n✔ stringr 1.4.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ faux::%||%()             masks purrr::%||%()\n✖ dplyr::filter()          masks stats::filter()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ rvest::guess_encoding()  masks readr::guess_encoding()\n✖ dplyr::lag()             masks stats::lag()\n\n\nNow, make a code chunk somewhere in your report like this and run it interactively (not by knitting). It should create a new table called droids with 6 rows.\ndroids <- starwars %>% filter(species == \"Droid\")\nNow try to knit the report. Because you didn’t load the tidyverse package bundle in the script, you’ll get an error about not being able to find the function %>% (you’ll learn about the pipe in Section 5.3.2). When you knit, any objects in your global environment or packages that you’ve loaded are unavailable and the script only has access to objects it creates and packages it loads.\nAdd library(tidyverse) to the setup chunk and knit to confirm this works.\n\nF.3.1 Could not find function\n\ntitle <- pasteO(\"Lavendar\", \"Haze\")\n\nError in pasteO(\"Lavendar\", \"Haze\"): could not find function \"pasteO\"\n\n\nWhen you get the message could not find function \"func\", usually one of two things has happened: you haven’t loaded the package that the function is from or you’ve made a typo in the function name. In this example, the function is actually paste0() with a zero.\n\nF.3.2 Unused argument\n\nrnorm(N = 10)\n\nError in rnorm(N = 10): unused argument (N = 10)\n\n\nWhen you get the error “unused argument”, it usually means either that you’ve made a typo in an argument name, or the function doesn’t have that argument. Remember that argument, like functions and objects, are case-sensitive. Check the arguments with tab-autocomplete or checking the help for that function.\n\nF.3.3 Non-numeric argument to binary operator\n\n1 + \"A\"\n\nError in 1 + \"A\": non-numeric argument to binary operator\n\n\nWhen you try to apply mathematical operations to objects that aren’t numbers, you get this error. You might see this from a function that internally applies these operators; it just means that the person who wrote the function didn’t specifically check that the arguments you input were numeric and write a more specific error message, they just used what you provided and relied on the error messages from the binary operators. Either way, to solve this you need to figure out what should be numeric, but isn’t.\n\nF.3.4 Tibble columns must have compatible sizes\n\ntibble::tibble(\n  x = 1:2,\n  y = 1:3\n)\n\nError:\n! Tibble columns must have compatible sizes.\n• Size 2: Existing data.\n• Size 3: Column `y`.\nℹ Only values of size one are recycled.\n\n\nThis error occurs when you’re creating a table using tibble() and the columns have different lengths. You can set a column to a single value (i.e., a vector with length 1) and it will be “recycled” for every row, but you can’t give two columns values with different lengths if their lengths are greater than 1.\nThe same problem occurs if the function you’re using adds columns to a tibble. The tidyverse error messages are generall very useful in this case.\n\nmtcars3 <- mutate(mtcars, newcol = 1:3)\n\nError in `mutate()`:\n! Problem while computing `newcol = 1:3`.\n✖ `newcol` must be size 32 or 1, not 3.\n\n\n\nF.3.5 Arguments imply differing number of rows\n\ndata.frame(\n  x = 1:2,\n  y = 1:3\n)\n\nError in data.frame(x = 1:2, y = 1:3): arguments imply differing number of rows: 2, 3\n\n\nA similar problem occurs if you’re using the base R function data.frame() (or the function you’re using does). The error message is different, but it’s the same problem. You will also see a related error message if you use base R techniques to add a column with a different length to the data frame.\n\nmtcars$newcol <- 1:3\n\nError in `$<-.data.frame`(`*tmp*`, newcol, value = 1:3): replacement has 3 rows, data has 32"
  },
  {
    "objectID": "app-debugging.html#debugging-methods",
    "href": "app-debugging.html#debugging-methods",
    "title": "Appendix F — Debugging",
    "section": "\nF.4 Debugging methods",
    "text": "F.4 Debugging methods\n\nF.4.1 Restart and rerun\nIt’s very useful to be able to run code interactively, but this can sometimes lead to confusion about what objects are available in your code. You might have made a data table called profits, and then decided to edit the code to make it slightly differently. If you forgot to re-run the code, you’ll be using the old table in your interactive code, but the new table when you knit.\nRestart R (under the Session menu) and run the code in order up to the chunk where you’re having a problem. You can use the Run menu in the upper right of the source pane to run all chunks above your cursor position.\n\nF.4.2 Comment out\nA useful method of debugging a tricky error is commenting out parts of your code and re-running the code to figure out exactly which code is causing the problem. Try\n\ndat <- starwars %>%\n  select(name, height, mass, species) %>%\n  filter(Species == \"Droid\") %>%\n  select(-species) %>%\n  filter(mass < 100)\n\nError in `filter()`:\n! Problem while computing `..1 = Species == \"Droid\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Species' not found\n\n\nImagine the error message was a bit less helpful. You can try running the code line by line. Either select just the code you want to run, or comment out the code you don’t want to run. Remember to also comment out linking functions at the end of lines, like the pipe (%>%) or the ggplot plus (+).\n\ndat <- starwars #%>%\n  # select(name, height, mass, species) %>%\n  # filter(Species == \"Droid\") %>%\n  # select(-species) %>%\n  # filter(mass < 100)\n\n\n\n\n\n\n\nTip\n\n\n\nYou can comment out multiple lines by selecting them with your cursor and choosing Code > Comment/Uncomment Lines (or using the keyboard shortcut).\n\n\nSelect more code or delete the comments until you locate the error.\n\ndat <- starwars %>%\n  select(name, height, mass, species) %>%\n  filter(Species == \"Droid\") #%>%\n\nError in `filter()`:\n! Problem while computing `..1 = Species == \"Droid\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Species' not found\n\n  # select(-species) %>%\n  # filter(mass < 100)\n\n\nF.4.3 Google the error\nMany error messages seem incomprehensible. Googling this message can often lead you to solutions. Take the famous example of “object of type ‘closure’ is not subsettable”.\n\ndata$x <- 1\n\nError in data$x <- 1: object of type 'closure' is not subsettable\n\n\nA Google search will show several sources explaining this confounding message and how to fix it. Although you may also find Jenny Bryan’s famous talk of the same name, which is an excellent discussion of troubleshooting in R.\n\n\n\n\n\n\nNote\n\n\n\nAn “object of type ‘closure’” is coding jargon for a function (like the type of 1 is numeric or the type of \"A\" is character). And “subsetting” is accessing part of a table using $ or square brackets. Here, it means that data isn’t a table, but actually a function, so you can’t add a column to it.\n\n\n\nF.4.4 Reproducible examples\nYou might see people in coding forums like StackOverflow asking for a “reprex”, or a reproducible example. This is the smallest, completely self-contained example of your problem or question.\nFor example, you may have a question about how to figure out how to select rows that contain the value “test” in a certain column, but it isn’t working. It’s clearer if you can provide a concrete example, but you don’t want to have to type out the whole table you’re using or all the code that got you to this point in your script.\nYou can include a very small table with just the basics or a smaller version of your problem. Make comments at each step about what you expect and what you actually got.\nWhich version is easier for you to figure out the solution?\n\n# this doesn't work\nno_test_data <- data |>\n  filter(!str_detect(type, \"test\"))\n\n… OR …\n\nlibrary(tidyverse)\n\n# with a minimal example table\ndata <- tribble(\n  ~id, ~type, ~x,\n  1, \"test\", 12,\n  2, \"testosterone\", 15,\n  3, \"estrogen\", 10\n)\n\n# this should keep IDs 2 and 3, but removes ID 2\nno_test_data <- data |>\n  filter(!str_detect(type, \"test\"))\n\n# expected to be true\nall(no_test_data$type == c(\"testosterone\", \"estrogen\"))\n\nOne of the big benefits to creating a reprex is that you often solve your own problem while you’re trying to break it down to explain to someone else.\nIf you really want to go down the rabbit hole, you can create a reproducible example using the reprex package from tidyverse."
  },
  {
    "objectID": "app-datatypes.html#basic-data-types",
    "href": "app-datatypes.html#basic-data-types",
    "title": "Appendix G — Data Types",
    "section": "\nG.1 Basic data types",
    "text": "G.1 Basic data types\nData can be numbers, words, true/false values or combinations of these. The basic data types in R are: numeric, character, and logical, as well as the special classes of factor and date/times.\n\n\n\n\nData types are like the categories when you format cells in Excel.\n\n\n\n\n\nG.1.1 Numeric data\nAll of the numbers are numeric data types. There are two types of numeric data, integer and double. Integers are the whole numbers, like -1, 0 and 1. Doubles are numbers that can have fractional amounts. If you just type a plain number such as 10, it is stored as a double, even if it doesn’t have a decimal point. If you want it to be an exact integer, you can use the L suffix (10L), but this distinction doesn’t make much difference in practice.\nIf you ever want to know the data type of something, use the typeof function.\n\ntypeof(10)   # double\ntypeof(10.0) # double\ntypeof(10L)  # integer\n\n[1] \"double\"\n[1] \"double\"\n[1] \"integer\"\n\n\nIf you want to know if something is numeric (a double or an integer), you can use the function is.numeric() and it will tell you if it is numeric (TRUE) or not (FALSE).\n\nis.numeric(10L)\nis.numeric(10.0)\nis.numeric(\"Not a number\")\n\n[1] TRUE\n[1] TRUE\n[1] FALSE\n\n\n\nG.1.2 Character data\nCharacters (also called “strings”) are any text between quotation marks.\n\ntypeof(\"This is a character string\")\ntypeof('You can use double or single quotes')\n\n[1] \"character\"\n[1] \"character\"\n\n\nThis can include quotes, but you have to escape quotes using a backslash to signal that the quote isn’t meant to be the end of the string.\n\nmy_string <- \"The instructor said, \\\"R is cool,\\\" and the class agreed.\"\ncat(my_string) # cat() prints the arguments\n\nThe instructor said, \"R is cool,\" and the class agreed.\n\n\n\nG.1.3 Logical Data\nLogical data (also sometimes called “boolean” values) is one of two values: true or false. In R, we always write them in uppercase: TRUE and FALSE.\n\nclass(TRUE)\nclass(FALSE)\n\n[1] \"logical\"\n[1] \"logical\"\n\n\nWhen you compare two values with an operator, such as checking to see if 10 is greater than 5, the resulting value is logical.\n\nis.logical(10 > 5)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might also see logical values abbreviated as T and F, or 0 and 1. This can cause some problems down the road, so we will always spell out the whole thing.\n\n\n\nG.1.4 Factors\nA factor is a specific type of integer that lets you specify the categories and their order. This is useful in data tables to make plots display with categories in the correct order.\n\nmyfactor <- factor(\"B\", levels = c(\"A\", \"B\",\"C\"))\nmyfactor\n\n[1] B\nLevels: A B C\n\n\nFactors are a type of integer, but you can tell that they are factors by checking their class().\n\ntypeof(myfactor)\nclass(myfactor)\n\n[1] \"integer\"\n[1] \"factor\"\n\n\n\nG.1.5 Dates and Times\nDates and times are represented by doubles with special classes. Although typeof() will tell you they are a double, you can tell that they are dates by checking their class(). Datetimes can have one or more of a few classes that start with POSIX.\n\ndate <- as.Date(\"2022-01-24\")\ndatetime <- ISOdatetime(2022, 1, 24, 10, 35, 00, \"GMT\")\ntypeof(date)\ntypeof(datetime)\nclass(date)\nclass(datetime)\n\n[1] \"double\"\n[1] \"double\"\n[1] \"Date\"\n[1] \"POSIXct\" \"POSIXt\" \n\n\nSee Appendix H for how to use lubridate to work with dates and times.\n\n\n\n\n\n\nNote\n\n\n\nWhat data types are these:\n\n\n100 \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n100L \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n\"100\" \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n100.0 \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n-100L \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\nfactor(100) \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\nTRUE \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n\"TRUE\" \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\nFALSE \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n1 == 2 \ninteger\ndouble\ncharacter\nlogical\nfactor"
  },
  {
    "objectID": "app-datatypes.html#sec-containers",
    "href": "app-datatypes.html#sec-containers",
    "title": "Appendix G — Data Types",
    "section": "\nG.2 Basic container types",
    "text": "G.2 Basic container types\nIndividual data values can be grouped together into containers. The main types of containers we’ll work with are vectors, lists, and data tables.\n\nG.2.1 Vectors\nA vector in R is a set of items (or ‘elements’) in a specific order. All of the elements in a vector must be of the same data type (numeric, character, logical). You can create a vector by enclosing the elements in the function c().\n\n## put information into a vector using c(...)\nc(1, 2, 3, 4)\nc(\"this\", \"is\", \"cool\")\n1:6 # shortcut to make a vector of all integers x:y\n\n[1] 1 2 3 4\n[1] \"this\" \"is\"   \"cool\"\n[1] 1 2 3 4 5 6\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat happens when you mix types? What class is the variable mixed?\n\nmixed <- c(2, \"good\", 2L, \"b\", TRUE)\n\n\n\n\nSolution\n\ntypeof(mixed)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can’t mix data types in a vector; all elements of the vector must be the same data type. If you mix them, R will coerce them so that they are all the same. If you mix doubles and integers, the integers will be changed to doubles. If you mix characters and numeric types, the numbers will be coerced to characters, so 10 would turn into \"10\".\n\n\nSelecting values from a vector\nIf we wanted to pick specific values out of a vector by position, we can use square brackets (an extract operator, or []) after the vector.\n\nvalues <- c(10, 20, 30, 40, 50)\nvalues[2] # selects the second value\n\n[1] 20\n\n\nYou can select more than one value from the vector by putting a vector of numbers inside the square brackets. For example, you can select the 18th, 19th, 20th, 21st, 4th, 9th and 15th letter from the built-in vector LETTERS (which gives all the uppercase letters in the Latin alphabet).\n\nword <- c(18, 19, 20, 21, 4, 9, 15)\nLETTERS[word]\n\n[1] \"R\" \"S\" \"T\" \"U\" \"D\" \"I\" \"O\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan you decode the secret message?\n\nsecret <- c(14, 5, 22, 5, 18, 7, 15, 14, 14, 1, 7, 9, 22, 5, 25, 15, 21, 21, 16)\n\n\n\n\nSolution\n\nLETTERS[secret]\n\n [1] \"N\" \"E\" \"V\" \"E\" \"R\" \"G\" \"O\" \"N\" \"N\" \"A\" \"G\" \"I\" \"V\" \"E\" \"Y\" \"O\" \"U\" \"U\" \"P\"\n\n\n\n\n\nYou can also create ‘named’ vectors, where each element has a name. For example:\n\nvec <- c(first = 77.9, second = -13.2, third = 100.1)\nvec\n\n first second  third \n  77.9  -13.2  100.1 \n\n\nWe can then access elements by name using a character vector within the square brackets. We can put them in any order we want, and we can repeat elements:\n\nvec[c(\"third\", \"second\", \"second\")]\n\n third second second \n 100.1  -13.2  -13.2 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can get the vector of names using the names() function, and we can set or change them using something like names(vec2) <- c(\"n1\", \"n2\", \"n3\").\n\n\nAnother way to access elements is by using a logical vector within the square brackets. This will pull out the elements of the vector for which the corresponding element of the logical vector is TRUE. If the logical vector doesn’t have the same length as the original, it will repeat. You can find out how long a vector is using the length() function.\n\nlength(LETTERS)\nLETTERS[c(TRUE, FALSE)]\n\n[1] 26\n [1] \"A\" \"C\" \"E\" \"G\" \"I\" \"K\" \"M\" \"O\" \"Q\" \"S\" \"U\" \"W\" \"Y\"\n\n\nRepeating Sequences\nHere are some useful tricks to save typing when creating vectors.\nIn the command x:y the : operator would give you the sequence of number starting at x, and going to y in increments of 1.\n\n1:10\n15.3:20.5\n0:-10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n[1] 15.3 16.3 17.3 18.3 19.3 20.3\n [1]   0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10\n\n\nWhat if you want to create a sequence but with something other than integer steps? You can use the seq() function. Look at the examples below and work out what the arguments do.\n\nseq(from = -1, to = 1, by = 0.2)\nseq(0, 100, length.out = 11)\nseq(0, 10, along.with = LETTERS)\n\n [1] -1.0 -0.8 -0.6 -0.4 -0.2  0.0  0.2  0.4  0.6  0.8  1.0\n [1]   0  10  20  30  40  50  60  70  80  90 100\n [1]  0.0  0.4  0.8  1.2  1.6  2.0  2.4  2.8  3.2  3.6  4.0  4.4  4.8  5.2  5.6\n[16]  6.0  6.4  6.8  7.2  7.6  8.0  8.4  8.8  9.2  9.6 10.0\n\n\nWhat if you want to repeat a vector many times? You could either type it out (painful) or use the rep() function, which can repeat vectors in different ways.\n\nrep(0, 10)                      # ten zeroes\nrep(c(1L, 3L), times = 7)       # alternating 1 and 3, 7 times\nrep(c(\"A\", \"B\", \"C\"), each = 2) # A to C, 2 times each\n\n [1] 0 0 0 0 0 0 0 0 0 0\n [1] 1 3 1 3 1 3 1 3 1 3 1 3 1 3\n[1] \"A\" \"A\" \"B\" \"B\" \"C\" \"C\"\n\n\nThe rep() function is useful to create a vector of logical values (TRUE/FALSE or 1/0) to select values from another vector.\n\n# Get IDs in the pattern Y Y N N ...\nids <- 1:40\nyynn <- rep(c(TRUE, FALSE), each = 2, \n            length.out = length(ids))\nids[yynn]\n\n [1]  1  2  5  6  9 10 13 14 17 18 21 22 25 26 29 30 33 34 37 38\n\n\n\nG.2.2 Lists\nRecall that vectors can contain data of only one type. What if you want to store a collection of data of different data types? For that purpose you would use a list. Define a list using the list() function.\n\ndata_types <- list(\n  double = 10.0,\n  integer = 10L,\n  character = \"10\",\n  logical = TRUE\n)\n\nstr(data_types) # str() prints lists in a condensed format\n\nList of 4\n $ double   : num 10\n $ integer  : int 10\n $ character: chr \"10\"\n $ logical  : logi TRUE\n\n\nYou can refer to elements of a list using square brackets like a vector, but you can also use the dollar sign notation ($) if the list items have names.\n\ndata_types$logical\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNote\n\n\n\nExplore the 5 ways shown below to extract a value from a list. What data type is each object? What is the difference between the single and double brackets? Which one is the same as the dollar sign?\n\nbracket1 <- data_types[1]\nbracket2 <- data_types[[1]]\nname1    <- data_types[\"double\"]\nname2    <- data_types[[\"double\"]]\ndollar   <- data_types$double\n\n\n\nThe single brackets (bracket1 and name1) return a list with the subset of items inside the brackets. In this case, that’s just one item, but can be more (try data_types[1:2]). The items keep their names if they have them, so the returned value is list(double = 10).\nThe double brackets (bracket2 and name2 return a single item as a vector. You can’t select more than one item; data_types[[1:2]] will give you a “subscript out of bounds” error.\nThe dollar-sign notation is the same as double-brackets. If the name has spaces or any characters other than letters, numbers, underscores, and full stops, you need to surround the name with backticks (e.g., sales$`Customer ID`).\n\nG.2.3 Tables\nTabular data structures allow for a collection of data of different types (characters, integers, logical, etc.) but subject to the constraint that each “column” of the table (element of the list) must have the same number of elements. The base R version of a table is called a data.frame, while the ‘tidyverse’ version is called a tibble. Tibbles are far easier to work with, so we’ll be using those. To learn more about differences between these two data structures, see vignette(\"tibble\").\n\n# construct a table by column with tibble\navatar <- tibble(\n  name = c(\"Katara\", \"Toph\", \"Sokka\"),\n  bends = c(\"water\", \"earth\", NA),\n  friendly = TRUE\n)\n\n# or by row with tribble\navatar <- tribble(\n  ~name,    ~bends,  ~friendly,\n  \"Katara\", \"water\", TRUE,\n  \"Toph\",   \"earth\", TRUE,\n  \"Sokka\",  NA,      TRUE\n)\n\n\n# export the data to a file\nrio::export(avatar, \"data/avatar.csv\")\n\n# or by importing data from a file\navatar <- rio::import(\"data/avatar.csv\")\n\nTabular data becomes especially important for when we talk about tidy data in Chapter 8, which consists of a set of simple principles for structuring data.\nTable info\nWe can get information about the table using the following functions.\n\n\nncol(): number of columns\n\nnrow(): number of rows\n\ndim(): the number of rows and number of columns\n\nname(): the column names\n\nglimpse(): the column types\n\n\nnrow(avatar)\nncol(avatar)\ndim(avatar)\nnames(avatar)\nglimpse(avatar)\n\n[1] 3\n[1] 3\n[1] 3 3\n[1] \"name\"     \"bends\"    \"friendly\"\nRows: 3\nColumns: 3\n$ name     <chr> \"Katara\", \"Toph\", \"Sokka\"\n$ bends    <chr> \"water\", \"earth\", NA\n$ friendly <lgl> TRUE, TRUE, TRUE\n\n\nAccessing rows and columns\nThere are various ways of accessing specific columns or rows from a table. You’ll be learning more about this in Chapter 8 and Chapter 9.\n\nsiblings   <- avatar %>% slice(1, 3) # rows (by number)\nbends      <- avatar %>% pull(2) # column vector (by number)\nfriendly   <- avatar %>% pull(friendly) # column vector (by name)\nbends_name <- avatar %>% select(bends, name) # subset table (by name)\ntoph       <- avatar %>% pull(name) %>% pluck(2) # single cell\n\nThe code below uses base R to produce the same subsets as the functions above. This format is useful to know about, since you might see them in other people’s scripts.\n\n# base R access\n\nsiblings   <- avatar[c(1, 3), ] # rows (by number)\nbends      <- avatar[, 2] # column vector (by number)\nfriendly   <- avatar$friendly  # column vector (by name)\nbends_name <- avatar[, c(\"bends\", \"name\")] # subset table (by name)\ntoph       <- avatar[[2, 1]] # single cell (row, col)"
  },
  {
    "objectID": "app-datatypes.html#sec-glossary-datatypes",
    "href": "app-datatypes.html#sec-glossary-datatypes",
    "title": "Appendix G — Data Types",
    "section": "\nG.3 Glossary",
    "text": "G.3 Glossary\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\nbase r\n\n\nThe set of R functions that come with a basic installation of R, before you add external packages.\n\n\n\n\ncharacter\n\n\nA data type representing strings of text.\n\n\n\n\ncoercion\n\n\nChanging the data type of values in a vector to a single compatible type.\n\n\n\n\ndata type\n\n\nThe kind of data represented by an object.\n\n\n\n\ndouble\n\n\nA data type representing a real decimal number\n\n\n\n\nescape\n\n\nInclude special characters like ” inside of a string by prefacing them with a backslash.\n\n\n\n\nextract operator\n\n\nA symbol used to get values from a container object, such as [, [[, or $\n\n\n\n\nfactor\n\n\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\n\n\ninteger\n\n\nA data type representing whole numbers.\n\n\n\n\nlist\n\n\nA container data type that allows items with different data types to be grouped together.\n\n\n\n\nlogical\n\n\nA data type representing TRUE or FALSE values.\n\n\n\n\nnumeric\n\n\nA data type representing a real decimal number or integer.\n\n\n\n\noperator\n\n\nA symbol that performs some mathematical or comparative process.\n\n\n\n\ntabular data\n\n\nData in a rectangular table format, where each row has an entry for each column.\n\n\n\n\ntidy data\n\n\nA format for data that maps the meaning onto the structure.\n\n\n\n\nvector\n\n\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings."
  },
  {
    "objectID": "app-dates.html#parsing",
    "href": "app-dates.html#parsing",
    "title": "Appendix H — Dates and Times",
    "section": "\nH.1 Parsing",
    "text": "H.1 Parsing\nDates can be in many formats. The ymd functions can deal with almost all of them, regardless of the punctuation used in the format. All of the examples below produce a date in the standard format “2022-01-03”.\n\n# year-month-day orders\nymd(\"22 Jan 3\")\nymd(\"2022 January 3rd\")\n\n# month-day-year orders\nmdy(\"January 3, 2022\")\nmdy(\"Jan/03/22\")\n\n# day-month-year orders\ndmy(\"3JAN22\")\ndmy(\"3rd of January in the year 2022\")\n\n\n\n\n\n\n\nNote\n\n\n\nSee if you can make a date format that one of the parsers can’t handle.\n\n\nThere are similar functions for date/times, too.\n\nymd_hms(\"2022 Jan 3, 6:05 and 20s pm\")\nmdy_h(\"January 3rd, 2022 at 6pm\")\n\n[1] \"2022-01-03 18:05:20 UTC\"\n[1] \"2022-01-03 18:00:00 UTC\"\n\n\nThe date/time functions can also take a timezone argument. If you don’t specify it, it defaults to “UTC”.\n\nymd_hm(\"2022-01-03 18:05\", tz = \"GMT\")\n\n[1] \"2022-01-03 18:05:00 GMT\""
  },
  {
    "objectID": "app-dates.html#get-parts",
    "href": "app-dates.html#get-parts",
    "title": "Appendix H — Dates and Times",
    "section": "\nH.2 Get Parts",
    "text": "H.2 Get Parts\nYou frequently need to extract parts of a date/time for plotting. The following functions extract specific parts of a date or datetime object. This is a godsend for those of us who never have a clue what week of the year it is today.\n\n# get the date and time when this function is run\nnow <- now(tzone = \"GMT\")\n\n# get separate parts\ntime_parts <- list(\n  second  = second(now),\n  minute  = minute(now),\n  hour    = hour(now),\n  day     = day(now),  # day of the month (same as mday())\n  wday    = wday(now), # day of the week\n  yday    = yday(now), # day of the year\n  week    = week(now),\n  isoweek = isoweek(now), # ISO 8501 week calendar (Monday start)\n  epiweek = epiweek(now), # CDC epidemiological week (Sunday Start)\n  month   = month(now),\n  year    = year(now),\n  tz      = tz(now)\n)\n\nstr(time_parts)\n\nList of 12\n $ second : num 49.2\n $ minute : int 49\n $ hour   : int 14\n $ day    : int 21\n $ wday   : num 4\n $ yday   : num 355\n $ week   : num 51\n $ isoweek: num 51\n $ epiweek: num 51\n $ month  : num 12\n $ year   : num 2022\n $ tz     : chr \"GMT\"\n\n\nThe month() and wday() functions can return factor labels.\n\njan1 <- ymd(20220101)\nwday(jan1, label = TRUE)\nwday(jan1, label = TRUE, abbr = TRUE)\nmonth(jan1, label = TRUE)\nmonth(jan1, label = TRUE, abbr = TRUE)\n\n[1] Sat\nLevels: Sun < Mon < Tue < Wed < Thu < Fri < Sat\n[1] Sat\nLevels: Sun < Mon < Tue < Wed < Thu < Fri < Sat\n[1] Jan\n12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\n[1] Jan\n12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat day of the week were you born?\n\n\n\nSolution\n\nbirthdate <- ymd(19761118) # put your own birthdate here\nwday(birthdate, label = TRUE)\n\n[1] Thu\nLevels: Sun < Mon < Tue < Wed < Thu < Fri < Sat"
  },
  {
    "objectID": "app-dates.html#date-arithmetic",
    "href": "app-dates.html#date-arithmetic",
    "title": "Appendix H — Dates and Times",
    "section": "\nH.3 Date Arithmetic",
    "text": "H.3 Date Arithmetic\nYou can add and subtract dates. For example, you can get the dates two weeks from today by adding weeks(2) to today(). You can probably guess how to add and subtract seconds, minutes, days, months, and years.\n\ntoday() + weeks(1)\n\n[1] \"2022-12-28\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat day of the week will your 100th birthday be?\n\n\n\nSolution\n\nbirthdate <- ymd(19761118) # put your own birthdate here\ncentennial <- birthdate + years(100)\nwday(centennial, label = TRUE, abbr = FALSE)\n\n[1] Wednesday\n7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhat do you think will happen if you subtract one month from March 31st? You get NA, since February doesn’t have a 31st day.\n\nymd(20220331) - months(1)\n\n[1] NA\n\n\nUse the special date operators %m+% and %m-% to add and subtract months without risking an impossible date.\n\nymd(20220331) %m-% months(1)\n\n[1] \"2022-02-28\"\n\n\n\n\n\nH.3.1 First and last of month\nFor things like billing, you might need to find the first or last days of the current, previous, or next month. The rollback() and rollforward() functions are easier than trying to parse dates.\n\nd <- ymd(\"2022-01-24\")\nrollback(d)                          # last day of the previous month\nrollforward(d)                       # last day of the current month\nrollback(d, roll_to_first = TRUE)    # first day of the current month\nrollforward(d, roll_to_first = TRUE) # first day of the next month\n\n[1] \"2021-12-31\"\n[1] \"2022-01-31\"\n[1] \"2022-01-01\"\n[1] \"2022-02-01\"\n\n\n\nH.3.2 Rounding\nYou can round dates and times to the nearest unit. This can be useful when you have, for example, time measured to the nearest second, but want to group data by the nearest hour, rather than extract the hour component.\n\nymd_hm(\"2022-01-24 10:25\") %>% round_date(unit = \"hour\")\nymd_hm(\"2022-01-24 10:30\") %>% round_date(unit = \"hour\")\nymd_hm(\"2022-01-24 10:35\") %>% round_date(unit = \"hour\")\n\n[1] \"2022-01-24 10:00:00 UTC\"\n[1] \"2022-01-24 11:00:00 UTC\"\n[1] \"2022-01-24 11:00:00 UTC\""
  },
  {
    "objectID": "app-dates.html#internationalisation",
    "href": "app-dates.html#internationalisation",
    "title": "Appendix H — Dates and Times",
    "section": "\nH.4 Internationalisation",
    "text": "H.4 Internationalisation\nYou may need to work with dates from a different locale than your computer’s defaults, such as dates written in French or Russian. Or your computer may have a non-English locale. Set the locale argument to the relevant language code.\n\nymd(\"2022 January 24\", locale = \"en_GB\")\nymd(\"2022 Janvier 24\", locale = \"fr_FR\")\nwday(\"2022-01-03\", label = TRUE, locale = \"ru_RU\")\n\n[1] \"2022-01-24\"\n[1] \"2022-01-24\"\n[1] пн\nLevels: вс < пн < вт < ср < чт < пт < сб\n\n\nSome of the locale functions only work on unix-based machines, like Macs or machines running linux.\n\n# check your own locale; doesn't work for Windows\nlocale()\n\n<locale>\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n<date_names>\nDays:   Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday\n        (Thu), Friday (Fri), Saturday (Sat)\nMonths: January (Jan), February (Feb), March (Mar), April (Apr), May (May),\n        June (Jun), July (Jul), August (Aug), September (Sep), October\n        (Oct), November (Nov), December (Dec)\nAM/PM:  AM/PM\n\n\n\n# check which locales are available on your computer\n# doesn't work for Windows\nsystem(\"locale -a\")"
  },
  {
    "objectID": "app-dates.html#example",
    "href": "app-dates.html#example",
    "title": "Appendix H — Dates and Times",
    "section": "\nH.5 Example",
    "text": "H.5 Example\nLet’s work through some examples with downloaded tweets from the class data.\n\n# read all metrics files in data/tweets/\ntweets <- list.files(\n  path = \"data/tweets\", \n  pattern = \"^tweet_activity_metrics\",\n  full.names = TRUE\n) %>%\n  map_df(read_csv) %>%\n  select(!starts_with(\"promoted\"))\n\nThe time column is already in date/time (POSIXct) format, but what if we wanted to plot tweets by hour for each day of the week?\n\ntweets %>%\n  mutate(weekday = wday(time, label = TRUE),\n         hour = hour(time)) %>%\n  ggplot(aes(x = hour, fill = weekday)) +\n  geom_bar(size = 1, alpha = 0.5, show.legend = FALSE) +\n  facet_grid(~weekday) +\n  scale_fill_manual(values = rainbow(7)) +\n  scale_x_continuous(breaks = seq(0, 24, 4))\n\n\n\n\n\n\n\nA nice side-effect of using the lubridate function to get days of the week or months of the year is that the results are an ordered factor, so display correctly in a plot. Let’s display the months in Greek (if that’s available on your system).\n\ntweets %>%\n  mutate(month = month(time, label = TRUE, abbr = FALSE, locale = \"el_GR.UTF-8\")) %>%\n  ggplot(aes(x = month, fill = month)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = NULL, guide = guide_axis(n.dodge=2))"
  },
  {
    "objectID": "app-styling.html#aesthetics",
    "href": "app-styling.html#aesthetics",
    "title": "Appendix I — Styling Plots",
    "section": "\nI.1 Aesthetics",
    "text": "I.1 Aesthetics\n\nI.1.1 Colour/Fill\nThe colour argument changes the point and line colour, while the fill argument changes the interior colour of shapes. Type colours() into the console to see a list of all the named colours in R. Alternatively, you can use hexadecimal colours like \"#FF8000\" or the rgb() function to set red, green, and blue values on a scale from 0 to 1.\nHover over a colour to see its R name.\n\n\n\n\nblack\n\n\ngray1\n\n\ngray2\n\n\ngray3\n\n\ngray4\n\n\ngray5\n\n\ngray6\n\n\ngray7\n\n\ngray8\n\n\ngray9\n\n\ngray10\n\n\ngray11\n\n\ngray12\n\n\ngray13\n\n\ngray14\n\n\ngray15\n\n\ngray16\n\n\ngray17\n\n\ngray18\n\n\ngray19\n\n\ngray20\n\n\ngray21\n\n\ngray22\n\n\ngray23\n\n\ngray24\n\n\ngray25\n\n\ngray26\n\n\ngray27\n\n\ngray28\n\n\ngray29\n\n\ngray30\n\n\ngray31\n\n\ngray32\n\n\ngray33\n\n\ngray34\n\n\ngray35\n\n\ngray36\n\n\ngray37\n\n\ngray38\n\n\ngray39\n\n\ngray40\n\n\ndimgray\n\n\ngray42\n\n\ngray43\n\n\ngray44\n\n\ngray45\n\n\ngray46\n\n\ngray47\n\n\ngray48\n\n\ngray49\n\n\ngray50\n\n\ngray51\n\n\ngray52\n\n\ngray53\n\n\ngray54\n\n\ngray55\n\n\ngray56\n\n\ngray57\n\n\ngray58\n\n\ngray59\n\n\ngray60\n\n\ngray61\n\n\ngray62\n\n\ngray63\n\n\ngray64\n\n\ngray65\n\n\ndarkgray\n\n\ngray66\n\n\ngray67\n\n\ngray68\n\n\ngray69\n\n\ngray70\n\n\ngray71\n\n\ngray72\n\n\ngray73\n\n\ngray74\n\n\ngray\n\n\ngray75\n\n\ngray76\n\n\ngray77\n\n\ngray78\n\n\ngray79\n\n\ngray80\n\n\ngray81\n\n\ngray82\n\n\ngray83\n\n\nlightgray\n\n\ngray84\n\n\ngray85\n\n\ngainsboro\n\n\ngray86\n\n\ngray87\n\n\ngray88\n\n\ngray89\n\n\ngray90\n\n\ngray91\n\n\ngray92\n\n\ngray93\n\n\ngray94\n\n\ngray95\n\n\ngray96\n\n\ngray97\n\n\ngray98\n\n\ngray99\n\n\nwhite\n\n\nsnow4\n\n\nsnow3\n\n\nsnow2\n\n\nsnow\n\n\nrosybrown4\n\n\nrosybrown\n\n\nrosybrown3\n\n\nrosybrown2\n\n\nrosybrown1\n\n\nlightcoral\n\n\nindianred\n\n\nindianred4\n\n\nindianred2\n\n\nindianred1\n\n\nindianred3\n\n\nbrown4\n\n\nbrown\n\n\nbrown3\n\n\nbrown2\n\n\nbrown1\n\n\nfirebrick4\n\n\nfirebrick\n\n\nfirebrick3\n\n\nfirebrick1\n\n\nfirebrick2\n\n\ndarkred\n\n\nred3\n\n\nred2\n\n\nred\n\n\nmistyrose3\n\n\nmistyrose4\n\n\nmistyrose2\n\n\nmistyrose\n\n\nsalmon\n\n\ntomato3\n\n\ncoral4\n\n\ncoral3\n\n\ncoral2\n\n\ncoral1\n\n\ntomato2\n\n\ntomato\n\n\ntomato4\n\n\ndarksalmon\n\n\nsalmon4\n\n\nsalmon3\n\n\nsalmon2\n\n\nsalmon1\n\n\ncoral\n\n\norangered4\n\n\norangered3\n\n\norangered2\n\n\nlightsalmon3\n\n\nlightsalmon2\n\n\nlightsalmon\n\n\nlightsalmon4\n\n\nsienna\n\n\nsienna3\n\n\nsienna2\n\n\nsienna1\n\n\nsienna4\n\n\norangered\n\n\nseashell4\n\n\nseashell3\n\n\nseashell2\n\n\nseashell\n\n\nchocolate4\n\n\nchocolate3\n\n\nchocolate\n\n\nchocolate2\n\n\nchocolate1\n\n\nlinen\n\n\npeachpuff4\n\n\npeachpuff3\n\n\npeachpuff2\n\n\npeachpuff\n\n\nsandybrown\n\n\ntan4\n\n\nperu\n\n\ntan2\n\n\ntan1\n\n\ndarkorange4\n\n\ndarkorange3\n\n\ndarkorange2\n\n\ndarkorange1\n\n\nantiquewhite3\n\n\nantiquewhite2\n\n\nantiquewhite1\n\n\nbisque4\n\n\nbisque3\n\n\nbisque2\n\n\nbisque\n\n\nburlywood4\n\n\nburlywood3\n\n\nburlywood\n\n\nburlywood2\n\n\nburlywood1\n\n\ndarkorange\n\n\nantiquewhite4\n\n\nantiquewhite\n\n\npapayawhip\n\n\nblanchedalmond\n\n\nnavajowhite4\n\n\nnavajowhite3\n\n\nnavajowhite2\n\n\nnavajowhite\n\n\ntan\n\n\nfloralwhite\n\n\noldlace\n\n\nwheat4\n\n\nwheat3\n\n\nwheat2\n\n\nwheat\n\n\nwheat1\n\n\nmoccasin\n\n\norange4\n\n\norange3\n\n\norange2\n\n\norange\n\n\ngoldenrod\n\n\ngoldenrod1\n\n\ngoldenrod4\n\n\ngoldenrod3\n\n\ngoldenrod2\n\n\ndarkgoldenrod4\n\n\ndarkgoldenrod\n\n\ndarkgoldenrod3\n\n\ndarkgoldenrod2\n\n\ndarkgoldenrod1\n\n\ncornsilk\n\n\ncornsilk4\n\n\ncornsilk3\n\n\ncornsilk2\n\n\nlightgoldenrod4\n\n\nlightgoldenrod3\n\n\nlightgoldenrod\n\n\nlightgoldenrod2\n\n\nlightgoldenrod1\n\n\ngold4\n\n\ngold3\n\n\ngold2\n\n\ngold\n\n\nlemonchiffon4\n\n\nlemonchiffon3\n\n\nlemonchiffon2\n\n\nlemonchiffon\n\n\npalegoldenrod\n\n\nkhaki\n\n\ndarkkhaki\n\n\nkhaki4\n\n\nkhaki3\n\n\nkhaki2\n\n\nkhaki1\n\n\nivory4\n\n\nivory3\n\n\nivory2\n\n\nivory\n\n\nbeige\n\n\nlightyellow4\n\n\nlightyellow3\n\n\nlightyellow2\n\n\nlightyellow\n\n\nlightgoldenrodyellow\n\n\nyellow4\n\n\nyellow3\n\n\nyellow2\n\n\nyellow\n\n\nolivedrab\n\n\nolivedrab4\n\n\nolivedrab3\n\n\nolivedrab2\n\n\nolivedrab1\n\n\ndarkolivegreen\n\n\ndarkolivegreen4\n\n\ndarkolivegreen3\n\n\ndarkolivegreen2\n\n\ndarkolivegreen1\n\n\ngreenyellow\n\n\nchartreuse4\n\n\nchartreuse3\n\n\nchartreuse2\n\n\nlawngreen\n\n\nchartreuse\n\n\nhoneydew4\n\n\nhoneydew3\n\n\nhoneydew2\n\n\nhoneydew\n\n\ndarkseagreen4\n\n\ndarkseagreen\n\n\ndarkseagreen3\n\n\ndarkseagreen2\n\n\ndarkseagreen1\n\n\nlightgreen\n\n\npalegreen\n\n\npalegreen4\n\n\npalegreen3\n\n\npalegreen1\n\n\nforestgreen\n\n\nlimegreen\n\n\ndarkgreen\n\n\ngreen4\n\n\ngreen3\n\n\ngreen2\n\n\ngreen\n\n\nmediumseagreen\n\n\nseagreen\n\n\nseagreen3\n\n\nseagreen2\n\n\nseagreen1\n\n\nmintcream\n\n\nspringgreen4\n\n\nspringgreen3\n\n\nspringgreen2\n\n\nspringgreen\n\n\naquamarine3\n\n\naquamarine2\n\n\naquamarine\n\n\nmediumspringgreen\n\n\naquamarine4\n\n\nturquoise\n\n\nmediumturquoise\n\n\nlightseagreen\n\n\nazure4\n\n\nazure3\n\n\nazure2\n\n\nazure\n\n\nlightcyan4\n\n\nlightcyan3\n\n\nlightcyan2\n\n\nlightcyan\n\n\npaleturquoise\n\n\npaleturquoise4\n\n\npaleturquoise3\n\n\npaleturquoise2\n\n\npaleturquoise1\n\n\ndarkslategray\n\n\ndarkslategray4\n\n\ndarkslategray3\n\n\ndarkslategray2\n\n\ndarkslategray1\n\n\ncyan4\n\n\ncyan3\n\n\ndarkturquoise\n\n\ncyan2\n\n\ncyan\n\n\ncadetblue4\n\n\ncadetblue\n\n\nturquoise4\n\n\nturquoise3\n\n\nturquoise2\n\n\nturquoise1\n\n\npowderblue\n\n\ncadetblue3\n\n\ncadetblue2\n\n\ncadetblue1\n\n\nlightblue4\n\n\nlightblue3\n\n\nlightblue\n\n\nlightblue2\n\n\nlightblue1\n\n\ndeepskyblue4\n\n\ndeepskyblue3\n\n\ndeepskyblue2\n\n\ndeepskyblue\n\n\nskyblue\n\n\nlightskyblue4\n\n\nlightskyblue3\n\n\nlightskyblue2\n\n\nlightskyblue1\n\n\nlightskyblue\n\n\nskyblue4\n\n\nskyblue3\n\n\nskyblue2\n\n\nskyblue1\n\n\naliceblue\n\n\nslategray\n\n\nlightslategray\n\n\nslategray3\n\n\nslategray2\n\n\nslategray1\n\n\nsteelblue4\n\n\nsteelblue\n\n\nsteelblue3\n\n\nsteelblue2\n\n\nsteelblue1\n\n\ndodgerblue4\n\n\ndodgerblue3\n\n\ndodgerblue2\n\n\ndodgerblue\n\n\nlightsteelblue4\n\n\nlightsteelblue3\n\n\nlightsteelblue\n\n\nlightsteelblue2\n\n\nlightsteelblue1\n\n\nslategray4\n\n\ncornflowerblue\n\n\nroyalblue\n\n\nroyalblue4\n\n\nroyalblue3\n\n\nroyalblue2\n\n\nroyalblue1\n\n\nghostwhite\n\n\nlavender\n\n\nmidnightblue\n\n\nnavy\n\n\nblue4\n\n\nblue3\n\n\nblue2\n\n\nblue\n\n\ndarkslateblue\n\n\nslateblue\n\n\nmediumslateblue\n\n\nlightslateblue\n\n\nslateblue1\n\n\nslateblue4\n\n\nslateblue3\n\n\nslateblue2\n\n\nmediumpurple4\n\n\nmediumpurple3\n\n\nmediumpurple\n\n\nmediumpurple2\n\n\nmediumpurple1\n\n\npurple4\n\n\npurple3\n\n\nblueviolet\n\n\npurple1\n\n\npurple2\n\n\npurple\n\n\ndarkorchid\n\n\ndarkorchid4\n\n\ndarkorchid3\n\n\ndarkorchid2\n\n\ndarkorchid1\n\n\ndarkviolet\n\n\nmediumorchid4\n\n\nmediumorchid3\n\n\nmediumorchid\n\n\nmediumorchid2\n\n\nmediumorchid1\n\n\nthistle4\n\n\nthistle3\n\n\nthistle\n\n\nthistle2\n\n\nthistle1\n\n\nplum4\n\n\nplum3\n\n\nplum2\n\n\nplum1\n\n\nplum\n\n\nviolet\n\n\ndarkmagenta\n\n\nmagenta3\n\n\nmagenta2\n\n\nmagenta\n\n\norchid4\n\n\norchid3\n\n\norchid\n\n\norchid2\n\n\norchid1\n\n\nmaroon4\n\n\nvioletred\n\n\nmaroon3\n\n\nmaroon2\n\n\nmaroon1\n\n\nmediumvioletred\n\n\ndeeppink3\n\n\ndeeppink2\n\n\ndeeppink\n\n\ndeeppink4\n\n\nhotpink2\n\n\nhotpink1\n\n\nhotpink4\n\n\nhotpink\n\n\nvioletred4\n\n\nvioletred3\n\n\nvioletred2\n\n\nvioletred1\n\n\nhotpink3\n\n\nlavenderblush4\n\n\nlavenderblush3\n\n\nlavenderblush2\n\n\nlavenderblush\n\n\nmaroon\n\n\npalevioletred4\n\n\npalevioletred3\n\n\npalevioletred\n\n\npalevioletred2\n\n\npalevioletred1\n\n\npink4\n\n\npink3\n\n\npink2\n\n\npink1\n\n\npink\n\n\nlightpink\n\n\nlightpink4\n\n\nlightpink3\n\n\nlightpink2\n\n\nlightpink1\n\n\n\n\nI.1.2 Alpha\nThe alpha argument changes transparency (0 = totally transparent, 1 = totally opaque).\n\n\n\n\nVarying alpha values.\n\n\n\n\n\nI.1.3 Shape\nThe shape argument changes the shape of points.\n\n\n\n\nThe 25 shape values\n\n\n\n\n\nI.1.4 Linetype\nYou can probably guess what the linetype argument does.\n\n\n\n\nThe 6 linetype values at different sizes."
  },
  {
    "objectID": "app-styling.html#palettes",
    "href": "app-styling.html#palettes",
    "title": "Appendix I — Styling Plots",
    "section": "\nI.2 Palettes",
    "text": "I.2 Palettes\nDiscrete palettes change depending on the number of categories.\n\n\n\n\nDefault discrete palette with different numbers of levels.\n\n\n\n\n\nI.2.1 Viridis Palettes\nViridis palettes are very good for colourblind-safe and greyscale-safe plots. The work with any number of categories, but are best for larger numbers of categories or continuous colours.\nDiscrete Viridis Palettes\nSet discrete viridis colours with scale_colour_viridis_d() or scale_fill_viridis_d() and set the option argument to one of the options below. Set direction = -1 to reverse the order of colours.\n\n\n\n\nDiscrete viridis palettes.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the end colour is too light for your plot or the start colour too dark, you can set the begin and end arguments to values between 0 and 1, such as scale_colour_viridis_c(begin = .1, end = .9).\n\n\nContinuous Viridis Palettes\nSet continuous viridis colours with scale_colour_viridis_c() or scale_fill_viridis_c() and set the option argument to one of the options below. Set direction = -1 to reverse the order of colours.\n\n\n\n\nContinuous viridis palettes.\n\n\n\n\n\nI.2.2 Brewer Palettes\nBrewer palettes give you a lot of control over plot colour and fill. You set them with scale_color_brewer() or scale_fill_brewer() and set the palette argument to one of the palettes below. Set direction = -1 to reverse the order of colours.\nQualitative Brewer Palettes\nThese palettes are good for categorical data with up to 8 categories (some palettes can handle up to 12). The “Paired” palette is useful if your categories are arranged in pairs.\n\n\n\n\nQualitative brewer palettes.\n\n\n\n\nSequential Brewer Palettes\nThese palettes are good for up to 9 ordinal categories with a lot of categories.\n\n\n\n\nSequential brewer palettes.\n\n\n\n\nDiverging Brewer Palettes\nThese palettes are good for ordinal categories with up to 11 levels where the centre level is a neutral or baseline category and the levels above and below it differ in an important way, such as agree versus disagree options.\n\n\n\n\nDiverging brewer palettes."
  },
  {
    "objectID": "app-styling.html#sec-themes-appendix",
    "href": "app-styling.html#sec-themes-appendix",
    "title": "Appendix I — Styling Plots",
    "section": "\nI.3 Themes",
    "text": "I.3 Themes\nggplot2 has 8 built-in themes that you can add to a plot like plot + theme_bw() or set as the default theme at the top of your script like theme_set(theme_bw()).\n\n\n\n\n{ggplot2} themes.\n\n\n\n\n\nI.3.1 ggthemes\nYou can get more themes from add-on packages, like ggthemes”, “https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/”). Most of the themes also have customscale_functions likescale_colour_economist()`. Their website has extensive examples and instructions for alternate or dark versions of these themes.\n\n\n\n\n{ggthemes} themes.\n\n\n\n\n\nI.3.2 Fonts\nYou can customise the fonts used in themes. All computers should be able to recognise the families “sans”, “serif”, and “mono”, and some computers will be able to access other installed fonts by name.\n\nsans <- g + theme_bw(base_family = \"sans\") + \n  ggtitle(\"Sans\")\nserif <- g + theme_bw(base_family = \"serif\") + \n  ggtitle(\"Serif\")\nmono <- g + theme_bw(base_family = \"mono\") + \n  ggtitle(\"Mono\")\nfont <- g + theme_bw(base_family = \"Comic Sans MS\") + \n  ggtitle(\"Comic Sans MS\")\n\nsans + serif + mono + font + plot_layout(nrow = 1)\n\n\n\nDifferent fonts.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are working on a Windows machine and get the error “font family not found in Windows font database”, you may need to explicitly map the fonts. In your setup code chunk, add the following code, which should fix the error. You may need to do this for any fonts that you specify.\n\n\n\n\n\nThe showtext package is a flexible way to add fonts.\nIf you have a .ttf file from a font site, like Font Squirrel, you can load the file directly using font_add(). Set regular as the path to the file for the regular version of the font, and optionally add other versions. Set the family to the name you want to use for the font. You will need to include any local font files if you are sharing your script with others.\n\nlibrary(showtext)\n\n# font from https://www.fontsquirrel.com/fonts/SF-Cartoonist-Hand\n\nfont_add(\n  regular = \"fonts/cartoonist/SF_Cartoonist_Hand.ttf\",\n  bold = \"fonts/cartoonist/SF_Cartoonist_Hand_Bold.ttf\",\n  italic = \"fonts/cartoonist/SF_Cartoonist_Hand_Italic.ttf\",\n  bolditalic = \"fonts/cartoonist/SF_Cartoonist_Hand_Bold_Italic.ttf\",\n  family = \"cartoonist\" \n)\n\nTo download fonts directly from Google fonts, use the function font_add_google(), set the name to the exact name from the site, and the family to the name you want to use for the font.\n\n# download fonts from Google\nfont_add_google(name = \"Courgette\", family = \"courgette\")\nfont_add_google(name = \"Poiret One\", family = \"poiret\")\n\nAfter you’ve added fonts from local files or Google, you need to make them available to R using showtext_auto(). You will have to do these steps in each script where you want to use the custom fonts.\n\nshowtext_auto() # load the fonts\n\nTo change the fonts used overall in a plot, use the theme() function and set text to element_text(family = \"new_font_family\").\n\na <- g + theme(text = element_text(family = \"courgette\")) +\n  ggtitle(\"Courgette\")\nb <- g + theme(text = element_text(family = \"cartoonist\")) +\n  ggtitle(\"Cartoonist Hand\")\nc <- g + theme(text = element_text(family = \"poiret\")) +\n  ggtitle(\"Poiret One\")\n\na + b + c\n\n\n\nCustom Fonts.\n\n\n\n\nTo set the fonts for individual elements in the plot, you need to find the specific argument for that element. You can use the argument face to choose “bold”, “italic”, or “bolditalic” versions, if they are available.\n\ng + ggtitle(\"Cartoonist Hand\") +\n  theme(\n    title = element_text(family = \"cartoonist\", face = \"bold\"),\n    strip.text = element_text(family = \"cartoonist\", face = \"italic\"),\n    axis.text = element_text(family = \"sans\")\n  )\n\n\n\nMultiple custom fonts on the same plot."
  },
  {
    "objectID": "app-spotify.html#by-artist",
    "href": "app-spotify.html#by-artist",
    "title": "Appendix J — Spotify Data",
    "section": "\nJ.1 By Artist",
    "text": "J.1 By Artist\nChoose your favourite artist and download their discography. Set include_groups to one or more of “album”, “single”, “appears_on”, and “compilation”.\n\ngaga <- get_artist_audio_features(\n  artist = 'Lady Gaga',\n  include_groups = \"album\"\n)\n\nLet’s explore the data you get back. Use glimpse() to see what columns are available and what type of data they have. It looks like there is a row for each of this artist’s tracks.\nLet’s answer a few simple questions first.\n\nJ.1.1 Tracks per Album\nHow many tracks are on each album? Some tracks have more than one entry in the table, so first select just the album_name and track_name columns and use distinct() to get rid of duplicates. Then count() the tracks per album. We’re using DT::datatable() to make the table interactive. Try sorting the table by number of tracks.\n\ngaga %>%\n  select(album_name, track_name) %>%\n  distinct() %>%\n  count(album_name) %>%\n  datatable(colnames = c(\"Albumn Name\", \"Number of Tracks\"))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse count() to explore the columns key_name, mode_name, and any other non-numeric columns.\n\n\n\nJ.1.2 Tempo\nWhat sort of tempo is Lady Gaga’s music? First, let’s look at a very basic plot to get an overview.\n\nggplot(gaga, aes(tempo)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\nWhat’s going on with the tracks with a tempo of 0?\n\ngaga %>% \n  filter(tempo == 0) %>%\n  select(album_name, track_name)\n\n\n\n  \n\n\n\nLooks like it’s all dialogue, so we should omit these. Let’s also check how variable the tempo is for multiple instances of the same track. A quick way to do this is to group by album and track, then check the standard deviation of the tempo. If it’s 0, this means that all of the values are identical. The bigger it is, the more the values vary. If you have a lot of data with a normal distribution (like a bell curve), then about 68% of the data are within one SD of the mean, and about 95% are within 2 SDs.\nIf we filter to tracks with SD greater than 0 (so any variation at all), we see that most tracks have a little variation. However, if we filter to tracks with an SD greater than 1, we see a few songs with slightly different tempo, and a few with wildly different tempo.\n\ngaga %>%\n  # omit tracks with \"Dialogue\" in the name\n  filter(!str_detect(track_name, \"Dialogue\")) %>%\n  # check for varying tempos for same track\n  group_by(album_name, track_name) %>%\n  filter(sd(tempo) > 1) %>%\n  ungroup() %>%\n  select(album_name, track_name, tempo) %>%\n  arrange(album_name, track_name)\n\n\n\n  \n\n\n\nYou can deal with these in any way you choose. Filter out some versions of the songs or listen to them to see which value you agree with and change the others. Here, we’ll deal with it by averaging the values for each track. This will also remove the tiny differences in the majority of duplicate tracks. Now we’re ready to plot.\n\ngaga %>%\n  filter(tempo > 0) %>%\n  group_by(album_name, track_name) %>%\n  summarise(tempo = round(mean(tempo)),\n            .groups = \"drop\") %>%\n  ungroup() %>%\n  ggplot(aes(x = tempo, fill = ..x..)) +\n  geom_histogram(binwidth = 4, show.legend = FALSE) +\n  scale_fill_gradient(low = \"#521F64\", high = \"#E8889C\") +\n  labs(x = \"Beats per minute\",\n       y = \"Number of tracks\",\n       title = \"Tempo of Lady Gaga Tracks\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan you see how we made the gradient fill for the histograms? Since the x-value of each bar depends on the binwidth, you have to use the code ..x.. in the mapping (not tempo) to make the fill correspond to each bar’s value.\n\n\nThis looks OK, but maybe we want a more striking plot. Let’s make a custom plot style and assign it to gaga_style in case we want to use it again. Then add it to the shortcut function, last_plot() to avoid having to retype the code for the last plot we created.\n\n# define style\ngaga_style <- theme(\n  plot.background = element_rect(fill = \"black\"),\n  text = element_text(color = \"white\", size = 11),\n  panel.background = element_rect(fill = \"black\"),\n  panel.grid.major.x = element_blank(),\n  panel.grid.minor.x = element_blank(),\n  panel.grid.major.y = element_line(colour = \"white\", size = 0.2),\n  panel.grid.minor.y = element_line(colour = \"white\", size = 0.2),\n  axis.text = element_text(color = \"white\"),\n  plot.title = element_text(hjust = 0.5)\n)\n\n## add it to the last plot created\nlast_plot() + gaga_style"
  },
  {
    "objectID": "app-spotify.html#by-playlist",
    "href": "app-spotify.html#by-playlist",
    "title": "Appendix J — Spotify Data",
    "section": "\nJ.2 By Playlist",
    "text": "J.2 By Playlist\nYou need to know the “uri” of a public playlist to access data on it. You can get this by copying the link to the playlist and selecting the 22 characters between “https://open.spotify.com/playlist/” and “?si=…”. Let’s have a look at the Eurovision 2021 playlist.\n\neurovision2021 <- get_playlist_audio_features(\n  playlist_uris = \"37i9dQZF1DWVCKO3xAlT1Q\"\n)\n\nUse glimpse() and count() to explore the structure of this table.\n\nJ.2.1 Track ratings\nEach track has several ratings: danceability, energy, speechiness, acousticness, instrumentalness, liveness, and valence. I’m not sure how these are determined (almost certainly by an algorithm). Let’s select the track names and these columns to have a look.\n\neurovision2021 %>%\n  select(track.name, danceability, energy, speechiness:valence) %>%\n  datatable()\n\n\n\n\n\n\nWhat was the general mood of Eurovision songs in 2021? Let’s use plots to assess. First, we need to get the data into long format to make it easier to plot multiple attributes.\n\nplaylist_attributes <- eurovision2021 %>%\n  select(track.name, danceability, energy, speechiness:valence) %>%\n  pivot_longer(cols = danceability:valence,\n               names_to = \"attribute\",\n               values_to = \"rating\")\n\nWhen we plot everything on the same plot, instrumentalness has such a consistently low value that all the other attributes disappear,\n\nggplot(playlist_attributes, aes(x = rating, colour = attribute)) +\n  geom_density()\n\n\n\n\n\n\n\nYou can solve this by putting each attribute into its own facet and letting the y-axis differ between plots by setting scales = \"free_y\". Now it’s easier to see that Eurovision songs tend to have pretty high danceability and energy.\n\nggplot(playlist_attributes, aes(x = rating, colour = attribute)) +\n  geom_density(show.legend = FALSE) +\n  facet_wrap(~attribute, scales = \"free_y\", nrow = 2)\n\n\n\nSeven track attributes for the playlist ‘Eurovision 2021’\n\n\n\n\n\nJ.2.2 Popularity\nLet’s look at how these attributes relate to track popularity. We’ll exclude instrumentalness, since it doesn’t have much variation.\n\npopularity <- eurovision2021 %>%\n  select(track.name, track.popularity,\n         acousticness, danceability, energy, \n         liveness, speechiness, valence) %>%\n  pivot_longer(cols = acousticness:valence,\n               names_to = \"attribute\",\n               values_to = \"rating\")\n\n\nggplot(popularity, aes(x = rating, y = track.popularity, colour = attribute)) +\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  geom_smooth(method = lm, formula = y~x, show.legend = FALSE) +\n  facet_wrap(~attribute, scales = \"free_x\", nrow = 2) +\n  labs(x = \"Attribute Value\",\n       y = \"Track Popularity\")\n\n\n\nThe relationship between track attributes and popularity.\n\n\n\n\n\nJ.2.3 Nested data\nSome of the columns in this table contain more tables. For example, each entry in the track.artist column contains a table with columns href, id, name, type, uri, and external_urls.spotify. Use unnest() to extract these tables. If there is more than one artist for a track, this will expand the table. For example, the track “Adrenalina” has two rows now, one for Senhit and one for Flo Rida.\n\neurovision2021 %>%\n  unnest(track.artists) %>%\n  select(track = track.name, \n         artist = name, \n         popularity = track.popularity) %>%\n  datatable()\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re a Eurovision nerd (like Emily), try downloading playlists from several previous years and visualise trends. See if you can find lists of the scores for each year and join the data to see what attributes are most related to points."
  },
  {
    "objectID": "app-spotify.html#by-genre",
    "href": "app-spotify.html#by-genre",
    "title": "Appendix J — Spotify Data",
    "section": "\nJ.3 By Genre",
    "text": "J.3 By Genre\nSelect the first 20 artists in the genre “eurovision”. So that people don’t spam the Spotify API, you are limited to up to 50 artists per request.\n\neuro_genre <- get_genre_artists(\n  genre = \"eurovision\",\n  limit = 20,\n  offset = 0\n)\n\n\neuro_genre %>%\n  select(name, popularity, followers.total) %>%\n  datatable()\n\n\n\n\n\n\nNow you can select the next 20 artists, incrementing the offset by 20, join that to the first table, and process the data.\n\neuro_genre2 <- get_genre_artists(\n  genre = \"eurovision\",\n  limit = 20,\n  offset = 20\n)\n\n\nbind_rows(euro_genre, euro_genre2) %>%\n  select(name, popularity, followers.total) %>%\n  datatable()\n\n\n\n\n\n\n\nJ.3.1 Repeated calls\nThere is a programmatic way to make several calls to a function that limits you. You usually want to set this up so that you are waiting a few seconds or minutes between calls so that you don’t get locked out (depending on how strict the API is). Use map_df() to automatically join the results into one big table.\n\n# create a slow version of get_genre_artists \n# delays 2 seconds after running\nslow_get_genre_artists <- slowly(get_genre_artists, \n                                 rate = rate_delay(2))\n\n# set 4 offsets from 0 to 150 by 50\noffsets <- seq(0, 150, 50)\n\n# run the slow function once for each offset\neuro_genre200 <- map_df(.x = offsets, \n                       .f = ~slow_get_genre_artists(\"eurovision\", \n                                                    limit = 50,\n                                                    offset = .x))\n\n\neuro_genre200 %>%\n  select(name, popularity, followers.total) %>%\n  arrange(desc(followers.total)) %>%\n  datatable()"
  },
  {
    "objectID": "app-spotify.html#by-track",
    "href": "app-spotify.html#by-track",
    "title": "Appendix J — Spotify Data",
    "section": "\nJ.4 By Track",
    "text": "J.4 By Track\nYou can get even more info about a specific track if you know its Spotify ID. You can get this from an artist, album, or playlist tables.\n\n# get the ID for Born This Way from the original album\nbtw_id <- gaga %>%\n  filter(track_name == \"Born This Way\", \n         album_name == \"Born This Way\") %>%\n  pull(track_id)\n\n\nJ.4.1 Features\nFeatures are a list of summary attributes of the track. These are also included in the previous tables, so this function isn’t very useful unless you are getting track IDs directly.\n\nbtw_features <- get_track_audio_features(btw_id)\n\n\n\ntibble [1 × 18] (S3: tbl_df/tbl/data.frame)\n $ danceability    : num 0.586\n $ energy          : num 0.827\n $ key             : int 11\n $ loudness        : num -5.1\n $ mode            : int 1\n $ speechiness     : num 0.15\n $ acousticness    : num 0.0037\n $ instrumentalness: int 0\n $ liveness        : num 0.328\n $ valence         : num 0.508\n $ tempo           : num 124\n $ type            : chr \"audio_features\"\n $ id              : chr \"6aDi4gOE2Cfc6ecynvP81R\"\n $ uri             : chr \"spotify:track:6aDi4gOE2Cfc6ecynvP81R\"\n $ track_href      : chr \"https://api.spotify.com/v1/tracks/6aDi4gOE2Cfc6ecynvP81R\"\n $ analysis_url    : chr \"https://api.spotify.com/v1/audio-analysis/6aDi4gOE2Cfc6ecynvP81R\"\n $ duration_ms     : int 260253\n $ time_signature  : int 4\n\n\n\nJ.4.2 Analysis\nThe analysis gives you seven different tables of details about the track. Use the names() function to see their names and look at each object to see what information it contains.\n\nbtw_analysis <- get_track_audio_analysis(btw_id)\n\n\n\n[1] \"meta\"     \"track\"    \"bars\"     \"beats\"    \"sections\" \"segments\" \"tatums\"  \n\n\n\n\nmeta gives you a list of some info about the analysis.\n\ntrack gives you a list of attributes, including duration, loudness, end_of_fade_in, start_of_fade_out, and time_signature. Some of this info was available in the previous tables.\n\nbars, beats, and tatums are tables with the start, duration and confidence for each bar, beat, or tatum of music (whatever a “tatum” is).\n\nsections is a table with the start, duration, loudness, tempo, key, mode, time signature for each section of music, along with confidence measures of each.\n\nsegments is a table with information about loudness, pitch and timbre of segments of analysis, which tend to be around 0.2 (seconds?)\n\nYou can use this data to map a song.\n\nggplot(btw_analysis$segments, aes(x = start, \n                                  y = loudness_start, \n                                  color = loudness_start)) +\n  geom_point(show.legend = FALSE) +\n  scale_colour_gradient(low = \"red\", high = \"purple\") +\n  scale_x_continuous(breaks = seq(0, 300, 30)) +\n  labs(x = \"Seconds\",\n       y = \"Loudness\",\n       title = \"Loudness Map for 'Born This Way'\") +\n  gaga_style\n\n\n\nUse data from the segments table of a track analysis to plot loudness over time."
  },
  {
    "objectID": "app-webpage.html#sec-webpage-create",
    "href": "app-webpage.html#sec-webpage-create",
    "title": "Appendix K — Webpages",
    "section": "\nK.1 Create a webpage",
    "text": "K.1 Create a webpage\n\nK.1.1 Create a project\n\nChoose New Project... from the File menu (don’t save any workspaces)\nChoose New Directory > Simple R Markdown Website\n\nSet your project name to “mywebpage”\n\nK.1.2 Site header\nThis is where you can set options like whether to show a table of contents and what the navigation bar will look like. We’ll edit this later to add a section menu.\n\nOpen the file _site.yml\n\nReplace the text with the following:\nname: \"mywebpage\"  \nauthor: \"YOUR NAME\"  \noutput_dir: \"docs\"  \noutput:  \n  html_document:  \n    self_contained: no  \n    theme: \n      version: 4\n      bootswatch: yeti \nnavbar:  \n  title: \"My First Webpage\"  \n    left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n\nSave the file (do not change the name)\n\nK.1.3 Edit the pages\nEdit the text in the index.Rmd and about.Rmd pages. You can use R markdown, including code chunks.\n\nK.1.4 Render the site\nIn the upper right “Build” pane, click on the “Build website” hammer icon. This will render the website and automatically open it in a browser window. Alternatively, type the following into the Console pane:\n\nbrowseURL(rmarkdown::render_site(encoding = 'UTF-8'))\n\nIf you accidentally close the website and want to look at it again, you don’t have to re-render it. Click on the docs directory in the Files tab of the lower right pane, then click on index.html and choose View in Web Browser."
  },
  {
    "objectID": "app-webpage.html#sec-webpage-pages",
    "href": "app-webpage.html#sec-webpage-pages",
    "title": "Appendix K — Webpages",
    "section": "\nK.2 Add pages",
    "text": "K.2 Add pages\n\nCreate a new .Rmd file for each webpage\nAdd content to the webpages using R Markdown\nRe-render the site\n\nIf you include linked content like image files, make sure they are copied to your main project directory and linked using relative paths.\nTo get your webpage online, copy the contents of the docs directory to a web server. If you don’t have access to a web server, you can make free websites using a GitHub repository and GitHub Pages)."
  },
  {
    "objectID": "app-webpage.html#sec-webpage-styles",
    "href": "app-webpage.html#sec-webpage-styles",
    "title": "Appendix K — Webpages",
    "section": "\nK.3 Styles",
    "text": "K.3 Styles\nYou can change the appearance of your website by changing the theme in the _site.yml file (see Appendix I), but the instructions below will help you to customise things even further.\n\nK.3.1 Add custom styles\nYou can add a custom style sheet (a document that determines how each element of your website should look) by adding the line css: style.css under html_document: in the _site.yml file.\noutput:  \n  html_document:  \n    self_contained: no  \n    theme: \n      version: 4\n      bootswatch: readable\n    css: style.css\nThen you need to create a file named style.css and add your custom styles there. The web has thousands of guides to CSS, but codeacademy has great interactive tutorials for learning html, css, and even more advanced web coding like javascript.\nHowever, the basics of css are easy to learn and it’s best to just start playing around with it. Add the following text to your style.css file and re-render the website.\n\nK.3.2 Change global fonts and colours\nbody {\n  font-size: 2em;\n  font-family: \"Times New Roman\";\n  color: white;\n  background-color: #660000;\n}\n\n\nThis will make the text on your website larger, a different font, and change the text and background colours.\n\nThe theme you’re using might have css that blocks the styles you’re trying to change. You can add !important before the end colon to override that.\n\nK.3.3 Change certain elements\nMaybe you only want to change the font colour for your headings, not the rest of the text. You can apply a style to a specific element type by specifying the element name before the curly brackets.\nh1, h2, h3 {\n  text-align: center;\n  color: hsl(0, 100%, 20%);\n}\n\nh3 {\n  font-style: italic;\n}\n\np {\n  border: 1px solid green;\n  padding: 10px;\n  line-height: 2;\n}\n\nul {\n  border: 3px dotted red;\n  border-radius: 10px;\n  padding: 10px 30px;\n}"
  },
  {
    "objectID": "app-webpage.html#example-using-the-styles-above",
    "href": "app-webpage.html#example-using-the-styles-above",
    "title": "Appendix K — Webpages",
    "section": "\nK.4 Example using the styles above",
    "text": "K.4 Example using the styles above\nThe CSS above changes the styles for three levels of headers (h2, h3, h4) and sets the third level to italics.\n\nK.4.1 Level 3 header\nIt also gives paragraphs (p) a green border and double-spacing.\nLevel 4 header\nUnordered Lists (ul) get:\n\ndotted red border\nround corners\nincreased padding on top (10px) and sides (30px)"
  }
]